% The Dakota blackbox and derivative-free simulation optimization framework, a numerical software package (in C++) maintained by Sandia that offers support for AI/ML surrogate modeling, multifidelity modeling, uncertainty quantification (UQ), and distributed and parallel computing
@techreport{adams2022dakota,,
	author = {Adams, Brian M. and Bohnhoff, William J. and Dalbey, Keith R. and Ebeida, Mohamed S. and Eddy, John P. and Eldred, Michael S. and Hooper, Russell W. and Hough, Patricia D. and Hu, Kenneth T. and Jakeman, John D. and Khalil, Mohammad and Maupin, Kathryn A. and Monschke, Jason A. and Ridgeway, Elliott M. and Rushdi, Ahmad A. and Seidl, D. Thomas and Stephens, J. Adam and Swiler, Laura P. and Tran, Anh and Winokur, Justin G.},
	title = {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.16 User's Manual},
	year = {2022},
	number = {SAND2022-6171 version 6.16},
	institution = {Sandia National Laboratory},
	address = {Albuquerque, NM, USA},
	url = {https://dakota.sandia.gov/sites/default/files/docs/6.16.0/Users-6.16.0.pdf},
}

% Using RBF surrogates to solve blackbox / derivative-free multiobjective optimization problems
@article{akhtar2016multi,
	author = {Akhtar, Taimoor and Shoemaker, Christine A.},
	title = {Multi objective optimization of computationally expensive multi-modal functions with {RBF} surrogates and multi-rule selection},
	year = {2016},
	month = {1},
	journal = {Journal of Global Optimization},
	volume = {64},
	number = {1},
	pages = {17--32},
	publisher = {Springer},
	doi = {10.1007/s10898-015-0270-y},
	url = {http://link.springer.com/10.1007/s10898-015-0270-y},
	issn = {0925-5001},
}

% Optuna open source software for fully distributed hyperparameter optimization. Maintained by a private startup in Japan (named Optuna). This software provides high-quality distributed wrappers for many neural architecture search and generic optimization algorithms.
@inproceedings{akiba2019optuna,
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	title = {Optuna: A next-generation hyperparameter optimization framework},
	year = {2019},
	month = {7},
	booktitle = {Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
	pages = {2623--2631},
	organization = {ACM},
	location = {Anchorage AK USA},
	doi = {10.1145/3292500.3330701},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
}

% The recommended citation for the jax software project -- one of my personal favorite open source numerical software in Python. Performs autograd (or algorithmic differentiation) in either forward or reverse mode, is strongly typed, can act as a drop-in replacement for numpy, and can be just-in-time (jit) compiled for massive speedups
@misc{bradbury2018jax,
	author = {Bradbury, J. and others},
	title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
	year = {2018},
	number = {0.3.13},
	url = {http://github.com/google/jax},
}

% JAHS-Bench-201: The latest test suite of benchmark problems for neural architecture search. The baseline is random search, but you can solve the problems in their parameterized search space with any optimization algorithm, record the number of true function / simulation evaluations (i.e., networks trained) and submit this to the JAHS-Bench leaderboards on GitHub. This is a good representative test problem for NAS. Both single and multiobjective benchmarks are provided, also most problems can be run in both single or multifidelity evaluation modes
@inproceedings{al.2022jahsbench201,
	author = {al., A. Bansal et},
	title = {JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search},
	year = {2022},
	booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
}

% Introducing an algorithm for solving multiobjective optimization (MOO) problems via the global optimization algorithm DIRECT. Some theory and preliminary results, but no software. Likely not scalable for real-world computationally expensive problems, due to the number of boxes that would need to be divided per iteration using this method. More of a theoretical foundation for a later practical algorithm
@inproceedings{aldujaili2016dividing,
	author = {Al-Dujaili, Abdullah and Suresh, Sundaram},
	title = {Dividing rectangles attack multi-objective optimization},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 IEEE Congress on Evolutionary Computation (CEC '16)},
	pages = {3606--3613},
	organization = {IEEE},
	location = {Vancouver, BC, Canada},
	doi = {10.1109/CEC.2016.7744246},
	url = {http://ieeexplore.ieee.org/document/7744246/},
}

% A numerical software package written in MATLAB -- provides a surrogate modeling toolbox for multiobjective optimization problems
@inproceedings{aldujaili2016matlab,
	author = {Al-Dujaili, Abdullah and Suresh, Sundaram},
	title = {A {MATLAB} toolbox for surrogate-assisted multi-objective optimization: A preliminary study},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
	pages = {1209--1216},
	organization = {ACM},
	location = {Denver, CO, USA},
	doi = {10.1145/2908961.2931703},
	url = {https://dl.acm.org/doi/10.1145/2908961.2931703},
}

% A survey and review of surrogate modeling and response-surface modeling (RSM) techniques used in engineering
@article{alizadeh2020managing,
	author = {Alizadeh, Reza and Allen, Janet K. and Mistree, Farrokh},
	title = {Managing computational complexity using surrogate models: a critical review},
	year = {2020},
	month = {7},
	journal = {Research in Engineering Design},
	volume = {31},
	number = {3},
	pages = {275--298},
	publisher = {Springer},
	doi = {10.1007/s00163-020-00336-7},
	url = {https://link.springer.com/10.1007/s00163-020-00336-7},
	issn = {0934-9839},
}

% A Fortran 90 implementation of quasi-Newton stochastic optimization algorithms. This open source numerical software solves both determinisitc and stochastic blackbox optimization problems via a quasi-Newton trust-region method. It is a bit wasteful in terms of the number of function evaluations per iteration as it performs a fully Latin hypercube sampling of the trust region in each iteration, and does not explicitly re-use previous iterates to reduce iteration costs, like some of the more advanced model based methods. Still, it is extremely robust and a good choice in stochastic situations. Also includes a good Fortran implementation of Latin hypercube sampling and efficient sorting algorithms
@article{amos2020algorithm,
	author = {Amos, Brandon D. and Easterling, David R. and Watson, Layne T. and Thacker, William I. and Castle, Brent S. and Trosset, Michael W.},
	title = {Algorithm 1007: {QNSTOP}: {Q}uasi-{N}ewton algorithm for stochastic optimization},
	year = {2020},
	month = {6},
	journal = {ACM Transactions on Mathematical Software},
	volume = {46},
	number = {2},
	numpages = {17},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3374219},
	url = {https://dl.acm.org/doi/10.1145/3374219},
	issn = {0098-3500},
	keywords = {},
}

% The MOSEK solver is an open source numerical optimization software solver that is used in older versions of scipy.optimize to solve linear programming problems via the interior point method
@inproceedings{andersen2000mosek,
	author = {Andersen, Erling D and Andersen, Knud D},
	title = {The {MOSEK} interior point optimizer for linear programming: an implementation of the homogeneous algorithm},
	year = {2000},
	booktitle = {High performance optimization},
	pages = {197--232},
	organization = {Springer},
	url = {https://link.springer.com/chapter/10.1007/978-1-4757-3216-0_8},
}

% The user guide for the reference implementation of LAPACK: the original open source numerical software for all common dense linear algebra operations.
@book{anderson1999lapack,
	author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
	title = {{LAPACK} Users' Guide},
	year = {1999},
	edition = {3},
	publisher = {SIAM},
	address = {Philidelphia, PA, USA},
	keywords = {},
}

% Is derivative / gradient information counterproductive for solving MOOs? In this paper, it appears to make direct-search type methods perform worse by the metrics used. It could be that the metrics favor diversity over convergence, in which case one can get better diversity by taking bad evaluations. But I need to read more carefully to decide whether that is what's going on here
@article{andreani2022using,
	author = {Andreani, R. and Cust{\'o}dio, Ana Lu{\'i}sa and Raydan, M.},
	title = {Using first-order information in direct multisearch for multiobjective optimization},
	year = {2022},
	month = {11},
	journal = {Optimization Methods and Software},
	volume = {37},
	number = {6},
	pages = {2135--2156},
	publisher = {Taylor \& Francis},
	doi = {10.1080/10556788.2022.2060971},
	url = {https://www.tandfonline.com/doi/full/10.1080/10556788.2022.2060971},
	issn = {1055-6788},
}

% Applying grey-box bayesian optimization tutorial: using Bayesian optimization on structured problems, where a blackbox function is composed with an algebraic function, just like with ParMOO and Jeff's GOOMBAH paper. Tutorial performed using BoTorch
@inproceedings{astudillo2021thinking,
	author = {Astudillo, Raul and Frazier, Peter I.},
	title = {Thinking inside the box: a tutorial on grey-box bayesian optimization},
	year = {2021},
	month = {12},
	booktitle = {Proc. 2021 Winter Simulation Conference (WSC 2021)},
	articleno = {2},
	numpages = {15},
	organization = {IEEE},
	location = {Phoenix, Arizona},
	doi = {10.1109/WSC52266.2021.9715343},
	url = {https://ieeexplore.ieee.org/document/9715343/},
}

% BiMADS -- a biobjective direct search / generalized pattern search via the MADS algorithm with an adaptive weighting scheme to trace the Pareto front from one end to the other. The numerical software implementation is part of the NOMAD software package (written in C++)
@article{audet2008multiobjective,
	author = {Audet, Charles and Savard, Gilles and Zghal, Walid},
	title = {Multiobjective optimization through a series of single-objective formulations},
	year = {2008},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {19},
	number = {1},
	pages = {188--210},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/060677513},
	url = {http://epubs.siam.org/doi/10.1137/060677513},
	issn = {1052-6234},
}

% The progressive barrier penalty for nonlinear blackbox optimization methods. Basically adds a progressive penalty for violating constraints based on the distance to feasibility
@article{audet2009progressive,
	author = {Audet, Charles and Dennis, John E.},
	title = {A Progressive Barrier for Derivative-Free Nonlinear Programming},
	year = {2009},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {20},
	number = {1},
	pages = {445--472},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/070692662},
	url = {http://epubs.siam.org/doi/10.1137/070692662},
	issn = {1052-6234},
}

% Multi-MADS -- a multiobjective direct search / generalized pattern search via MADS algorithm, using normal boundary intersection (NBI) for adaptive weighting
@article{audet2010mesh,
	author = {Audet, Charles and Savard, Gilles and Zghal, Walid},
	title = {A mesh adaptive direct search algorithm for multiobjective optimization},
	year = {2010},
	month = {8},
	journal = {European Journal of Operational Research},
	volume = {204},
	number = {3},
	pages = {545--556},
	publisher = {Elsevier BV},
	doi = {10.1016/j.ejor.2009.11.010},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221709008601},
	issn = {0377-2217},
}

% Book on fundamental methods, terminology, and theory in blackbox and derivative-free optimization (DFO) -- covers topics such as definitions of blackbox functions, heuristics, classical methods, positive bases and minimum spanning sets, generalized pattern search, and direct search, fully linear and quadratic models, model-drive descent and trust-region methods and ensuring model quality, general surrogate modeling, constraints, and multiobjective basics.
@book{audet2017derivativefree,
	author = {Audet, Charles and Hare, Warren},
	title = {Derivative-free and blackbox optimization},
	year = {2017},
	booktitle = {Springer Series in Operations Research and Financial Engineering},
	series = {Springer Series in Operations Research and Financial Engineering},
	publisher = {Springer International},
	address = {Charm, Switzerland},
	doi = {10.1007/978-3-319-68913-5},
	url = {http://link.springer.com/10.1007/978-3-319-68913-5},
	isbn = {9783319689128},
	issn = {1431-8598},
}

% A thorough survey of over 50 commonly used performance indicators in multiobjective optimization -- key takeaways: performance metrics can measure different properties of an algorithm, such as whether it is converging to the true Pareto front, the coverage of the true Pareto front, and the average diversity of solutions. One of the only metrics that is monotonic (i.e., cannot become worse when a solution contains a previous solution) is the hypervolume indicator, which is the standard in evolutionary algorithms
@article{audet2021performance,
	author = {Audet, Charles and Bigeon, Jean and Cartier, Dominique and Digabel}, Sébastien {Le and Salomon, Ludovic},
	title = {Performance indicators in multiobjective optimization},
	year = {2021},
	month = {7},
	journal = {European Journal of Operational Research},
	volume = {292},
	number = {2},
	pages = {397--422},
	publisher = {Elsevier BV},
	doi = {10.1016/j.ejor.2020.11.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221720309620},
	issn = {0377-2217},
	keywords = {Multiobjective optimization, Quality indicators, Performance indicators},
}

% NOMAD v4 -- open source numerical software package (in C++) for blackbox and derivative-free optimization via the MADS algorithms. After publication, they have added support for multiobjective optimization, mixed variables, nonlinear constraints, etc. Great example of high-impact open source numerical and optimization software. Improvements over NOMAD v3 include improvements to fundamental algorithms, coding practices, release process, and general project structure to support continuous research and development into the future
@article{audet2022algorithm,
	author = {Audet, Charles and Le Digabel, S\'{e}bastien and Rochon Montplaisir, Viviane and Tribes, Christophe},
	title = {{Algorithm 1027}: {NOMAD} Version 4: Nonlinear Optimization with the {MADS} Algorithm},
	year = {2022},
	month = {9},
	journal = {ACM Transactions on Mathematical Software},
	volume = {48},
	number = {3},
	articleno = {35},
	numpages = {22},
	publisher = {ACM},
	doi = {10.1145/3544489},
	url = {https://dl.acm.org/doi/10.1145/3544489},
	issn = {0098-3500},
}

% Handling categorical/integer/mixed variables in blackbox optimization: This is the method used to perform hyperparameter tuning of neural-networks and other AI models via MADS. In general, they decompose variables into meta variables (which determine whether other variables are active or not, such as the number of layers in the network which can deactivate variables associated with inactive layers), categorical variables (which either need to be embedded somehow or can be explored in an unordered manner via direct search / generalized pattern search), and finally standard variables which includes both continuous and relaxed integer variables
@inproceedings{audet2023general,
	author = {Audet, Charles and Hall{\'e}-Hannan, Edward and Le Digabel, S{\'e}bastien},
	title = {A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables},
	year = {2023},
	month = {2},
	booktitle = {Operations Research Forum},
	volume = {4},
	number = {1},
	numpages = {12},
	organization = {Springer},
	doi = {10.1007/s43069-022-00180-6},
	url = {https://link.springer.com/10.1007/s43069-022-00180-6},
	issn = {2662-2556},
}

% BoTorch: Modular bayesian optimization framework and numerical software package. Uses Ax for deploying A/B testing applications. Uses pytorch for autograd, and uses monte carlo sampling and kernel reparameterization tricks in order to efficiently evaluate composite objective and acquisition functions and non gaussian kernels. Great example of high-impact open source numerical software and optimization software
@inproceedings{balandat2020botorch,
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	title = {{BoTorch}: A Framework for Efficient {M}onte-{C}arlo {B}ayesian Optimization},
	year = {2020},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {21524--21538},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
}

% Prasanna and Stefan's original DeepHyper publication -- DeepHyper is an open source extreme-scale distributed optimization package, designed to scale to Argonne's exascale HPCs. Able to train ensembles of neural networks with diverse architectures at scale, with both single and multiobjective hyperparameter tuning support
@inproceedings{balaprakash2018deephyper,
	author = {Balaprakash, Prasanna and Salim, Michael and Uram, Thomas D. and Vishwanath, Venkat and Wild, Stefan M.},
	title = {DeepHyper: Asynchronous hyperparameter search for deep neural networks},
	year = {2018},
	month = {12},
	booktitle = {IEEE 25th international conference on high performance computing (HiPC)},
	pages = {42--51},
	organization = {IEEE},
	location = {Bengaluru, India},
	doi = {10.1109/hipc.2018.00014},
	url = {https://ieeexplore.ieee.org/document/8638041/},
}

% The PETSc user's guide. I haven't used it but PETSc is a widely-used C++ numerical software library and linear algebra / iterative algorithms framework developed at Argonne and used for implementing many well-known iterative solvers, especially in the area of CFD. This is a great example of high-impact open source numerical software and best practices in open source scientific software. Now ships together with TAO, a similar simulation optimization software package
@techreport{balay2022petsc/tao,
	author = {Balay, Satish and Abhyankar, Shrirang and Adams, Mark F. and Benson, Steven and Brown, Jed and Brune, Peter and Buschelman, Kris and Constantinescu, Emil and Dalcin, Lisandro and Dener, Alp and Eijkhout, Victor and Gropp, William D. and Hapla, V\'{a}clav and Isaac, Tobin and Jolivet, Pierre and Karpeev, Dmitry and Kaushik, Dinesh and Knepley, Matthew G. and Kong, Fande and Kruger, Scott and May, Dave A. and McInnes, Lois Curfman and Mills, Richard Tran and Mitchell, Lawrence and Munson, Todd and Roman, Jose E. and Rupp, Karl and Sanan, Patrick and Sarich, Jason and Smith, Barry F. and Zampini, Stefano and Zhang, Hong and Zhang, Hong and Zhang, Junchao},
	title = {{PETSc/TAO} Users Manual},
	year = {2022},
	number = {ANL-21/39 - Revision 3.17},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://petsc.org/release/docs/manual/manual.pdf},
}

% AMOSA algorithm -- apparently this is a widely-known standard in multiobjective simulated annealing because reviewers regularly ask me to cite this. But I've never met anyone who uses this and I can't find the software anywhere. The algorithm seems very reasonable though
@article{bandyopadhyay2008simulated,
	author = {Bandyopadhyay, Sanghamitra and Saha, Sriparna and Maulik, Ujjwal and Deb, Kalyanmoy},
	title = {A Simulated Annealing-Based Multiobjective Optimization Algorithm: {AMOSA}},
	year = {2008},
	month = {6},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {12},
	number = {3},
	pages = {269--283},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TEVC.2007.900837},
	url = {http://ieeexplore.ieee.org/document/4358775/},
	issn = {1941-0026},
}

% An open source numerical software library for solving multiobjective optimization problems in java in real-time via heuristics. The authors combine jMetal with data streaming via Apache Spark to solve distributed multiobjective optimization problems with streaming data in real-time
@article{barbagonzález2018jmetalsp,
	author = {Barba-González, Cristóbal and García-Nieto, José and Nebro, Antonio J. and Cordero, José A. and Durillo, Juan J. and Navas-Delgado, Ismael and Aldana-Montes, José F.},
	title = {{jMetalSP}: A framework for dynamic multi-objective big data optimization},
	year = {2018},
	month = {8},
	journal = {Applied Soft Computing},
	volume = {69},
	pages = {737--748},
	publisher = {Elsevier BV},
	doi = {10.1016/j.asoc.2017.05.004},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494617302557},
	issn = {1568-4946},
}

% The FAIR principles for open source scientific software, data, source code, and experiments should by findable (via DOIs or other), accessible (clear purpose and metadata), interoperable (should use standard interfaces, data formats, and schemas), and reusable (well documented, understandable, and not overly specialized to an unnecessarilly niche use-case). These are good principles for any open source software development practices
@article{barker2022introducing,
	author = {Barker, Michelle and Chue Hong, Neil P. and Katz, Daniel S. and Lamprecht, Anna-Lena and Martinez-Ortiz, Carlos and Psomopoulos, Fotis and Harrow, Jennifer and Castro, Leyla Jael and Gruenpeter, Morane and Martinez, Paula Andrea and Honeyman, Tom},
	title = {Introducing the {FAIR} Principles for research software},
	year = {2022},
	month = {10},
	journal = {Scientific Data},
	volume = {9},
	number = {1},
	numpages = {622},
	publisher = {Nature Publishing Group},
	doi = {10.1038/s41597-022-01710-x},
	url = {https://www.nature.com/articles/s41597-022-01710-x},
	issn = {2052-4463},
}

% jMetalPy -- a Python framework for solving MOOs with EAs -- this open source numerical software is a Python implementation of jMetal, with some improvements to code quality and new features for better open source software development and parallelism
@article{benitezhidalgo2019jmetalpy,
	author = {Ben{\'i}tez-Hidalgo, Antonio and Nebro, Antonio J. and Garc{\'i}a-Nieto, Jos{\'e} and Oregi, Izaskun and Ser}, Javier {Del},
	title = {{jMetalPy}: A {P}ython framework for multi-objective optimization with metaheuristics},
	year = {2019},
	month = {12},
	journal = {Swarm and Evolutionary Computation},
	volume = {51},
	numpages = {100598},
	publisher = {Elsevier BV},
	doi = {10.1016/j.swevo.2019.100598},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650219301397},
	issn = {2210-6502},
}

% A numerical algorithm for multiobjective descent, using RBF surrogates + trust regions. Builds heavily off of Stefan's PhD thesis (ORBIT)
@article{berkemeier2021derivativefree,
	author = {Berkemeier, Manuel and Peitz, Sebastian},
	title = {Derivative-Free Multiobjective Trust Region Descent Method Using Radial Basis Function Surrogate Models},
	year = {2021},
	month = {4},
	journal = {Mathematical and Computational Applications},
	volume = {26},
	number = {2},
	numpages = {31},
	publisher = {Multidisciplinary Digital Publishing Institute},
	doi = {10.3390/mca26020031},
	url = {https://www.mdpi.com/2297-8747/26/2/31},
	issn = {2297-8747},
}

% Proof that the complexity of calculating the hypervolume indicator with o objectives is exponential. Roughly the same reasons that calculating simplices in an o-dimensional Delaunay triangulation or computing the facets of an o-dimensional convex hull are exponential
@article{beume2009complexity,
	author = {Beume, Nicola and Fonseca, Carlos M. and Lopez-Ibanez, Manuel and Paquete, Luis and Vahrenhold, Jan},
	title = {On the complexity of computing the hypervolume indicator},
	year = {2009},
	month = {10},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {13},
	number = {5},
	pages = {1075--1082},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2009.2015575},
	url = {http://ieeexplore.ieee.org/document/5208224/},
	issn = {1941-0026},
}

% NASA's FUN3D CFD solver. This is one of the oldest and standard numerical softwares for solving CFD problems. Written in mostly Fortran 90. Uses a form of the problem that yields the adjoints, which can be used to optimize structures in fewer steps and perform sensitivity analyses. The kernel uses an iterative solver to solve a massive block-sparse linear system (I think derived from the weak form). Some a priori multiobjective optimization solvers are described in Section 9.9
@techreport{biedron2019fun3d,
	author = {Biedron, Robert T. and Carlson, Jan Renee and Derlaga, Joseph M. and Gnoffo, Peter A. and Hammond, Dana P. and Jones, William T. and Kleb, Bill and Lee-Rausch, Elizabeth M. and Nielson, Eric J. and Park, Michael A. and Rumsey, Christopher L. and Thomas, James L. and Thompson, Kyle B. and Wood, William A.},
	title = {{FUN3D Manual}: 13.6},
	year = {2019},
	number = {{NASA} {T}echnical {M}emorandum ({TM}) 2019-220416},
	institution = {NASA Langley Research Center},
	address = {Hampton, VA, USA},
	url = {https://fun3d.larc.nasa.gov/papers/FUN3D_Manual-13.6.pdf},
}

% DMulti-MADS: Improved Multi-MADS using direct search / generalized pattern search plus some improvements to the Multi-MADS algorithm -- I need to re-read this to remember what the improvements were
@article{bigeon2020dmultimads,
	author = {Bigeon, Jean and Le Digabel, S{\'e}bastien and Salomon, Ludovic},
	title = {{DM}ulti-{MADS}: {M}esh adaptive direct multisearch for blackbox multiobjective optimization},
	year = {2020},
	month = {6},
	journal = {Computational Optimization and Applications},
	volume = {79},
	number = {2},
	pages = {301--338},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-021-00272-9},
	url = {https://link.springer.com/10.1007/s10589-021-00272-9},
	issn = {0926-6003},
}

% pagmo/pygmo - Parallel frameworks for solving multiobjective optimization problems (MOO) in Java and Python. Great example of open source numerical software, published in JOSS
@article{biscani2020parallel,
	author = {Biscani, Francesco and Izzo, Dario},
	title = {A parallel global multiobjective framework for optimization: pagmo},
	year = {2020},
	month = {9},
	journal = {Journal of Open Source Software},
	volume = {5},
	number = {53},
	numpages = {2338},
	publisher = {The Open Journal},
	doi = {10.21105/joss.02338},
	url = {https://joss.theoj.org/papers/10.21105/joss.02338},
	issn = {2475-9066},
}

% The ScaLAPACK user's guide: A highly parallel and scalable open source implementation of the LAPACK software, for solving massive scale numerical linear algebra systems on distributed systems
@book{blackford1997scalapack,
	author = {Blackford, L. Susan and Choi, Jaeyoung and Cleary, Andy and D'Azevedo, Eduardo and Demmel, James and Dhillon, Inderjit and Dongarra, Jack and Hammarling, Sven and Henry, Greg and Petitet, Antoine and Stanley, K. and Walker, D. and Whaley, R. C.},
	title = {ScaLAPACK Users' Guide},
	year = {1997},
	volume = {4},
	publisher = {SIAM},
	keywords = {},
}

% pymoo is an open source software package implementing NSGA-II, NSGA-III, and many other multiobjective evolutionary algorithms (MOEAs), plus extensions for handling things such as categorical variables. This is a well-maintained and well-documented open-source numerical software package. It is maintained by the lab of the original NSGA-II author, and therefore could be considered the official NSGA-II implementation. all source code in Python
@article{blank2020pymoo,
	author = {Blank, Julian and Deb, Kalyanmoy},
	title = {{pymoo}: Multi-Objective Optimization in {Python}},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {89497--89509},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2020.2990567},
	url = {https://ieeexplore.ieee.org/document/9078759/},
	issn = {2169-3536},
	git = {http://github.com/anyoptimization/pymoo},
}

% Methodolgies for calibrating the Fayans EDF model to experimental data. Data is expensive and limited and the model itself is computationally expensive, so this is a classical inverse problem. The problem is actually multiobective because the data themselves come from various categories representing different types of observations, and the standard deviations for each of these observables is not known. Could be configured as a 3 or 9-objective problem
@article{bollapragada2020optimization,
	author = {Bollapragada, Raghu and Menickelly, Matt and Nazarewicz, Witold and O'Neal, Jared and Reinhard, Paul-Gerhard and Wild, Stefan M.},
	title = {Optimization and supervised machine learning methods for fitting numerical physics models without derivatives},
	year = {2020},
	month = {2},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	volume = {48},
	number = {2},
	numpages = {24001},
	publisher = {IOP Publishing},
	doi = {10.1088/1361-6471/abd009},
	url = {https://iopscience.iop.org/article/10.1088/1361-6471/abd009},
	issn = {0954-3899},
}

% The open source numerical software package (in Python) pySMT. This is a surrogate modeling and Bayesian optimization toolbox for solving multidisciplinary engineering design optimization (MDO) problems, while utilizing derivatives and providing numerical stability analysis for each surrogate model class.
@article{bouhlel2019python,
	author = {Bouhlel, Mohamed Amine and Hwang, John T. and Bartoli, Nathalie and Lafage, Rémi and Morlier, Joseph and Martins, Joaquim R.R.A.},
	title = {A {P}ython surrogate modeling framework with derivatives},
	year = {2019},
	month = {9},
	journal = {Advances in Engineering Software},
	volume = {135},
	pages = {102--662},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2019.03.005},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0965997818309360},
	issn = {0965-9978},
}

% A Multiobjective Bayesian optimization algorithm, very similar to ParEGO -- this algorithm uses the Gaussian process surrogates with NSGA-II to solve the problem. However, spectral sampling and thompson sampling are then employed to subselect a diverse set of candidates for batch evaluation. The resulting algorithm is called TSEMO
@article{bradford2018efficient,
	author = {Bradford, Eric and Schweidtmann, Artur M. and Lapkin, Alexei},
	title = {Efficient multiobjective optimization employing {Gaussian} processes, spectral sampling and a genetic algorithm},
	year = {2018},
	month = {6},
	journal = {Journal of Global Optimization},
	volume = {71},
	number = {2},
	pages = {407--438},
	publisher = {Springer},
	doi = {10.1007/s10898-018-0609-2},
	url = {http://link.springer.com/10.1007/s10898-018-0609-2},
	issn = {0925-5001},
}

% A study on utilizing polynomial surrogate models during multiobjective direct search and generalized pattern search techniques
@article{bras2020use,
	author = {Br{\'a}s, Carmo P. and Cust{\'o}dio, Ana Lu{\'\i}sa},
	title = {On the use of polynomial models in multiobjective directional direct search},
	year = {2020},
	month = {12},
	journal = {Computational Optimization and Applications},
	volume = {77},
	number = {3},
	pages = {897--918},
	publisher = {Springer},
	doi = {10.1007/s10589-020-00233-8},
	url = {https://link.springer.com/10.1007/s10589-020-00233-8},
	issn = {0926-6003},
}

% The original TOMS open source numerical software code implementing Sobol sequence (low discrepancy sequence) generation in Fortran 90. Apparently there is a bug or limitation to this code, fixed by Joe et al. 2003 in their TOMS Remark on 659, and subsequent publication of a new generator used in Scipy
@article{bratley1988algorithm,
	author = {Bratley, Paul and Fox, Bennett L.},
	title = {Algorithm 659: Implementing Sobol's Quasirandom Sequence Generator},
	year = {1988},
	month = {3},
	journal = {ACM Transactions on Mathematical Software},
	volume = {14},
	number = {1},
	numpages = {13},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/42288.214372},
	url = {https://doi.org/10.1145/42288.214372},
	issn = {0098-3500},
}

% Analysis of hypervolume indicator as proxy for solution set quality -- results for 2-objectives only, this paper shows that the hypervolume indicator is the best single indicator we have, but with some caveates
@article{bringmann2013approximation,
	author = {Bringmann, Karl and Friedrich, Tobias},
	title = {Approximation quality of the hypervolume indicator},
	year = {2013},
	month = {2},
	journal = {Artificial Intelligence},
	volume = {195},
	number = {0004-3702},
	pages = {265--290},
	publisher = {Elsevier BV},
	doi = {10.1016/j.artint.2012.09.005},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370212001178},
	issn = {0004-3702},
}

% Key findings from the VarSys project on modeling HPC performance variability with surrogates and RSM, and using these models to inform decision making through visualizations, optimization, and otherwise -- a good real-world example of how modeling, interpolation, optimization, and data science can come together to produce actionable results (in the field of HPC performance tuning)
@article{cameron2019moana,
	author = {Cameron, Kirk W. and Anwar, Ali and Cheng, Yue and Xu, Li and Li, Bo Ananth  Uday and Bernard, Jon and Jearls, Chandler and Lux, Thomas and Hong, Yili and Watson, Layne T. and Butt, Ali R.},
	title = {{MOANA}: {M}odeling and analyzing {I/O} variability in parallel system experimental design},
	year = {2019},
	month = {8},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	volume = {30},
	number = {8},
	pages = {1843--1856},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/tpds.2019.2892129},
	url = {https://ieeexplore.ieee.org/document/8631172/},
	issn = {1045-9219},
}

% The open source numerical software MODIR is proposed here. MODIR can be used to solve multiobjective blackbox optimization problems via a multiobjective variant of the DIRECT blackbox algorithm (direct search method). The motivating application is a multidisciplinary shiphull engineering design problem
@article{campana2018multiobjective,
	author = {Campana, Emilio Fortunato and Diez, Matteo and Liuzzi, Giampaolo and Lucidi, Stefano and Pellegrini, Riccardo and Piccialli, Veronica and Rinaldi, Francesco and Serani, Andrea},
	title = {A multi-objective {DIRECT} algorithm for ship hull optimization},
	year = {2018},
	month = {9},
	journal = {Computational Optimization and Applications},
	volume = {71},
	number = {1},
	pages = {53--72},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-017-9955-0},
	url = {http://link.springer.com/10.1007/s10589-017-9955-0},
	issn = {0926-6003},
}

@article{chang2020algorithm,
	author = {Chang, Tyler H. and Watson, Layne T. and Lux, Thomas C. H. and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Algorithm 1012: {DELAUNAYSPARSE}: {I}nterpolation via a sparse subset of the {D}elaunay triangulation in medium to high dimensions},
	year = {2020},
	month = {12},
	journal = {home},
	series = {Collections of the ACM},
	volume = {46},
	number = {4},
	articleno = {38},
	numpages = {20},
	publisher = {Association of Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3422818},
	url = {https://dl.acm.org/doi/10.1145/3422818},
	issn = {0098-3500},
	git = {https://github.com/vtopt/DelaunaySparse},
	web = {https://vtopt.github.io/DelaunaySparse},
	keywords = {software, algorithms, delaunay triangulation, research, phd paper, watson paper},
}

% Paper on the challenges of integrating VTMOP into the libEnsemble parallel computing Python software library at Argonne
@inproceedings{chang2020managing,
	author = {Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T. and Lux, Thomas C. H.},
	title = {Managing computationally expensive blackbox multiobjective optimization problems using {libEnsemble}},
	year = {2020},
	booktitle = {Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20)},
	numpages = {31},
	organization = {SCS},
	location = {Fairfax, VA, USA},
	doi = {10.22360/SpringSim.2020.HPC.001},
	url = {https://dl.acm.org/doi/abs/10.5555/3408207.3408245},
}

% My PhD thesis, including multiobjective optimization techniques, algorithm, performance analysis, and software review; description of VTMOP, running parallel simulations, integrating with libE. Also scientific machine learning via Delaunay interpolation and algorithms and proofs for doing so. Several applications related to HPC performance modeling and autotuning.
@phdthesis{chang2020mathematical,
	author = {Chang, Tyler H.},
	title = {Mathematical Software for Multiobjective Optimization Problems},
	year = {2020},
	school = {Virginia Tech, Dept. of Computer Science},
	url = {https://vtechworks.lib.vt.edu/handle/10919/98915},
}

% A study on the multiobjective optimization of the LINPACK benchmark's config files on the leadership class HPC Bebop at Argonne National Laboratory. We used VTMOP but some modifications were required to ensure that mixed variables were properly handled. Some of the techniques that we used here inspired me to provide automatic support in ParMOO. Ultimately, we achieve 3x reduction in performance variability without sacrificing max/mean throughput.
@inproceedings{chang2020multiobjective,
	author = {Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T.},
	title = {Multiobjective optimization of the variability of the high-performance {LINPACK} solver},
	year = {2020},
	month = {12},
	booktitle = {Proc. 2020 Winter Simulation Conference (WSC 2020)},
	pages = {3081--3092},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC48552.2020.9383875},
	url = {https://ieeexplore.ieee.org/document/9383875/},
}

% Publication of my second open source numerical software package: VTMOP a Fortran software for solving blackbox multiobjective optimization problems. Uses surrogate modeling (RSM), with an adaptive weighting scheme, within a trust region framework. The motivating application is a particle accelerator tuning problem at SLAC
@article{chang2022algorithm,
	author = {Chang, Tyler H. and Watson, Layne T. and Larson, Jeffrey and Neveu, Nicole and Thacker, William I. and Deshpande, Shubhangi and Lux, Thomas C. H.},
	title = {{Algorithm 1028}: {VTMOP}: Solver for Blackbox Multiobjective Optimization Problems},
	year = {2022},
	month = {9},
	journal = {{ACM} Transactions on Mathematical Software},
	volume = {48},
	number = {3},
	numpages = {36},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3529258},
	url = {https://dl.acm.org/doi/10.1145/3529258},
	issn = {0098-3500},
	git = {https://github.com/Libensemble/libe-community-examples/tree/main/vtmop},
}

% Using ParMOO software with the MDML software (wrapper on Apache Kafka with automatic data logging and analysis dashboard) in order to optimize chemical manufacturing processes in a wet-lab environment. The kafka querries are sent directly to a continuous flow reactor (CFR) through a smart-lab setup in the MERF at Argonne. Through this setup, ParMOO is able to automatically steer the solvents, bases, temperatures, flow rates, and mixing ratios of a complex chemical manufacturing process in order to produce optimized yields and purities -- achieving multi-hundred-fold improvement over the previous manual process
@inproceedings{chang2023framework,
	author = {Chang, Tyler H. and Elias, Jakob R. and Wild, Stefan M. and Chaudhuri, Santanu and Libera, Joseph A.},
	title = {A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps},
	year = {2023},
	booktitle = {11th International Conference on Learning Representations (ICLR 2023), Workshop on Machine Learning for Materials (ML4Materials)},
	numpages = {10},
	location = {Kigali, Rwanda},
	url = {https://openreview.net/forum?id=8KJS7RPjMqG},
}

% The ParMOO JOSS article -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@article{chang2023parmoo,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {{ParMOO}: A {P}ython library for parallel multiobjective simulation optimization},
	year = {2023},
	month = {2},
	journal = {Journal of Open Source Software},
	volume = {8},
	number = {82},
	numpages = {4468},
	publisher = {The Open Journal},
	doi = {10.21105/joss.04468},
	url = {https://joss.theoj.org/papers/10.21105/joss.04468},
	issn = {2475-9066},
}

% This is the OJOC ParMOO repository DOI -- this is an archive of the software experiments for obtaining our test problems and reproducing our experimental results on those test problems with customized ParMOO solvers.
@misc{chang2024designing,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
	year = {2024},
	month = {3},
	booktitle = {INFORMS Journal on Computing},
	publisher = {INFORMS Journal on Computing},
	doi = {10.1287/ijoc.2023.0250.cd},
	url = {https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0250},
	issn = {1091-9856},
	note = {Available for download at https://github.com/INFORMSJoC/2023.0250},
}

% The ParMOO docs -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@techreport{chang2024parmoo,
	author = {Chang, Tyler H. and Wild, Stefan M. and Dickinson, Hyrum},
	title = {{ParMOO}: {P}ython library for parallel multiobjective simulation optimization},
	year = {2024},
	number = {Version 0.4.1},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://parmoo.readthedocs.io/en/latest},
}

% The ParMOO IJOC article describing the design of the ParMOO software, motivation, and providing examples of how ParMOO can be used to solve common scientific problems more efficiently with low effort -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@article{chang2025designing,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
	year = {2025},
	month = {3},
	journal = {INFORMS Journal on Computing},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2023.0250},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2023.0250},
	issn = {1091-9856},
}

% The funcX and updated Globus publication on the techniques and benefits of using a function-as-a-service (FaaS) framework to perform scientific experimentation at Argonne and other labs. funcX and Globus are scientific software products for performing distributed function evaluations and parallel computing that started at Argonne, and spun off into independent companies
@inproceedings{chard2020funcx,
	author = {Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and Foster, Ian and Chard, Kyle},
	title = {{funcX}: A federated function serving fabric for science},
	year = {2020},
	month = {6},
	booktitle = {Proc. 29th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '20)},
	pages = {65--76},
	organization = {ACM},
	location = {Stockholm, Sweden},
	doi = {10.1145/3369583.3392683},
	url = {https://dl.acm.org/doi/10.1145/3369583.3392683},
}

% Publication of our work on multiobjective shape optimization of the RF-gun cavity for the Argonne wakefield accelerator using ParMOO with the POISSON/SUPERFISH simulation software
@inproceedings{chen2023integrated,
	author = {Chen, Gongxiaohui and Chang, Tyler H. and Power, John and Jing, Chungunag},
	title = {An Integrated Multi-Physics Optimization Framework for Particle Accelerator Design},
	year = {2023},
	booktitle = {Proc. 2023 Winter Simulation Conference (WSC 2023), Industrial Applications Track},
	numpages = {2},
	location = {Orlando, FL, USA},
	doi = {10.48550/arXiv.2311.09415},
}

% The classic textbook by Cheney and Light on the fundamentals of approximation theory for multivariate functions -- topics include: basics of interpolation, approximation theory, and linear operators; multivariate polynomials, their interpolation nodes, and error kernels; selecting good polynomial interpolants via Newton and Lagrange type methods; positive-definite functions, kernel interpretations, and good kernels for interpolation; basis functions, orthonormal bases, common bases for interpolation and convergence rates; Chebyshev nodes; B-splines, Box splines, and thin-plate splines; and basics of artificial neural networks. Other topics include wavelets, orthogonal projection algorithms, Hilbert spaces, and reproducing kernel Hilbert spaces (RKHS).
@book{cheney2009course,
	author = {Cheney, Elliott W. and Light, William A.},
	title = {A Course in Approximation Theory},
	year = {2009},
	month = {1},
	booktitle = {Graduate Studies in Mathematics},
	series = {Graduate Studies in Mathematics},
	publisher = {AMS},
	address = {Providence, RI, USA},
	doi = {10.1090/gsm/101},
	url = {http://www.ams.org/gsm/101},
	isbn = {9780821847985},
	issn = {1065-7339},
}

% The Keras docs -- great and highly impactful open source Python software, needs no introduction. A simplified interface for quickly building neural networks and other deep learning models with various backends frameworks such as Tensorflow, jax, and Pytorch.
@misc{chollet2015keras,
	author = {Chollet, Fran\c{c}ois and others, },
	title = {Keras},
	year = {2015},
	howpublished = {\url{https://keras.io}},
}

% pyOED an open source Python numerical software library for performing optimal experimental design, e.g, for sensor placement at Argonne
@techreport{chowdhary2024pyoed,
	author = {Chowdhary, Abhijit and Ahmed, Shady E. and Attia, Ahmed},
	title = {{PyOED}: An Extensible Suite for Data Assimilation and Model-Constrained Optimal Design of Experiments},
	year = {2024},
	month = {6},
	booktitle = {ACM Transactions on Mathematical Software},
	volume = {50},
	number = {2},
	articleno = {11},
	numpages = {22},
	institution = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3653071},
	url = {https://doi.org/10.1145/3653071},
	issn = {0098-3500},
}

% An algorithm for multiobjective implicit filtering (MOIF). Not mentioned here, but the open source numerical software for MOIF on the author's GitHub is often cited via this paper
@article{cocchi2018implicit,
	author = {Cocchi, Guido and Liuzzi, Giampaolo and Papini, Alessandra and Sciandrone, Marco},
	title = {An implicit filtering algorithm for derivative-free multiobjective optimization with box constraints},
	year = {2018},
	month = {3},
	journal = {Computational Optimization and Applications},
	volume = {69},
	number = {2},
	pages = {267--296},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-017-9953-2},
	url = {http://link.springer.com/10.1007/s10589-017-9953-2},
	issn = {0926-6003},
}

% Building a multiobjective augmented Lagrangian -- basically, use a standard augmented Lagrangian penalty but apply it to all components of the objective. There is a proof that this will work. I don't use this method, but I use the same trick with the progressive barrier of Audet all the time and usually cite both papers
@article{cocchi2020augmented,
	author = {Cocchi, Guido and Lapucci, Matteo},
	title = {An augmented {Lagrangian} algorithm for multi-objective optimization},
	year = {2020},
	month = {9},
	journal = {Computational Optimization and Applications},
	volume = {77},
	number = {1},
	pages = {29--56},
	publisher = {Springer},
	doi = {10.1007/s10589-020-00204-z},
	url = {https://link.springer.com/10.1007/s10589-020-00204-z},
	issn = {0926-6003},
}

% Andrew Conn's landmark paper on interpolation dataset geometry -- leads to the definition of sets being "well-poised" for interpolation, meaning that when the interpolation set's geometry meats some local geometric conditions (basically bounded away from singularity), then the resulting interpolant's error (and gradient / hessian errors) can be bounded and the resulting models can be used to perform gradient descent or SQP within a trust-region framework with guaranteed convergence
@article{conn2008geometry,
	author = {Conn, Andrew R and Scheinberg, Katya and Vicente, Lu{\'\i}s N},
	title = {Geometry of interpolation sets in derivative free optimization},
	year = {2008},
	month = {6},
	journal = {Mathematical programming},
	volume = {111},
	number = {1-2},
	pages = {141--172},
	publisher = {Springer},
	doi = {10.1007/s10107-006-0073-5},
	url = {http://link.springer.com/10.1007/s10107-006-0073-5},
	issn = {0025-5610},
}

% Conn and Scheinberg book on DFO -- describes the geometry of good interpolation sets, linear and quadratic interpolants and how to use them for derivative-free gradient descent and SQP frameworks, bounds on interpolation and gradient errors of various models, and how to efficiently restore good geometry when the optimization algorithm samples points in a subspace
@book{conn2009introduction,
	author = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
	title = {Introduction to derivative-free optimization},
	year = {2009},
	month = {1},
	series = {MPS-SIAM Series on Optimization},
	publisher = {SIAM},
	address = {Philadelphia, PA, USA},
	doi = {10.1137/1.9780898718768},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898718768},
	isbn = {9780898716689},
}

% PyMOSO: an open source Python numerical software library for solving multiobjective simulation optimization problems with integer and discrete variables via direct search / pattern search like techniques
@article{cooper2020pymoso,
	author = {Cooper, Kyle and Hunter, Susan R.},
	title = {{PyMOSO}: {S}oftware for multi-objective simulation optimization with {R-PERLE} and {R-MinRLE}},
	year = {2020},
	month = {4},
	journal = {INFORMS Journal on Computing},
	volume = {32},
	number = {4},
	pages = {1101--1108},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2019.0902},
	url = {http://pubsonline.informs.org/doi/10.1287/ijoc.2019.0902},
	issn = {1091-9856},
}

% RBFOpt an open source library for solving single-objective blackbox optimization
@article{costa2018rbfopt,
	author = {Costa, Alberto and Nannicini, Giacomo},
	title = {{RBFOpt}: an open-source library for black-box optimization with costly function evaluations},
	year = {2018},
	month = {12},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {4},
	pages = {597--629},
	publisher = {Springer},
	doi = {10.1007/s12532-018-0144-7},
	url = {http://link.springer.com/10.1007/s12532-018-0144-7},
	issn = {1867-2949},
}

% ParEGO latest code and update introducing algorithmic updates, improved software quality, and (I think) some parallel computing -- ParEGO is the first open source numerical multiobjective bayesian optimization software package and written in C++. It is basically EGO (the original Bayesian optimization software using expected improvment acquisition) plus augmented Lagrangian scalarization
@inproceedings{cristescu2015surrogatebased,
	author = {Cristescu, Cristina and Knowles, Joshua},
	title = {Surrogate-based multiobjective optimization: {ParEGO} update and test},
	year = {2015},
	booktitle = {Workshop on Computational Intelligence (UKCI)},
	volume = {770},
	url = {https://www.cs.bham.ac.uk/~jdk/UKCI-2015.pdf},
	git = {https://github.com/CristinaCristescu/ParEGO_Eigen},
}

% Direct multisearch (DMS) is one of Custodio's earlier multiobjective direct search algorithms, which I think is a precursor to MutiGLODS. The numerical MATLAB software can be obtained by contacting her lab, but I'm not sure if they still distribute it as part of BoostDFO
@article{custodio2011direct,
	author = {Cust\'odio, Ana Lu\'isa and Madeira, Jose F. A. and Vaz, A. Ismael F. and Vicente, Lu\'is N.},
	title = {Direct Multisearch for Multiobjective Optimization},
	year = {2011},
	month = {7},
	journal = {SIAM Journal on Optimization},
	volume = {21},
	number = {3},
	pages = {1109--1140},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/10079731x},
	url = {http://epubs.siam.org/doi/10.1137/10079731X},
	issn = {1052-6234},
}

% The MultiGLODS numerical software package is written in MATLAB and used for solving multiobjective optimization problems via direct search / pattern search with a clever restart algorithm for selecting directions to explore in order obtain good coverage of the Pareto front. I believe this version also can use polynomial surrogates to pre-select good search directions and filter out unneeded blackbox function / simulation evaluations. It is now part of the BoostDFO MATLAB numerical software toolkit, obtainable from contacting Custodio
@article{custodio2018multiglods,
	author = {Cust{\'{o}}dio, Ana Lu\'isa and Madeira, Jose F. A.},
	title = {{MultiGLODS}: global and local multiobjective optimization using direct search},
	year = {2018},
	month = {10},
	journal = {Journal of Global Optimization},
	volume = {72},
	number = {2},
	pages = {323--345},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10898-018-0618-1},
	url = {http://link.springer.com/10.1007/s10898-018-0618-1},
	issn = {0925-5001},
}

% Applications of low-discrepancy sequences in Monte carlo simulation
@inproceedings{dalal2008low,
	author = {Dalal, Ishaan L. and Stefan, Deian and Harwayne-Gidansky, Jared},
	title = {Low discrepancy sequences for {M}onte {C}arlo simulations on reconfigurable platforms},
	year = {2008},
	month = {7},
	booktitle = {2008 International Conference on Application-Specific Systems, Architectures and Processors},
	volume = {},
	number = {},
	pages = {108--113},
	organization = {IEEE},
	location = {Leuven, Belgium},
	doi = {10.1109/ASAP.2008.4580163},
	url = {http://ieeexplore.ieee.org/document/4580163/},
}

% Description and proof of coverage for a quadratic scalarization scheme for scalarizing multiobjective optimization problems
@article{dandurand2016quadratic,
	author = {Dandurand, Brian and Wiecek, Margaret M.},
	title = {Quadratic scalarization for decomposed multiobjective optimization},
	year = {2016},
	month = {10},
	journal = {{OR} Spectrum},
	volume = {38},
	number = {4},
	pages = {1071--1096},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00291-016-0453-z},
	url = {http://link.springer.com/10.1007/s00291-016-0453-z},
	issn = {0171-6468},
}

% Dantzig's original (landmark) textbook on solving linear programming problems via the simplex method. This was obviously a landmark achievement in how to solve linear programming problems and more generally in the field of numerical optimization
@book{dantzig1998linear,
	author = {Dantzig, George B.},
	title = {Linear Programming and Extensions},
	year = {1998},
	series = {Princeton Landmarks in Mathematics and Physics},
	edition = {11},
	publisher = {Princeton University Press},
	address = {Princeton, NJ, USA},
	keywords = {},
}

% The normal boundary intersection (NBI) method was one of the first adaptive scalarization schemes for multiobjective optimization problems. It uses linear scalarization but does so adaptively using the angle of the intersecting vector at a target point to set the weights in order to adaptively fill in gaps on the Pareto front
@article{das1998normalboundary,
	author = {Das, Indraneel and Dennis, John E.},
	title = {Normal-boundary intersection: A new method for generating the {P}areto surface in nonlinear multicriteria optimization problems},
	year = {1998},
	month = {8},
	journal = {SIAM Journal on Optimization},
	volume = {8},
	number = {3},
	pages = {631--657},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/S1052623496307510},
	url = {http://epubs.siam.org/doi/10.1137/S1052623496307510},
	issn = {1052-6234},
}

% An early paper on using surrogate models to reduce the cost (in terms of true simulation / blackbox function evaluations) when using multiobjective evolutionary algorithm to solve computationally expensive blackbox and simulation optimization problems
@article{datta2016surrogateassisted,
	author = {Datta, Rituparna and Regis, Rommel G.},
	title = {A surrogate-assisted evolution strategy for constrained multi-objective optimization},
	year = {2016},
	month = {9},
	journal = {Expert Systems with Applications},
	volume = {57},
	pages = {270--284},
	publisher = {Elsevier BV},
	doi = {10.1016/j.eswa.2016.03.044},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417416301452},
	issn = {0957-4174},
}

% A technique for differentiating expected hypervolume improvement EHVI (and its monte carlo variant qEHVI), which can be used as the acquisition function for solving multiobjective blackbox optimization problems with BoTorch
@inproceedings{daulton2020differentiable,
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	title = {Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective {B}ayesian Optimization},
	year = {2020},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {9851--9864},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2020/file/6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf},
}

% The first paper on performing parallel Bayesian optimization using the expected hypervolume improvement acquisition function in BoTorch
@inproceedings{daulton2021parallel,
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	title = {Parallel {B}ayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement},
	year = {2021},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {34},
	pages = {2187--2200},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf},
}

% Interesting paper on why it is generally OK to use local optimizers when solving non convex optimization problems in high-dimensional spaces. In general, in high-dimensional spaces, almost every critical point will be a saddle point with high probability. Therefore, first-order methods tend to perform very well on these problems as they converge quickly but are not attracted to saddle points and therefore tend to find the global optimum in the limit. The analysis of the probability that a critical point will be a saddle point is based on a spectral analysis of the hessian at each critical point other than the global minimum/maximum -- all of the eigenvalues must be positive or negative for the critical point to be a local minima / maxima, and the probability of this occurring decays as the number of eigenvalues grows with the dimension of the Hessian. The authors also experimentally validate these claims by extracting critical points from the loss landscapes of single layer MLPs trained on down-sampled versions of MNIST and CIFAR-10.
@inproceedings{dauphin2014identifying,
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	year = {2014},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {27},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf},
}

% The exascale computing project press release
@misc{deanl2019press,
	title = {Press Release: {U.S.\ Department of Energy and Intel} to deliver first exascale supercomputer},
	year = {2019},
	month = {March},
	publisher = {{U.S.\ Department of Energy, Argonne National Laboratory}},
	url = {https://www.anl.gov/article/us-department-of-energy-and-intel-to-deliver-first-exascale-supercomputer},
	note = {accessed April 28, 2020},
	keywords = {},
}

% The original NSGA-II paper: a multiobjective evolutionary algorithm (MOEA) that scales well and performs extremely well in practice. The main contribution is a fast method for performing nondominated sorting so that the authors can ensure all efficient points persist in every generation. This method is generally considered to be the baseline in multiobjective optimization. While the algorithm is a simple heuristic that is extremely wasteful in terms of the number of true blackbox function / simulation evaluations, it performs extremely well in practice by the hypervolume indicator. I have found that it is very difficult to obtain better performance than NSGA-II on both benchmark and real-world problems in the limit, unless you have some "secret sauce" to exploit for your particular problem
@article{deb2002fast,
	author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwel, Sameer and Meyarivan, T.},
	title = {A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
	year = {2002},
	month = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {6},
	number = {2},
	pages = {182--197},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/4235.996017},
	url = {http://ieeexplore.ieee.org/document/996017/},
	issn = {1089-778X},
}

% The DTLZ test problems are the standard test problems used in all multiobjective evolutionary optimization papers. They are algebraic test problems that can scale to as many objectives and variables as one desires. Each problem also has a pathological property that makes it extremely difficult or degenerate for multiobjective optimization algorithms. This maeks the suite as a whole extremely convenient for testing, scaling, and evaluating results. However, several of the problems are so difficult that no solvers can reliably solve them. Additionally, all the problems have the property that the last "n" variables are essentially unused, with their optimum being 0.5 and not changing as we move across the Pareto front, which could be unrealistic for certain problems
@inproceedings{deb2002scalable,
	author = {Deb, Kalyanmoy and Thiele, Lothar and Laumanns, Marco and Zitzler, Eckart},
	title = {Scalable multi-objective optimization test problems},
	year = {2002},
	booktitle = {Proc. 2002 IEEE Congress on Evolutionary Computation (CEC '02)},
	volume = {1},
	pages = {825--830},
	organization = {IEEE},
	location = {Honolulu, HI, USA},
	doi = {10.1109/CEC.2002.1007032},
	url = {http://ieeexplore.ieee.org/document/1007032/},
}

% The orgiinal NSGA-III paper part 1: a multiobjective evolutionary algorithm similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many objectives by having the user provide a collection of well-spaced reference points and optimizing toward those
@article{deb2013evolutionary,
	author = {Deb, Kalyanmoy and Jain, Himanshu},
	title = {An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part {I}: solving problems with box constraints},
	year = {2013},
	month = {8},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {18},
	number = {4},
	pages = {577--601},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2013.2281535},
	url = {http://ieeexplore.ieee.org/document/6600851/},
	issn = {1089-778X},
}

% A study of various surrogate models for optimizing with WBCSim design tool which simulates and analyzes the behavior of wood-based composites: in summary, Shepard's method (locally linear) performed best in this analysis, and therefore was chosen as the main surrogate model in Deshpande's 2016 algorithm (above). It is hypothesized that Shepard's method's ability to extrapolate (typically an undesirable feature in interpolation) makes it good for surrogate modeling for optimization, which requires extrapolation based on local trends
@article{deshpande2011data,
	author = {Deshpande, Shubhangi and Watson, Layne T. and Shu, Jiang and Ramakrishnan, Naren},
	title = {Data driven surrogate-based optimization in the problem solving environment {WBCSim}},
	year = {2011},
	month = {7},
	journal = {Engineering with Computers},
	volume = {27},
	number = {3},
	pages = {211--223},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00366-010-0192-8},
	url = {http://link.springer.com/10.1007/s00366-010-0192-8},
	issn = {0177-0667},
}

% An algorithm for solving blackbox multiobjective optimization problems via trust region descent, using locally linear (shepard's method) surrogate models, and a multiobjective variant of DIRECT. Also, the authors propose a novel adaptive weighting scheme within the trust regions. The motivating application is an aircraft design optimization problem.
@article{deshpande2016multiobjective,
	author = {Deshpande, Shubhangi and Watson, Layne T. and Canfield, Robert A.},
	title = {Multiobjective optimization using an adaptive weighting scheme},
	year = {2016},
	month = {1},
	journal = {Optimization Methods and Software},
	volume = {31},
	number = {1},
	pages = {110--133},
	publisher = {Informa UK Limited},
	doi = {10.1080/10556788.2015.1048861},
	url = {http://www.tandfonline.com/doi/full/10.1080/10556788.2015.1048861},
	issn = {1055-6788},
}

% CVXPY is an open source Python optimization and modeling language for solving convex optimization problems in a disciplined way (meaning that we ensure convexity through hard rules on the problem definition). From the lab of Stephen Boyd
@article{diamond2016cvxpy,
	author = {Diamond, Steven and Boyd, Stephen},
	title = {{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization},
	year = {2016},
	journal = {Journal of Machine Learning Research},
	volume = {17},
	number = {83},
	pages = {1--5},
	url = {http://jmlr.org/papers/v17/15-408.html},
}

% A taxonomy of constraint types encountered when solving blackbox / simulation optimization: quantifiable vs nonquantifiable, relaxable vs unrelaxable, a priori vs simulation-based, and known vs hidden.
@techreport{digabel2024taxonomy,
	author = {Digabel}, S\'ebastien {Le and Wild, Stefan M.},
	title = {A Taxonomy of Constraints in Black-Box Simulation-Based Optimization},
	year = {2024},
	month = {6},
	booktitle = {Optimization and Engineering},
	volume = {25},
	number = {2},
	pages = {1125--1143},
	institution = {Springer Science and Business Media LLC},
	doi = {10.1007/s11081-023-09839-3},
	url = {https://link.springer.com/10.1007/s11081-023-09839-3},
	issn = {1389-4420},
}

% ECOS is an open source numerical software for solving second-order cone optimization problems, from the lab of Stephen Boyd. In my experience, this software is the best tool from Boyd's lab and the most robust to degeneracy
@inproceedings{domahidi2013ecos,
	author = {Domahidi, Alexander and Chu, Eric and Boyd, Stephen},
	title = {{ECOS}: {A}n {SOCP} solver for embedded systems},
	year = {2013},
	month = {7},
	booktitle = {European Control Conference (ECC)},
	pages = {3071--3076},
	organization = {IEEE},
	location = {Z{\"u}rich, Switzerland},
	doi = {10.23919/ECC.2013.6669541},
	url = {https://ieeexplore.ieee.org/document/6669541/},
}

% NAS-Bench-201 introduces the cell-based neural architecture search space representation (i.e., problem embedding) that was used by most (all) neural architecture search software circa ~2022. This includes the benchmark problems in JAHS-Bench-201
@inproceedings{dong2020nasbench201,
	author = {Dong, Xuanyi and Yang, Yi},
	title = {{NAS-Bench-201}: Extending the Scope of Reproducible Neural Architecture Search},
	year = {2020},
	booktitle = {8th International Conference on Learning Representations (ICLR 2020)},
	url = {https://openreview.net/forum?id=HJxyZkBKDr},
}

% Achieving performance portability of parallel codes in the exascale computing project (ECP) across various GPU-based architectures, which is challenging since different GPU vendors use different GPU programming libraries (e.g., CUDA for NVIDIA vs HIPP for AMD)
@article{dubey2021performance,
	author = {Dubey, Anshu and McInnes, Lois Curfman and Thakur, Rajeev and Draeger, Erik W. and Evans, Thomas and Germann, Timothy C. and Hart, William E.},
	title = {Performance Portability in the {E}xascale {C}omputing {P}roject: Exploration Through a Panel Series},
	year = {2021},
	month = {9},
	journal = {Computing in Science \& Engineering},
	volume = {23},
	number = {5},
	pages = {46--54},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/MCSE.2021.3098231},
	url = {https://ieeexplore.ieee.org/document/9495114/},
	issn = {1521-9615},
}

% The original paper for AdaGrad (adaptive subgradient method) which replaced the subgradient method with an adaptive estimate for the gradient, where each component of the gradient is rescaled by an adaptive estimate for the standard deviation in that direction based on previous iterates. This adaptive estimate for standard deviation in each axis-aligned direction serves as a diagonal approximation to the Hessian matrix, giving second-order like properties to the method and greatly improving the practical convergence. AdaGrad was very popular and considered the state-of-the-art optimization algorithm for training neural networks upon its initial release, but was quickly replaced by Adam, which added a Nesterov momentum esque smoothing to this adaptive gradient estimation in order to further improve convergence rates on nonsmooth, highly stochastic, and ill-conditioned problems
@article{duchi2011adaptive,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	year = {2011},
	journal = {Journal of Machine Learning Research},
	volume = {12},
	number = {61},
	pages = {2121--2159},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
}

% The JuMP modeling language in Julia -- a modeling language for modeling and solving linear and nonlinear programming (optimization) problems in Julia. The implementation is an open source numerical software
@article{dunning2017jump,
	author = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
	title = {{JuMP}: A Modeling Language for Mathematical Optimization},
	year = {2017},
	month = {1},
	journal = {SIAM Review},
	volume = {59},
	number = {2},
	pages = {295--320},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/15M1020575},
	url = {https://epubs.siam.org/doi/10.1137/15M1020575},
	issn = {0036-1445},
}

% jMetal is an open source numerical software library implementing multiobjective optimization solvers in Java. Last I checked, most of the solvers were heuristic methods such as evolutionary algorithms and/or simulated annealing. This is widely used in some fields of engineering
@article{durillo2011jmetal,
	author = {Durillo, Juan J. and Nebro, Antonio J.},
	title = {{jMetal}: A {J}ava framework for multi-objective optimization},
	year = {2011},
	month = {10},
	journal = {Advances in Engineering Software},
	volume = {42},
	number = {10},
	pages = {760--771},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2011.05.014},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0965997811001219},
	issn = {0965-9978},
}

% A careful analysis of Johnson-Lindenstrauss transforms, and how we can sample dimensions from hashing matrices to ensure that the resulting random subspace method converges into the true optimum in the limit
@article{dzahini2025class,
	author = {Dzahini, K. J. and Wild, S. M.},
	title = {A Class of Sparse {Johnson–Lindenstrauss} Transforms and Analysis of their Extreme Singular Values},
	year = {2025},
	month = {3},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {46},
	number = {1},
	pages = {416--438},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/23M1605661},
	url = {https://doi.org/10.1137/23M1605661},
	issn = {0895-4798},
}

% A classical textbook on the fundamentals of multiobjective optimization theory. Topics include: basic definitions in multiobjective optimization, partial orderings and cones and basic theories, linear scalarization and its theory and drawbacks, other scalarization methods and their theory and drawbacks, standard algorithms for common types of multiobjective optimization problems, and sample applications
@book{ehrgott2005multicriteria,
	author = {Ehrgott, Matthias},
	title = {Multicriteria Optimization},
	year = {2005},
	series = {Lecture Notes in Economics and Mathematical Systems Series},
	edition = {2},
	publisher = {Springer Verlag},
	address = {Heidelberg, Germany},
	doi = {10.1007/3-540-27659-9},
	url = {http://link.springer.com/10.1007/3-540-27659-9},
	isbn = {3540213988},
}

% The Pascoletti-Serafini scalarization and its variations, this method involves drawing a line through the target to reach various points on the Pareto front. It is effective with nonconvex Pareto fronts, but it is not adaptive and not commonly used in modern algorithms
@article{eichfelder2009scalarizations,
	author = {Eichfelder, Gabriele},
	title = {Scalarizations for adaptively solving multi-objective optimization problems},
	year = {2009},
	month = {11},
	journal = {Computational Optimization and Applications},
	volume = {44},
	number = {2},
	pages = {249--273},
	publisher = {Springer},
	doi = {10.1007/s10589-007-9155-4},
	url = {http://link.springer.com/10.1007/s10589-007-9155-4},
	issn = {0926-6003},
}

% The MDML open source software is a wrapper around Apache Kafka with protocols for fast data streaming and logging and dashboard generation. This framework was developed for usage at the material engineering research facility (MERF) at Argonne in order to facilitate the creation of a "smart lab" where MDML is the protocol for sending experiment requests to various equipment in the lab and logging results -- in an old (out-of-date branch) of ParMOO, this was a valid backend for launching simulation / experiment requests
@article{elias2020manufacturing,
	author = {Elias, Jakob R. and Chard, Ryan and Libera, Joseph A. and Foster, Ian T. and Chaudhuri, Santanu},
	title = {The Manufacturing Data and Machine Learning Platform: Enabling Real-time Monitoring and Control of Scientific Experiments via {IoT}},
	year = {2020},
	month = {6},
	journal = {2020 IEEE 6th World Forum on Internet of Things (WF-IoT)},
	pages = {1--2},
	publisher = {IEEE},
	address = {New Orleans, LA, USA},
	doi = {10.1109/WF-IoT48130.2020.9221078},
	url = {https://ieeexplore.ieee.org/document/9221078/},
	git = {GitHub: \url{https://github.com/anl-mdml/MDML_Client}},
}

% A nice survey paper on neural architecture search covering common search (i.e., architecture) space representations, search (i.e., optimization) strategies, and how to evaluate the performance of NAS methods
@article{elsken2019neural,
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	title = {Neural architecture search: A survey},
	year = {2019},
	journal = {The Journal of Machine Learning Research},
	volume = {20},
	number = {1},
	pages = {1997--2017},
	publisher = {JMLR.org},
}

% A chapter from an optimization textbook on performing multiobjective Bayesian optimization using the expected hypervolume improvement (EHVI) scalarization / acquisition
@inbook{emmerich2016multicriteria,
	author = {Emmerich, Michael and Yang, Kaifeng and Deutz, Andr{\'e} and Wang, Hao and Fonseca, Carlos M.},
	editor = {Pardalos, Panos M. and Zhigljavsky, Anatoly and {\v{Z}}ilinskas, Julius},
	title = {A Multicriteria Generalization of {Bayesian} Global Optimization},
	year = {2016},
	booktitle = {Advances in Stochastic and Deterministic Global Optimization},
	pages = {229--242},
	publisher = {Springer},
	doi = {10.1007/978-3-319-29975-4},
	url = {http://link.springer.com/10.1007/978-3-319-29975-4},
	isbn = {9783319299730},
	issn = {1931-6828},
}

% TURBO -- This is an open source numerical software for solving high-dimensional optimization problems via Bayesian optimization using BoTorch. Since Bayesian optimization performs poorly in high dimensions, they have resorted to applying a rudimentary trust region framework on top of their Bayesian optimization algorithm. By squeesing the trust region inward (standard practice in derivative-free optimization) they are able to force the Bayesian optimization algorithm to converge in a reasonable number of true blackbox function evaluations
@article{eriksson2019scalable,
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
	title = {Scalable global optimization via local bayesian optimization},
	year = {2019},
	journal = {Advances in Neural Information Processing Systems},
	volume = {32},
	pages = {1--12},
	publisher = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},
}

% Performing accuracy and latency aware neural architecture search via multiobjective optimization using BoTorch. While not part of the publication, the software is available open source on the BoTorch website
@inproceedings{eriksson2021latencyaware,
	author = {Eriksson, David and Chuang, Pierce I-Jen and Daulton, Samuel and Xia, Peng and Shrivastava, Akshat and Babu, Arun and Zhao, Shicong and Aly, Ahmed A and Venkatesh, Ganesh and Balandat, Maximilian},
	title = {Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization},
	year = {2021},
	booktitle = {8th ICML Workshop on Automated Machine Learning (AutoML)},
	url = {https://openreview.net/forum?id=0ciyfd4SvbI},
}

% Publication and whitepaper on Pathmind, an RL-based solver for simulation optimization problems. They also offer multiobjective support but only by using a priori scalarization provided by the user (so not real multiobjective support). This tool is not open source, it is a service provided by a YC startup of the same name. Therefore, it could be considered industry software
@inproceedings{farhan2020reinforcement,
	author = {Farhan, Mohammed and G{\"o}hre, Brett},
	title = {Reinforcement Learning in {AnyLogic} Simulation Models: A Guiding Example using {Pathmind}},
	year = {2020},
	month = {12},
	booktitle = {Proc. 2020 Winter Simulation Conference (WSC 2020)},
	pages = {3212--3223},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC48552.2020.9383916},
	url = {https://ieeexplore.ieee.org/document/9383916/},
}

% A biobjective ranking and selection algorithm from Hunter's NSF Career award
@article{feldman2018score,
	author = {Feldman, Guy and Hunter, Susan R.},
	title = {{SCORE} Allocations for Bi-objective Ranking and Selection},
	year = {2018},
	month = {1},
	journal = {ACM Transactions on Modeling Computer and Simulation},
	volume = {28},
	number = {1},
	articleno = {7},
	numpages = {28},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3158666},
	url = {https://dl.acm.org/doi/10.1145/3158666},
	issn = {1049-3301},
}

% A Bayesian optimization algorithm for solving constrained optimization problems that are both single and multiobjective -- the authors propose expected hypervolume improvement (EHVI) which merges expected improvement acquisition from Bayesian optimization with the hypervolume indicator for multiobjective optimization
@article{feliot2016bayesian,
	author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
	title = {A {B}ayesian approach to constrained single- and multi-objective optimization},
	year = {2016},
	month = {1},
	journal = {Journal of Global Optimization},
	volume = {67},
	number = {1-2},
	pages = {97--133},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10898-016-0427-3},
	url = {http://link.springer.com/10.1007/s10898-016-0427-3},
	issn = {0925-5001},
}

% Fletcher's paper on how to handle degeneracy in active set methods for solving quadratic programming problems. This technique would become the basis for the BQPD software package, which is a recently open source (post mortem) numerical software for solving (degenerate) quadratic programs via active set methods
@article{fletcher1993resolving,
	author = {Fletcher, Roger},
	title = {Resolving degeneracy in quadratic programming},
	year = {1993},
	month = {9},
	journal = {Annals of Operations Research},
	volume = {46},
	number = {2},
	pages = {307--334},
	publisher = {Springer},
	doi = {10.1007/BF02023102},
	url = {http://link.springer.com/10.1007/BF02023102},
	issn = {0254-5330},
}

% Fletcher's paper on an efficient Hessian update method which can be used in an active set method for solving quadratic programming optimization problems
@article{fletcher2000stable,
	author = {Fletcher, Roger},
	title = {Stable reduced Hessian updates for indefinite quadratic programming},
	year = {2000},
	month = {4},
	journal = {Mathematical programming},
	volume = {87},
	number = {2},
	pages = {251--264},
	publisher = {Springer},
	doi = {10.1007/s101070050113},
	url = {http://link.springer.com/10.1007/s101070050113},
	issn = {0025-5610},
}

% The DEAP framework is a Python framework for easily implementing and deploying parallel and distributed evolutionary algorithms. Fairly high quality open source software. This is widely used by optimization practitioners, e.g., engineers and scientists that read an evolutionary algorithm paper and want to try it out on their problem
@article{fortin2012deap,
	author = {Fortin, F\'elix-Antoine and {De Rainville}, Fran\c{c}ois-Michel and Gardner, Marc-Andr\'e and Parizeau, Marc and Gagn\'e ", Christian},
	title = {{DEAP}: Evolutionary Algorithms Made Easy},
	year = {2012},
	journal = {Journal of Machine Learning Research},
	volume = {13},
	number = {1},
	pages = {2171--2175},
	url = {https://www.jmlr.org/papers/v13/fortin12a.html},
}

% Landmark textbook on standard design patterns in software engineering, such as builders, factories, observers, etc. Nicknamed the "Gang of Four" book on design patters
@book{gamma1995design,
	author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
	title = {Design patterns: elements of reusable object-oriented software},
	year = {1995},
	publisher = {Addison-Wesley},
	address = {Reading, MA, USA},
	isbn = {0-201-63361-2},
}

% A community reviewed textbook on Bayesian optimization theory and implementation. Very thorough description of Gaussian process and Bayesian optimization fundamentals and theory, common techniques and acquisition functions, and implementation details, drawbacks, and real-world challenges
@book{garnett2023bayesian,
	author = {Garnett, Roman},
	title = {Bayesian Optimization},
	year = {2023},
	publisher = {Cambridge University Press},
	url = {https://bayesoptbook.com},
	isbn = {978-1108425780},
}

% A survey of static DOE sampling strategies for surrogate modeling, as well as a novel model-based strategy proposed by the authors
@article{garud2017smart,
	author = {Garud, Sushant Suhas and Karimi, Iftekhar A. and Kraft, Markus},
	title = {Smart Sampling Algorithm for Surrogate Model Development},
	year = {2017},
	month = {1},
	journal = {Computers \& Chemical Engineering},
	volume = {96},
	pages = {103--114},
	publisher = {Elsevier BV},
	doi = {10.1016/j.compchemeng.2016.10.006},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135416303210},
	issn = {0098-1354},
}

% Commonly cited lecture notes on using the Levenberg-Marquardt algorithm to solve least-squares curve-fitting (optimization) problems via a Gauss-Newton esque method
@techreport{gavin2019levenbergmarquardt,
	author = {Gavin, HP.},
	title = {The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
	year = {2019},
	institution = {Duke University, Department of Civil and Environmental Engineering},
}

% A review of data analysis and machine learning methods for earth system modeling
@article{geer2021learning,
	author = {Geer, Alan J.},
	title = {Learning earth system models from observations: machine learning or data assimilation?},
	year = {2021},
	month = {4},
	journal = {Philosophical Transactions of the Royal Society A},
	volume = {379},
	number = {2194},
	numpages = {20200089},
	publisher = {The Royal Society Publishing},
	doi = {10.1098/rsta.2020.0089},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0089},
	issn = {1364-503X},
}

% An article on the benefits of co design of algorithms and software and hardware in the context of the DOE's exascale computing project (ECP)
@article{germann2021codesign,
	author = {Germann, Timothy C.},
	title = {Co-design in the {Exascale Computing Project}},
	year = {2021},
	month = {11},
	journal = {The International Journal of High Performance Computing Applications},
	volume = {35},
	number = {6},
	pages = {503--507},
	publisher = {SAGE Publications Sage UK: London, England},
	doi = {10.1177/10943420211059380},
	url = {https://journals.sagepub.com/doi/10.1177/10943420211059380},
	issn = {1094-3420},
}

% Online paper with interactive visualizations explaining what Nesterov's momentum is and how it works intuitively by smoothing out optimization sample paths and preventing oscillations in the optimizer that occur do to poor problem conditioning. Then, they show how the problem conditioning appears as an often ignored constant in the convergence rate of gradient descent. All this is to show intuitively and mathematically that gradient descent with Nesterov's momentum will convergence faster in practice for ill-conditioned problems
@article{goh2017why,
	author = {Goh, Gabriel},
	title = {Why Momentum Really Works},
	year = {2017},
	month = {4},
	journal = {Distill},
	volume = {2},
	number = {4},
	publisher = {Distill Working Group},
	doi = {10.23915/distill.00006},
	url = {http://distill.pub/2017/momentum},
	issn = {2476-0757},
}

% Google's OSS Vizier service is a (now open source) blackbox / derivative-free optimization numerical software package and service. As far as I can tell, the package is primarily used for solving system optimization and A/B testing type problems
@inproceedings{golovin2017google,
	author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D.},
	title = {{Google Vizier}: A Service for Black-Box Optimization},
	year = {2017},
	month = {8},
	booktitle = {Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17)},
	pages = {1487--1495},
	organization = {ACM},
	location = {Halifax, NS, Canada},
	doi = {10.1145/3097983.3098043},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098043},
}

% Classical textbook that serves as the "bible" of matrix computations and computational linear algebra -- contains all the standard factorizations, the common algorithms for computing them, and their sensitiviy analyses, pivoting, some basic approximation theory, and the basics of iterative methods
@book{golub2013matrix,
	author = {Golub, Gene H. and Van Loan, Charles F.},
	title = {Matrix computations},
	year = {2013},
	edition = {4th},
	publisher = {Johns Hopkins University Press},
	doi = {10.56021/9781421407944},
	url = {https://www.press.jhu.edu/books/title/10678/matrix-computations},
	isbn = {978-1421407944},
	keywords = {},
}

% Theorems on the curse of dimensionality when it comes to drawing data points in high-dimensional spaces. The main theorem implies that the convex hull of N points in D dimensions has volume ~0 for D sufficiently large -- this occurs because of a concentration of measure type result
@article{gorban2017stochastic,
	author = {Gorban, Alexander N and Tyukin, Ivan Yu},
	title = {Stochastic separation theorems},
	year = {2017},
	month = {10},
	journal = {Neural Networks},
	volume = {94},
	pages = {255--259},
	publisher = {Elsevier},
	doi = {10.1016/j.neunet.2017.07.014},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017301776},
	issn = {0893-6080},
}

% The OpenMDAO open source numerical software library for modeling and solving multidisciplinary engineering design optimization problems. Combines surrogate modeling, gradient based optimization, parallel computing frameworks, and derivative-free optimization techniques in one package so in order to solve large mixed-variable blackbox optimization problems. Developed by NASA Glenn
@article{gray2019openmdao,
	author = {Gray, Justin S. and Hwang, John T. and Martins, Joaquim R.R.A. and Moore, Kenneth T. and Naylor, Bret A.},
	title = {{OpenMDAO}: An open-source framework for multidisciplinary design, analysis, and optimization},
	year = {2019},
	month = {4},
	journal = {Structural and Multidisciplinary Optimization},
	volume = {59},
	number = {4},
	pages = {1075--1104},
	publisher = {Springer},
	doi = {10.1007/s00158-019-02211-z},
	url = {http://link.springer.com/10.1007/s00158-019-02211-z},
	issn = {1615-147X},
}

% Platypus: an open source numerical software package for performing multiobjective optimization in Python and comparing results
@techreport{hadka2015platypus,
	author = {Hadka, David},
	title = {Platypus -- multiobjective optimization in {P}ython},
	year = {2015},
	number = {Version 1.0.4},
	institution = {GitHub},
	url = {https://platypus.readthedocs.io/en/latest},
}

% Hanson's Fortran numerical software for solving equality constrained nonnegative least-squares (NNLS) problems via an iterative weighted least squares (WNNLS) solver. This is the default constrained least-squares optimization problem solver in the Fortran library SLATEC from Sandia
@article{hanson1982algorithm,
	author = {Hanson, Richard J. and Haskell, Karen H.},
	title = {Algorithm 587: Two Algorithms for the Linearly Constrained Least Squares Problem},
	year = {1982},
	month = {9},
	journal = {ACM Trans. Math. Softw.},
	volume = {8},
	number = {3},
	pages = {323--333},
	publisher = {ACM},
	address = {New York, NY, USA},
	doi = {10.1145/356004.356010},
	url = {https://dl.acm.org/doi/10.1145/356004.356010},
	issn = {0098-3500},
}

% The official publication of the open source numerical software numpy: the standard for basic multivariable computations, vector operations, and simple linear algebra in Python
@article{harris2020array,
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, St{\'{e}}fan J. van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and R{\'{i}}o, Jaime Fern{\'{a}}ndez del and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	title = {Array programming with {NumPy}},
	year = {2020},
	month = {9},
	journal = {Nature},
	volume = {585},
	number = {7825},
	pages = {357--362},
	publisher = {Springer Science and Business Media {LLC}},
	doi = {10.1038/s41586-020-2649-2},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	issn = {0028-0836},
}

% The official textbook on the Pyomo modeling language: an open source optimization modeling language and scientific software developed at Sandia by Bill Hart et al. Pyomo is a standard for solving large-scale mathematical programming (linear and nonlinear optimization) problems in Python
@book{hart2017pyomo,
	author = {Hart, William E. and Laird, Carl D. and Watson, Jean-Paul and Woodruff, David L. and Hackebeil, Gabriel A. and Nicholson, Bethany L. and Siirola, John D.},
	title = {Pyomo -- optimization modeling in {P}ython},
	year = {2017},
	booktitle = {Springer Optimization and Its Applications},
	series = {Springer Optimization and Its Applications},
	edition = {2},
	publisher = {Springer Cham},
	address = {Cham, Switzerland},
	doi = {10.1007/978-3-319-58821-6},
	url = {http://link.springer.com/10.1007/978-3-319-58821-6},
	isbn = {9783319588193},
	issn = {1931-6828},
}

% Chimera: a scientific software package for steering self-driving labs via multiobjective optimization -- the software is similar to what we did with ParMOO + MDML in the MERF at Argonne. They focus on applications in robot calibration and molecular system design. The multiobjective component helps them to explore the solution space with experimentation
@article{hase2018chimera,
	author = {H{\"a}se, Florian and Roch, Lo{\"\i}c M and Aspuru-Guzik, Al{\'a}n},
	title = {Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories},
	year = {2018},
	journal = {Chemical science},
	volume = {9},
	number = {39},
	pages = {7642--7655},
	publisher = {Royal Society of Chemistry},
	doi = {10.1039/C8SC02239A},
	url = {https://xlink.rsc.org/?DOI=C8SC02239A},
	issn = {2041-6520},
}

% A survey paper on multiobjective reinforcement learning -- RL is basically optimization with the addition of a dynamically changing state variable, so this is sort of relevant to multiobjecte optimization research
@article{hayes2022practical,
	author = {Hayes, Conor F and R{\u{a}}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M and Dazeley, Richard and Heintz, Fredrik and others, },
	title = {A practical guide to multi-objective reinforcement learning and planning},
	year = {2022},
	month = {4},
	journal = {Autonomous Agents and Multi-Agent Systems},
	volume = {36},
	number = {1},
	pages = {1--59},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10458-022-09552-y},
	url = {https://link.springer.com/10.1007/s10458-022-09552-y},
	issn = {1387-2532},
}

% VTDIRECT95 reference: a high-performance parallel Fortran implementation of the famous single-objective blackbox (direct search) optimization algorithm DIRECT. The numerical software is now open source (maintained by me) on Dr. Watson's GitHub page.
@article{he2009algorithm,
	author = {He, Jian and Watson, Layne T. and Sosonkina, Masha},
	title = {Algorithm 897: {VTDIRECT95}: Serial and Parallel Codes for the Global Optimization Algorithm {DIRECT}},
	year = {2009},
	month = {7},
	journal = {ACM Transactions on Mathematical Software},
	volume = {36},
	number = {3},
	numpages = {17},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1527286.1527291},
	url = {https://dl.acm.org/doi/10.1145/1527286.1527291},
	issn = {0098-3500},
}

% Studying the parallel performance of VTDIRECT95 at a massive scale: in summary VTDIRECT95 scales very well after the initial "warmup" period since there are not many boxes in the first couple iterations
@article{he2009performance,
	author = {He, Jian and Verstak, Alex and Watson, Layne T. and Sosonkina, Masha},
	title = {Performance modeling and analysis of a massively parallel {DIRECT} -- part 1},
	year = {2009},
	month = {2},
	journal = {The International Journal of High Performance Computing Applications},
	volume = {23},
	number = {1},
	pages = {14--28},
	publisher = {SAGE Publications},
	doi = {10.1177/1094342008098463},
	url = {https://journals.sagepub.com/doi/10.1177/1094342008098463},
	issn = {1094-3420},
	keywords = {},
}

% The better scientific software tech report, with recommendations and rules of thumb for improving the quality of scientific software within the DOE
@techreport{heroux2020advancing,
	author = {Heroux, Michael A. and McInnes, Lois and Bernholdt, David E. and Dubey, Anshu and Gonsiorowski, Elsa and Marques, Osni and Moulton, J. David and Norris, Boyana and Raybourn, Elaine and Balay, Satish and Bartlett, Roscoe A. and Childers, Lisa and Gamblin, Todd and Grubel, Patricia and Gupta, Rinku and Hartman-Baker, Rebecca and Hill, Judith C. and Hudson, Stephen and Junghans, Christoph and Klinvex, Alicia and Milewicz, Reed and Miller, Mark and Ah Nam, Hai and O'Neal, Jared and Riley, Katherine and Sims, Ben and Schuler, Jean and Smith, Barry F. and Vernon, Louis and Watson, Gregory R. and Willenbring, James and Wolfenbarger, Paul},
	title = {Advancing Scientific Productivity through Better Scientific Software: Developer Productivity and Software Sustainability Report},
	year = {2020},
	month = {1},
	number = {ORNL TM-2020 1459 / ECP-U-RPT-2020-0001},
	institution = {Oak Ridge National Laboratory},
	address = {Oak Ridge, TN, USA},
	doi = {10.2172/1606662},
	url = {https://www.osti.gov/servlets/purl/1606662/},
}

% A numerical-quadrature based approximation to discrepancy. For high-dimensional ill-spaced points, the quadrature error can be huge, and although discrepancies should always be between 0 and 1, the approximation can approach (13/12)^d - 1, where d is the dimension of the problem. When points are randomly or quasi-randomly sampled, such large values of the approximation could be indicative of measure collapse. This is the technique used in scipy.stats.qmc.discrepancy(...)
@article{hickernell1998generalized,
	author = {Hickernell, Fred J.},
	title = {A generalized discrepancy and quadrature error bound},
	year = {1998},
	journal = {Mathematics of computation},
	volume = {67},
	number = {221},
	pages = {299--322},
	publisher = {American Mathematical Society (AMS)},
	doi = {10.1090/S0025-5718-98-00894-1},
	url = {https://www.ams.org/mcom/1998-67-221/S0025-5718-98-00894-1/},
	issn = {0025-5718},
}

% An article on Google GlassBox research: Google's research division dedicated to interpretable machine learning
@article{hof2015google,
	author = {Hof, Robert D.},
	title = {Google Tries to Make Machine Learning a Little More Human},
	year = {2015},
	month = {nov},
	journal = {MIT Technology Review},
	url = {https://www.technologyreview.com/2015/11/05/165175/google-tries-to-make-machine-learning-a-little-more-human/},
	note = {Last accessed: June 20, 2022},
}

% Techniques for optimizing molecule properties via a latent-space embedding that comes from a molecule autoencoder, from IBM Research
@article{hoffman2022optimizing,
	author = {Hoffman, Samuel C. and Chenthamarakshan, Vijil and Wadhawan, Kahini and Chen, Pin-Yu and Das, Payel},
	title = {Optimizing molecules using efficient queries from property evaluations},
	year = {2022},
	month = {12},
	journal = {Nature Machine Intelligence},
	volume = {4},
	number = {1},
	pages = {21--31},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s42256-021-00422-y},
	url = {https://www.nature.com/articles/s42256-021-00422-y},
	issn = {2522-5839},
}

% The official publication for HiGHS numerical optimization open source software package and solver for linear programming problems. The first successful effort to parallelize a simplex method based solver, specifically, they have successfully parallelized the dual revised simplex approach. HiGHS is now the default solver in most scipy.optimize. HiGHS is written in C++ with Fortran, C, and Python interfaces -- support CPU and GPU parallelism, the authors don't get perfect scaling by any means but this is the first successful effort to parallelize linear programming and still an achievement
@article{huangfu2018parallelizing,
	author = {Huangfu, Qi and Hall, JA Julian},
	title = {Parallelizing the dual revised simplex method},
	year = {2018},
	month = {3},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {1},
	pages = {119--142},
	publisher = {Springer},
	doi = {10.1007/s12532-017-0130-5},
	url = {http://link.springer.com/10.1007/s12532-017-0130-5},
	issn = {1867-2949},
}

% The original libEnsemble publication focusing on its techniques for distributing and evaluating ensembles of functions in parallel. Although not discussed, libEnsemble was already open source at the time
@article{hudson2022libensemble,
	author = {Hudson, Stephen and Larson, Jeffrey and Navarro, John-Luke and Wild, Stefan M.},
	title = {{libEnsemble}: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations},
	year = {2022},
	month = {4},
	journal = {{IEEE} Transactions on Parallel and Distributed Systems},
	volume = {33},
	number = {4},
	pages = {977--988},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/tpds.2021.3082815},
	url = {https://ieeexplore.ieee.org/document/9439163/},
	issn = {1045-9219},
}

% The official JOSS paper for libEnsemble: an open source software library for performing parallel and distributed computations involving ensembles of computationally expensive function evaluations in Python
@techreport{hudson2023libensemble,
	author = {Hudson, Stephen and Larson, Jeffrey and Navarro, John-Luke and Wild, Stefan M.},
	title = {{libEnsemble: A} complete {Python} toolkit for dynamic ensembles of calculations},
	year = {2023},
	month = {12},
	booktitle = {Journal of Open Source Software},
	volume = {8},
	number = {92},
	numpages = {6031},
	institution = {The Open Journal},
	doi = {10.21105/joss.06031},
	url = {https://joss.theoj.org/papers/10.21105/joss.06031},
	issn = {2475-9066},
}

% A thorough survey on techniques and algorithms for solving multiobjective simulation optimization problems, including algorithms and techniques for handling small finite design spaces, discrete integer design spaces, and continuous design spaces. Covers theory, algorithms, and popular heuristics for all.
@article{hunter2019introduction,
	author = {Hunter, Susan R. and Applegate, Eric A. and Arora, Viplove and Chong, Bryan},
	title = {An introduction to multiobjective simulation optimization},
	year = {2019},
	month = {1},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	volume = {29},
	number = {1},
	pages = {1--36},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3299872},
	url = {https://dl.acm.org/doi/10.1145/3299872},
	issn = {1049-3301},
}

% ISO Fortran 2003 software standard -- definition of the Fortran 2003 standard
@techreport{ios2004information,
	title = {{Information technology -- Programming languages -- Fortran -- Part 1: Base Language}},
	year = {2004},
	month = {November},
	number = {ISO/IEC 1539-1:2004(E)},
	institution = {International Organization for Standardization},
	address = {Geneva, Switzerland},
	keywords = {},
}

% ISO Fortran 2008 software standard -- definition of the Fortran 2008 standard
@techreport{ios2010information,
	title = {{Information technology -- Programming languages -- Fortran -- Part 1: Base Language}},
	year = {2010},
	month = {October},
	number = {ISO/IEC 1539-1:2010(E)},
	institution = {International Organization for Standardization},
	address = {Geneva, Switzerland},
	keywords = {},
}

% The orgiinal NSGA-III paper part 2: a multiobjective evolutionary algorithm similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many objectives by having the user provide a collection of well-spaced reference points and optimizing toward those -- this paper focuses on how to handle constraints
@article{jain2013evolutionary,
	author = {Jain, Himanshu and Deb, Kalyanmoy},
	title = {An evolutionary many-objective optimization algorithm using reference-point based nondominated sorting approach, part {II}: Handling constraints and extending to an adaptive approach},
	year = {2013},
	month = {8},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {18},
	number = {4},
	pages = {602--622},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2013.2281534},
	url = {http://ieeexplore.ieee.org/document/6595567/},
	issn = {1089-778X},
}

% A modification to the original Sobol sequence generator code
@article{joe2003remark,
	author = {Joe, Stephen and Kuo, Frances Y.},
	title = {Remark on Algorithm 659: Implementing Sobol's Quasirandom Sequence Generator},
	year = {2003},
	month = {3},
	journal = {ACM Transactions on Mathematical Software},
	volume = {29},
	number = {1},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/641876.641879},
	url = {https://dl.acm.org/doi/10.1145/641876.641879},
	issn = {0098-3500},
}

% The numerical software algorithm used in scipy algorithm for generating Sobol sequences (low discrepancy sequences)
@article{joe2008constructing,
	author = {Joe, Stephen and Kuo, Frances Y.},
	title = {Constructing Sobol sequences with better two-dimensional projections},
	year = {2008},
	month = {1},
	journal = {SIAM Journal on Scientific Computing},
	volume = {30},
	number = {5},
	pages = {2635--2654},
	publisher = {SIAM},
	doi = {10.1137/070709359},
	url = {http://epubs.siam.org/doi/10.1137/070709359},
	issn = {1064-8275},
}

% Original publication on maximin and minimax designs for design-of-experiments. I.e., minimize the maximum distance from any point in the bounding box to the nearest point in the design, and maximize the minimum distance between any pair of points in the design.
@article{johnson1990minimax,
	author = {Johnson, M.E. and Moore, L.M. and Ylvisaker, D.},
	title = {Minimax and maximin distance designs},
	year = {1990},
	month = {10},
	journal = {Journal of Statistical Planning and Inference},
	volume = {26},
	number = {2},
	pages = {131--148},
	publisher = {Elsevier BV},
	doi = {10.1016/0378-3758(90)90122-B},
	url = {https://linkinghub.elsevier.com/retrieve/pii/037837589090122B},
	issn = {0378-3758},
}

% D.R. Jones' original paper on the landmark DIRECT (DIviding RECTangles) algorithm for direct search global blackbox optimization. The idea is that you can perform branch-and-bound style Lipschitzian optimization without knoweldge of the Lipschitz constant by dividing a rectangular design space into rectangular regions (rectangles) and subdividing those rectangles that could be potentially optimal given any Lipschitz constant by choosing those boxes on the lower left convex hull of the objective value at the center vs box diameter scatter plot
@article{jones1993lipschitzian,
	author = {Jones, Donald R. and Perttunen, Cary D. and Stuckman, Bruce E.},
	title = {Lipschitzian optimization without the Lipschitz constant},
	year = {1993},
	month = {10},
	journal = {Journal of optimization Theory and Applications},
	volume = {79},
	number = {1},
	pages = {157--181},
	publisher = {Springer},
	doi = {10.1007/bf00941892},
	url = {http://link.springer.com/10.1007/BF00941892},
	issn = {0022-3239},
}

% D.R. Jones' original paper on "efficient global optimization" (EGO). This is often credited as the original implementation of multivariate Bayesian optimization. Jones proposes using Gaussian processes to produce a Gaussian posterior, whose expected improvement function can be efficiently optimized to select the next candidate. This is also one of the early works in sequential optimization via a generic (i.e., non polynomial) surrogate model. However, earlier work on design-of-experiments and response surface modeling did exist in the engineering design optimization space. Earlier papers had explored the idea of optimizing Gaussian processes to select experiments (especially in one and two-dimensions). However, this paper is a landmark in that it gave rise to the field of multivariate Bayesian optimization. EGO is still often used as the benchmark Bayesian optimization algorithm.
@article{jones1998efficient,
	author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
	title = {Efficient global optimization of expensive black-box functions},
	year = {1998},
	journal = {Journal of Global optimization},
	volume = {13},
	pages = {455--492},
	publisher = {Springer},
}

% Introducing Dragonfly: an open source numerical software package for solving neural architecture search problems via Bayesian optimization and solving an optimal transport problem to evaluate the distance between two networks. Considered a bit of a landmark paper for neural network architecture search problems. The open source Python software is widely used for a variety of applications outside NAS, including molecular discovery
@article{kandasamy2020tuning,
	author = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabus and Xing, Eric P.},
	title = {Tuning Hyperparameters without Grad Students: Scalable and Robust {Bayesian} Optimisation with {Dragonfly}},
	year = {2020},
	journal = {Journal of Machine Learning Research},
	volume = {21},
	number = {81},
	pages = {1--27},
	url = {http://jmlr.org/papers/v21/18-223.html},
	git = {http://github.com/dragonfly/dragonfly},
}

% A survey of multiobjective optimization algorithms for hyperparameter tuning in the context of automatic machine learning (autoML)
@article{karl2023multiobjective,
	author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
	title = {Multi-Objective Hyperparameter Optimization in Machine Learning -- An Overview},
	year = {2023},
	month = {12},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	volume = {3},
	number = {4},
	pages = {1--50},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3610536},
	url = {https://dl.acm.org/doi/10.1145/3610536},
	issn = {2688-299X},
}

% A review of physics-informed machine learning techniques and models by Karniadakis himself, who is credited with starting the field of SciML
@article{karniadakis2021physicsinformed,
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	title = {Physics-informed machine learning},
	year = {2021},
	month = {5},
	journal = {Nature Reviews Physics},
	volume = {3},
	number = {6},
	pages = {422--440},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s42254-021-00314-5},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	issn = {2522-5820},
}

% An introductory-level paper to theory-guided data science. I haven't read but I assume it covers the same topics he taught in his class at Virginia Tech
@article{karpatne2017theoryguided,
	author = {Karpatne, Anuj and Atluri, Gowtham and Faghmous, James H. and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
	title = {Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data},
	year = {2017},
	month = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	volume = {29},
	number = {10},
	pages = {2318--2331},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TKDE.2017.2720168},
	url = {http://ieeexplore.ieee.org/document/7959606/},
	issn = {1041-4347},
}

% Proposes a new technique called manifold sampling to solve blackbox optimization problems where a smooth blackbox function is composed with a piecewise linear (non blackbox) function. Normally, this would create a nonsmooth blackbox function, but by modeling the blackbox function separately and sampling the different manifolds produced by the changes in active components of the piecewise function, we can still model the blackbox function with smooth techniques and solve the optimization problem efficiently
@article{khan2018manifold,
	author = {Khan, Kamil A. and Larson, Jeffrey and Wild, Stefan M.},
	title = {Manifold Sampling for Optimization of Nonconvex Functions that are Piecewise Linear Compositions of Smooth Components},
	year = {2018},
	month = {1},
	journal = {{SIAM} Journal on Optimization},
	volume = {28},
	number = {4},
	pages = {3001--3024},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/17m114741x},
	url = {https://epubs.siam.org/doi/10.1137/17M114741X},
	issn = {1052-6234},
}

% The SPEA2+ algorithm for solving multiobjective optimization problems with evolutionary algorithms. Apparently this is widely-used numerical software, but I can't find the download
@inproceedings{kim2004spea2+,
	author = {Kim, Mifa and Hiroyasu, Tomoyuki and Miki, Mitsunori and Watanabe, Shinya},
	title = {{SPEA2+}: Improving the performance of the {S}trength {P}areto {E}volutionary {A}lgorithm 2},
	year = {2004},
	booktitle = {Proc. International Conference on Parallel Problem Solving from Nature (PPSN VIII)},
	pages = {742--751},
	organization = {Springer},
	location = {Birmingham, UK},
	isbn = {978-3-540-30217-9_75},
}

% The original paper on Adam: an adaptive gradient and moment estimator that uses second order moments to approximate curvature (i.e., Hessian information) in order to accelerate the convergence of AdaGrad. In particular, this means applying Nesterov's momentum to both the gradient and curvature estimations. From 2015-2024 this was the state-of-the-art algorithm for optimization of neural network weights during training, and was what was typically meant when people talked about stochastic gradient descent.
@inproceedings{kingma2015adam,
	author = {Kingma, Diedrik and Ba, Jimmy},
	title = {Adam: A method for stochastic optimization},
	year = {2015},
	booktitle = {3rd International Conference on Learning Representations (ICLR 2015)},
	numpages = {11},
	location = {San Diego, CA, USA},
	url = {https://arxiv.org/abs/1412.6980},
}

% The Klee Minty cube: A famous counterexample showing that for every pivoting strategy for the simplex method, we can construct a pathological problem where that strategy will visit every vertex of the cube before the solution. This proves that the simplex method cannot be used to solve linear programming problems in strongly polynomial time
@article{klee1972how,
	author = {Klee, Victor and Minty, George J.},
	title = {How good is the simplex algorithm?},
	year = {1972},
	journal = {Inequalities},
	volume = {III},
	pages = {159--175},
	keywords = {},
}

% The original ParEGO algorithm paper: in each iteration of the algorithm, the authors use NSGA-II to optimize the expected improvement of the Gaussian process surrogates of each objective. Then, results are scalarized using augmented chebyshev and the best results are evaluated for the next iteration
@article{knowles2006parego,
	author = {Knowles, Joshua},
	title = {{ParEGO:} A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
	year = {2006},
	month = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {8},
	number = {5},
	pages = {1341--66},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/tevc.2005.851274},
	url = {http://ieeexplore.ieee.org/document/1583627/},
	issn = {1089-778X},
}

% The SORCER software is a service oriented computing environment used by the US Airforce research lab (AFRL) to distribute expensive computations (such as design optimizations) across their large distributed network of computing resources
@inproceedings{kolonay2011service,
	author = {Kolonay, Raymond M. and Sobolewski, Michael},
	title = {Service oriented computing environment ({SORCER}) for large scale, distributed, dynamic fidelity aeroelastic analysis},
	year = {2011},
	booktitle = {International Forum on Aeroelasticity and Structural Dynamics (IFASD 2011), Optimization},
	pages = {26--30},
	organization = {Citeseer},
	location = {Paris, France},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.656.7539},
}

% The textbook on discrepancy that Manisha used to learn about low discrepancy sequences and their motivation
@book{kuipers1974uniform,
	author = {Kuipers, L. and Niederreiter, H.},
	title = {Uniform distribution of sequences},
	year = {1974},
	series = {Pure and Applied Mathematics},
	pages = {xiv--390},
	publisher = {Wiley-Interscience [John Wiley \& Sons], New York-London-Sydney},
}

% Stochastic approximation algorithm (i.e., stochastic gradient descent) and how to analyze its radius of convergence for a fixed step-size -- you can decay its step size at a square-summable but not summable rate to guarantee convergence in the limit
@article{lai2003stochastic,
	author = {Lai, Tze Leung},
	title = {Stochastic approximation},
	year = {2003},
	journal = {The annals of Statistics},
	volume = {31},
	number = {2},
	pages = {391--406},
	publisher = {Institute of Mathematical Statistics},
}

% The APOSMM Python software is a framework for implementing multistart derivative-free optimization algorithms and running them in asynchronously
@article{larson2018asynchronously,
	author = {Larson, Jeffrey and Wild, Stefan M},
	title = {Asynchronously parallel optimization solver for finding multiple minima},
	year = {2018},
	month = {9},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {3},
	pages = {303--332},
	publisher = {Springer},
	doi = {10.1007/s12532-017-0131-4},
	url = {http://link.springer.com/10.1007/s12532-017-0131-4},
	issn = {1867-2949},
}

% A thorough survey of techniques and algorithms in derivative-free optimization
@article{larson2019derivativefree,
	author = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
	title = {Derivative-free optimization methods},
	year = {2019},
	month = {5},
	journal = {Acta Numerica},
	volume = {28},
	pages = {287--404},
	publisher = {Cambridge University Press},
	doi = {10.1017/S0962492919000060},
	url = {https://www.cambridge.org/core/product/identifier/S0962492919000060/type/journal_article},
	issn = {0962-4929},
}

% Jeff's GOOMBAH paper on exploiting composite structures in derivative-free optimization, meaning that a blackbox function is composed with an algebraic function and we want to optimize the result, while exploiting the fact that we know the equation of the algebraic function. Similar technique is used in ParMOO to exploit this same structure and others that are specific to the multiobjective case
@article{larson2024structureaware,
	author = {Larson, Jeffrey and Menickelly, Matt},
	title = {Structure-Aware Methods for Expensive Derivative-Free Nonsmooth Composite Optimization},
	year = {2024},
	month = {3},
	journal = {Mathematical Programming Computation},
	volume = {16},
	number = {1},
	pages = {1--36},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s12532-023-00245-5},
	url = {https://link.springer.com/10.1007/s12532-023-00245-5},
	issn = {1867-2949},
}

% A summary of classical solutions and challenges to solving sum-of-squares minimization of polynomials
@article{lasserre2001global,
	author = {Lasserre, Jean B},
	title = {Global optimization with polynomials and the problem of moments},
	year = {2001},
	month = {1},
	journal = {SIAM Journal on optimization},
	volume = {11},
	number = {3},
	pages = {796--817},
	publisher = {SIAM},
	doi = {10.1137/s1052623400366802},
	url = {http://epubs.siam.org/doi/10.1137/S1052623400366802},
	issn = {1052-6234},
}

% An adaptive scheme for selecting epsilon-constraint scalarizations when solving multiobjective optimization problems.
@article{laumanns2006efficient,,
	author = {Laumanns, Marco and Thiele, Lothar and Zitzler, Eckart},
	title = {An efficient, adaptive parameter variation scheme for metaheuristics based on the epsilon-constraint method},
	year = {2006},
	month = {3},
	journal = {European Journal of Operational Research},
	volume = {169},
	number = {3},
	pages = {932--942},
	publisher = {Elsevier},
	doi = {10.1016/j.ejor.2004.08.029},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704005715},
	issn = {0377-2217},
}

% A whitepaper describing Cerebras's wafer scale neuromorphic computing architectures, as used in the AI incubator at Argonne
@techreport{lavely2022powering,
	author = {Lavely, Adam},
	title = {Powering Extreme-Scale {HPC} with {C}erebras Wafer-Scale Accelerators},
	year = {2022},
	institution = {Cerebras Systems, Inc.},
	address = {Sunnyvale, CA, USA},
	url = {https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Powering-Extreme-Scale-HPC-with-Cerebras.pdf},
}

% Peter Lax's classic textbook on functional analysis and all the core theorems Banach spaces, Hilbert spaces, and approximation theory
@book{lax2002functional,
	author = {Lax, Peter D.},
	title = {Functional analysis},
	year = {2002},
	series = {Pure and Applied Mathematics (New York)},
	pages = {xx--580},
	publisher = {Wiley-Interscience [John Wiley \& Sons], New York},
	isbn = {0-471-55604-1},
}

% NOMAD v3 is a widely-used open source numerical software package (in C++) for blackbox and derivative-free optimization via the MADS algorithms. Includes the official implementations of algorithms such as Ortho-MADS, PSD-MADS, and BiMADS. Support for parallel computing and surrogate modeling, and fairly extensible. Can be linked as a C++ library, or usage from command line interface. Used by a variety of industries and officially supported by Exxon Mobile. Has recently been replaced by the major refactor/rewrite in NOMAD v4. Still an example of widely-used open source numerical software, but the NOMAD v4 paper gives a look at how open source software practices have changed (improved) in the last 10 years
@article{le digabel2011algorithm,
	author = {{Le Digabel}, S{\'e}bastien},
	title = {Algorithm 909: {NOMAD}: Nonlinear Optimization with the {MADS} Algorithm},
	year = {2011},
	month = {2},
	journal = {ACM Transactions on Mathematical Software},
	volume = {37},
	number = {4},
	numpages = {44},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1916461.1916468},
	url = {https://dl.acm.org/doi/10.1145/1916461.1916468},
	issn = {0098-3500},
}

% pyDOE a popular software repository for the common design-of-experiments in Python. This has been widely replaced by the new release of scipy which includes scipy.stats.qmc, which includes most of these techniques (used for monte carlo sampling, but still the same techniques)
@misc{lee2015pydoe,
	author = {Lee, Abraham D. et al.},
	title = {py{DOE}: The experimental design package for python},
	year = {2015},
	booktitle = {GitHub repository},
	number = {0.3.8},
	publisher = {GitHub},
	url = {https://github.com/tisimst/pyDOE},
}

% An algorithm that combines direct search / pattern search with an augmented Lagrangian penalty term in order to solve a constrained blackbox optimization problem
@article{lewis2002globally,
	author = {Lewis, Robert Michael and Torczon, Virginia},
	title = {A Globally Convergent Augmented {L}agrangian Pattern Search Algorithm for Optimization with General Constraints and Simple Bounds},
	year = {2002},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {12},
	number = {4},
	pages = {1075--1089},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/S1052623498339727},
	url = {http://epubs.siam.org/doi/10.1137/S1052623498339727},
	issn = {1052-6234},
}

% An early paper proposing the usage of surrogate models within multiobjective evolutionary algorithms in order to improve their performance on computationally expensive blackbox / simulation optimization problems, where the function evaluation budget may be limited
@inproceedings{liu2016surrogate,
	author = {Liu, Bo and Sun, Nan and Zhang, Qingfu and Grout, Vic and Gielen, Georges},
	title = {A surrogate model assisted evolutionary algorithm for computationally expensive design optimization problems with discrete variables},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 {IEEE} Congress on Evolutionary Computation ({CEC})},
	pages = {1650--1657},
	organization = {{IEEE}},
	location = {Vancouver, BC, Canada},
	doi = {10.1109/cec.2016.7743986},
	url = {http://ieeexplore.ieee.org/document/7743986/},
}

% Adaptive Kriging model-based sampling basically means using an interpolating Gaussian process's uncertainty information to select where to sample the next point during an adaptive sampling algorithm (for generating design-of-experiments or design space exploration).
@article{liu2017adaptive,
	author = {Liu, Haitao and Cai, Jianfei and Ong, Yew-Soon},
	title = {An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error},
	year = {2017},
	month = {11},
	journal = {Computers \& Chemical Engineering, Special Section - ESCAPE-26},
	volume = {106},
	pages = {171--182},
	publisher = {Elsevier BV},
	doi = {10.1016/j.compchemeng.2017.05.025},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009813541730234X},
	issn = {0098-1354},
}

% The DFMO algorithm is a multiobjective line search, which the authors recommend combining with MODIR to improve its convergence to the Pareto front after identifying the global Pareto front. The open source numerical software is implemented in Fortran and is currently bundled inside the MODIR software package on the authors' GitHub account
@article{liuzzi2016derivativefree,
	author = {Liuzzi, Giampaolo and Lucidi, Stefano and Rinaldi, Francesco},
	title = {A derivative-free approach to constrained multiobjective nonsmooth optimization},
	year = {2016},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {26},
	number = {4},
	pages = {2744--2774},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/15M1037810},
	url = {http://epubs.siam.org/doi/10.1137/15M1037810},
	issn = {1052-6234},
}

% The official GitHub account for the DFO-lib -- open source numerical software in Fortran for solving blackbox optimization and multiobjective optimization problems. The library used to be obtained via download from Liuzzi's personal website, where it was referred to as the DFO lib. In 2024 it appears to have been migrated to a GitHub account, with each individual piece of software in its own separate repository. Therefore, any reference to the DFO-lib must now be directed to the account as a whole, not an individual repository.
@misc{liuzzi2024dfolib,
	author = {Liuzzi, Giampaolo et al.},
	title = {DFO-lib},
	year = {2024},
	booktitle = {GitHub repository},
	number = {0.3.8},
	publisher = {GitHub},
	url = {https://github.com/DerivativeFreeLibrary},
}

% Publication describing Graphcore's IPU architecture (intelligence processing unit), which is a neuromorphic parallel processor optimized for high-throughput vector operations
@inproceedings{louw2021using,
	author = {Louw, Thorben and McIntosh-Smith, Simon},
	title = {Using the {Graphcore IPU} for traditional {HPC} applications},
	year = {2021},
	booktitle = {Proc. 3rd Workshop on Accelerated Machine Learning (AccML)},
	pages = {1--9},
	location = {virtual event},
	url = {https://easychair.org/publications/preprint/ztfj},
}

% Multiobjective extension of the DIRECT algorithm for derivative-free blackbox optimization. This algorithm may suffer from some scalability issues, but is a good first step
@article{lovison2021extension,
	author = {Lovison, Alberto and Miettinen, Kaisa},
	title = {On the Extension of the {DIRECT} Algorithm to Multiple Objectives},
	year = {2021},
	month = {2},
	journal = {Journal of Global Optimization},
	volume = {79},
	number = {2},
	pages = {387--412},
	publisher = {Springer},
	doi = {10.1007/s10898-020-00942-8},
	url = {https://link.springer.com/10.1007/s10898-020-00942-8},
	issn = {0925-5001},
}

% Summary of discussions from Oak Ridge National Laboratory Summit discussing GPU-based architectures and other developments from the exascale computing project
@article{luo2020preexascale,
	author = {Luo, L. and P. Straatsma, T. and Suarez, L. E. Aguilar and Broer, R. and Bykov, D. and F. D'Azevedo, E. and S. Faraji, S. and C. Gottiparthi, K. and De Graaf, C. and A. Harris, J. and A. Havenith, R. W. and Jensen, H. J. Aa. and Joubert, W. and K. Kathir, R. and Larkin, J. and W. Li, Y. and I. Lyakh, D. and B. Messer, O. E. and R. Norman, M. and C. Oefelein, J. and Sankaran, R. and F. Tillack, A. and L. Barnes, A. and Visscher, L. and C. Wells, J. and Wibowo, M.},
	title = {Pre-exascale accelerated application development: The {ORNL Summit} experience},
	year = {2020},
	month = {5},
	journal = {IBM Journal of Research and Development},
	volume = {64},
	number = {3/4},
	pages = {11:1--11:21},
	publisher = {IBM},
	doi = {10.1147/JRD.2020.2965881},
	url = {https://ieeexplore.ieee.org/document/8960361/},
	issn = {0018-8646},
}

% Discussion of online learning in the context of multiobjective optimization
@inproceedings{mannor2014approachability,
	author = {Mannor, Shie and Perchet, Vianney and Stoltz, Gilles},
	title = {Approachability in unknown games: {O}nline learning meets multi-objective optimization},
	year = {2014},
	month = {13--15 June},
	booktitle = {Proc. 27th Conference on Learning Theory (PMLR)},
	series = {Proceedings of Machine Learning Research},
	volume = {35},
	pages = {339--355},
	organization = {PMLR},
	location = {Barcelona, Spain},
	url = {https://proceedings.mlr.press/v35/mannor14.html},
}

% Survey of common multiobjective optimization algorithms and scalarization techniques in the context of engineering design optimization
@article{marler2004survey,
	author = {Marler, Timothy R. and Arora, Jasbir S.},
	title = {Survey of multi-objective optimization methods for engineering},
	year = {2004},
	month = {4},
	journal = {Structural and Multidisciplinary Optimization},
	volume = {26},
	number = {6},
	pages = {369--395},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00158-003-0368-6},
	url = {http://link.springer.com/10.1007/s00158-003-0368-6},
	issn = {1615-147X},
}

% pyMDO: an open source numerical Python framework for modeling and solving multidisciplinary engineering design optimization problems
@article{martins2009pymdo,
	author = {Martins, Joaquim R. R. A. and Marriage, Christopher and Tedford, Nathan},
	title = {{pyMDO}: An Object-Oriented Framework for Multidisciplinary Design Optimization},
	year = {2009},
	month = {8},
	journal = {ACM Transactions on Mathematical Software},
	volume = {36},
	number = {4},
	numpages = {20},
	publisher = {ACM},
	doi = {10.1145/1555386.1555389},
	url = {https://dl.acm.org/doi/10.1145/1555386.1555389},
	issn = {0098-3500},
}

% Python wrapper around the open source numerical simulation software superfish We used this when performing particle accelerator RF gun cavity optimization at Argonne in order to provide a bridge between POISSON/SUPERFISH (Fortran codes) and the ParMOO optimizer (a Python code)
@misc{mayes2023pysuperfish,
	author = {Mayes, Christopher},
	title = {PySuperfish},
	year = {2023},
	url = {https://github.com/ChristopherMayes/PySuperfish},
}

% Algorithms and theorems on the difficulty of finding basic solutions for linear programming problems
@article{megiddo1991finding,
	author = {Megiddo, Nimrod},
	title = {On finding primal- and dual-optimal bases},
	year = {1991},
	month = {2},
	journal = {ORSA Journal on Computing},
	volume = {3},
	number = {1},
	pages = {63--65},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.3.1.63},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.3.1.63},
	issn = {0899-1499},
	keywords = {},
}

% The POISSON/SUPERFISH open source numerical software (in Fortran) for calculating static, magnetic, and electric fields and rf electromagnetic fields. Used to design particle accelerators, developed and maintained by Los Alamos
@techreport{menzel1987users,
	author = {Menzel, M. T. and Stokes, H. K.},
	title = {Users guide for the {POISSON/SUPERFISH} group of codes},
	year = {1987},
	month = {1},
	number = {LA-UR-87-115},
	institution = {Los Alamos National Laboratory},
	address = {Los Alamos, NM, USA},
	doi = {10.2172/10140823},
	url = {https://www.osti.gov/servlets/purl/10140823},
}

% Report on challenges and experiences gained in porting PETSc to GPUs for the exascale computing project
@article{mills2021toward,
	author = {Mills, Richard Tran and Adams, Mark F. and Balay, Satish and Brown, Jed and Dener, Alp and Knepley, Matthew and Kruger, Scott E. and Morgan, Hannah and Munson, Todd and Rupp, Karl and Smith, Barry F. and Zampini, Stefano and Zhang, Hong and Zhang, Junchao},
	title = {Toward performance-portable {PETSc} for {GPU}-based exascale systems},
	year = {2021},
	month = {12},
	journal = {Parallel Computing},
	volume = {108},
	numpages = {102831},
	publisher = {Elsevier BV},
	doi = {10.1016/j.parco.2021.102831},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016781912100079X},
	issn = {0167-8191},
}

% DESDEO an open source Python framework for implementing interactive multiobjective optimization solvers
@article{misitano2021desdeo,
	author = {Misitano, Giovanni and Saini, Bhupinder S. and Afsar, Bekir and Shavazipour, Babooshka and Miettinen, Kaisa},
	title = {{DESDEO}: The Modular and Open Source Framework for Interactive Multiobjective Optimization},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {148277--148295},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2021.3123825},
	url = {https://ieeexplore.ieee.org/document/9591595/},
	issn = {2169-3536},
}

% MORDRED: A 3D molecular descriptor calculator, which is widely used for embedding molecules into a continuous latent space (parameterized by their descriptors) which can be used to solve chemical property optimization problems. The MORDRED software is available open source in Python.
@article{moriwaki2018mordred,
	author = {Moriwaki, Hirotomo and Tia, Yu-Shi and Kawashita, Norihito and Takagi, Tatsuya},
	title = {Mordred: a molecular descriptor calculator},
	year = {2018},
	month = {12},
	journal = {Journal of Cheminformatics},
	volume = {10},
	number = {1},
	articleno = {4},
	numpages = {14},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1186/s13321-018-0258-y},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y},
	issn = {1758-2946},
}

% SOCEMO: A response surface modeling (RSM) based algorithm for solving multiobjective optimization problems. Uses a Latin hypercube design-of-experiments, RBF surrogate modeling, multiple scalarizations, and solves the scalarized subproblem via evolutionary algorithms to produce a batch of evaluations in each iteration of the algorithm
@article{muller2017socemo,
	author = {M{\"u}ller, Juliane},
	title = {{SOCEMO}: {S}urrogate optimization of computationally expensive multiobjective problems},
	year = {2017},
	month = {11},
	journal = {INFORMS Journal on Computing},
	volume = {29},
	number = {4},
	pages = {581--596},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2017.0749},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2017.0749},
	issn = {1091-9856},
}

% This is the last TAO (toolkit for advanced optimization) reference before this open source numerical simulation optimization software when merged with PETSc, into a single PETSc + TAO release
@techreport{munson2015tao,
	author = {Munson, Todd and Sarich, Jason and Wild, Stefan and Benson, Steven and McInnes, Lois Curfman},
	title = {{TAO} 3.5 Users Manual},
	year = {2015},
	number = {ANL/MCS-TM-322 version 3.5},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://www.mcs.anl.gov/petsc/petsc-3.5.4/docs/tao_manual.pdf},
}

% The classical textbook on response surface methodology and modeling practices. Contains useful information on the basic framework and applications of RSM. Also a useful reference for many of the options for specific techniques: Chapter 7 is a good reference for basic techniques in multiobjective RSM and Chapters 8-9 surveys the basic methods in design-of-experiments
@book{myers2016response,
	author = {Myers, Raymond H. and Montgomery, Douglas C. and Anderson-Cook, Christine M.},
	title = {Response Surface Methodology: Process and Design Optimization Using Designed Experiments},
	year = {2016},
	edition = {4},
	publisher = {John Wiley \& Sons, Inc.},
	address = {Hoboken, NJ, USA},
	isbn = {9781118916032},
}

% Original publication on Nesterov's momentum. I haven't read it (it is hard to find a copy and likely in Russian) but this is the preferred citation. The equation for Nesterov momentum in gradient descent is instead of using the update: x' = x - a*g(x), use x' = x - a*g(y) - b*v where y = x - b*v and v = b*v + a*g(x) -- in this equation, b*v is the momentum term which smooths out poor conditioning in the problem by encouraging the algorithm to continue in the direction it was headed instead of oscillating. Nexterov proves that this term also leads to better convergence rates. For best results, b is usually chosen to be a large value such as 0.9 or 0.99
@inproceedings{nesterov1983method,
	author = {Nesterov, Yurii},
	title = {A method for solving the convex programming problem with convergence rate O (1/k2)},
	year = {1983},
	booktitle = {Dokl akad nauk Sssr},
	volume = {269},
	numpages = {543},
}

% An application of VTMOP for the multiobjective optimization (tuning) of the LCLS-II photoinjector (linear accelerator at SLAC). Had to use some hacks to get VTMOP to work, such as penalizing bad regions of the Pareto front, but ultimately performed better than NSGA-II
@article{neveu2023comparison,
	author = {Neveu, Nicole and Chang, Tyler H. and Franz, Paris and Hudson, Stephen and Larson, Jeffrey},
	title = {Comparison of multiobjective optimization methods for the {LCLS-II} photoinjector},
	year = {2023},
	month = {2},
	journal = {Computer Physics Communication},
	volume = {283},
	articleno = {108566},
	numpages = {10},
	publisher = {Elsevier BV},
	doi = {10.1016/j.cpc.2022.108566},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465522002855},
	issn = {0010-4655},
}

% The original publication where Sobol's sequences are generalized to the class now known as low-discrepancy sequenes
@article{niederreiter1988lowdiscrepancy,
	author = {Niederreiter, Harald},
	title = {Low-discrepancy and low-dispersion sequences},
	year = {1988},
	month = {9},
	journal = {Journal of Number Theory},
	volume = {30},
	number = {1},
	pages = {51--70},
	publisher = {Elsevier BV},
	doi = {10.1016/0022-314X(88)90025-X},
	url = {https://doi.org/10.1016/0022-314X(88)90025-X},
	issn = {0022-314X},
}

% The classic textbook by Nocedal on fundamental techniques in nonlinear programming, such as local modeling and trust-region methods
@book{nocedal2006numerical,
	author = {Nocedal, Jorge and Wright, Stephen J.},
	title = {Numerical Optimization},
	year = {2006},
	booktitle = {Springer Series in Operations Research and Financial Engineering},
	series = {Springer Series in Operations Research and Financial Engineering},
	edition = {2},
	publisher = {Springer Verlag},
	address = {New York, NY, USA},
	doi = {10.1007/978-0-387-40065-5},
	url = {http://link.springer.com/10.1007/978-0-387-40065-5},
	isbn = {9780387303031},
}

% OpenMP 4.5 standard: Official definition of OpenMP 4.5 software standard
@techreport{oarb(2015openmp,
	title = {{OpenMP Application Programming Interface} version 4.5},
	year = {2015},
	month = {November},
	number = {OpenMP 4.5},
	institution = {OpenMP Architecture Review Board (ARB)},
	keywords = {},
}

% SCS is an open source numerical software for solving second-order cone problems from Steph Boyd's lab
@article{odonoghue2016conic,
	author = {O'Donoghue, Brendan and Chu, Eric and Parikh, Neal and Boyd, Stephen},
	title = {Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding},
	year = {2016},
	month = {June},
	journal = {Journal of Optimization Theory and Applications},
	volume = {169},
	number = {3},
	pages = {1042--1068},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10957-016-0892-3},
	url = {http://link.springer.com/10.1007/s10957-016-0892-3},
	issn = {0022-3239},
}

% PABO - a multiobjective Bayesian optimization software package that is specialized for NAS -- basically an inner network is used as a surrogate and an outer network is used to predict which designs to evaluate next -- I have serious reservations about this kind of approach, and it doesn't seem to work that well
@inproceedings{parsa2019pabo,
	author = {Parsa, Maryam and Ankit, Aayush and Ziabari, Amirkoushyar and Roy, Kaushik},
	title = {{PABO}: Pseudo Agent-Based Multi-Objective {B}ayesian Hyperparameter Optimization for Efficient Neural Accelerator Design},
	year = {2019},
	month = {11},
	booktitle = {Proc. 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	pages = {1--8},
	organization = {IEEE/ACM},
	location = {Westin Westminster, CO, USA},
	doi = {10.1109/ICCAD45719.2019.8942046},
	url = {https://ieeexplore.ieee.org/document/8942046/},
}

% H-PABO multiobjective Bayesian optimization framework, specialized for NAS, and improvement on PABO
@article{parsa2020bayesian,
	author = {Parsa, Maryam and Mitchell, John P. and Schuman, Catherine D. and Patton, Robert M. and Potok, Thomas E. and Roy, Kaushik},
	title = {Bayesian Multi-objective Hyperparameter Optimization for Accurate, Fast, and Efficient Neural Network Accelerator Design},
	year = {2020},
	month = {7},
	journal = {Frontiers in Neuroscience},
	volume = {14},
	numpages = {667},
	publisher = {Frontiers Media SA},
	doi = {10.3389/fnins.2020.00667},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00667/full},
	issn = {1662-453X},
}

% The official publicatino for pytorch a gold standard in open source software, providing automatic differentiation and numerical linear algebra in Python, targeted at implementing deep learning algorithms. Pytorch is pretty much a standard in not just open source software, but also machine learning software, and also numerical software
@inproceedings{paszke2019pytorch,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch\'{e}-Buc, F. d\textquotesingle and Fox, E. and Garnett, R.},
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	year = {2019},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {32},
	pages = {1--12},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
}

% The official publicatino for scikit-learn a gold standard in open source software, providing a clean interface to several standard implementations of numerical approximation, optimization, machine learning, and deep learning algorithms. Scikit-learn is pretty much a standard in not just open source software, but also machine learning software, and also numerical software
@article{pedregosa2011scikitlearn,
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others, },
	title = {Scikit-learn: Machine learning in {P}ython},
	year = {2011},
	journal = {Journal of Machine Learning Research},
	volume = {12},
	pages = {2825--2830},
	url = {https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
}

% An active learning method for extreme event modeling. Specifically, the authors provide a novel acquisition function which targets extreme events
@article{pickering2022discovering,
	author = {Pickering, Ethan and Guth, Stephen and Karniadakis, George Em and Sapsis, Themistoklis P},
	title = {Discovering and forecasting extreme events via active learning in neural operators},
	year = {2022},
	month = {12},
	journal = {Nature Computational Science},
	volume = {2},
	number = {12},
	pages = {823--833},
	publisher = {Nature Publishing Group US New York},
	doi = {10.1038/s43588-022-00376-0},
	url = {https://www.nature.com/articles/s43588-022-00376-0},
	issn = {2662-8457},
}

% The COBYLA paper on Powell's constrainted optimization by linear approximation algorithm. COBYLA basically performs gradient descent on a constrainted blackbox optimization problem by fitting a linear model to the underlying function and following its gradient within a shrinking trust region. COBYLA is typically able to do this taking typically only one or two function evaluation per iteration since typically only one point is exiting the trust-region per iteration. (Occasionally, additional model improvement points must be sampled to maintain the interpolation set geometry). This makes COBYLA barely more expensive then true gradient descent. COBYQA has supplanted COBYLA since then (the Q standing for quadratic), but I personally prefer COBYLA still as a find the locally linear models to be more robust against noisy and nonsmooth data, still efficiently finding local minima even though COBYLA was not design for such problems. The original open source software was in impossibly complex old-style Fortran. A modern Fortran version has been provided in Pima by Zaikun Zhang, and a modern Python implementation is provided in PDFO by Ragonneau and Zhang.
@inproceedings{powell1994direct,
	author = {Powell, Michael J. D.},
	title = {A direct search optimization method that models the objective and constraint functions by linear interpolation},
	year = {1994},
	booktitle = {Gomez, S. and Hennart, J. P. (eds) Advances in Optimization and Numerical Analysis, vol 275},
	pages = {51--67},
	organization = {Springer},
	doi = {10.1007/978-94-015-8330-5_4},
	url = {http://link.springer.com/10.1007/978-94-015-8330-5_4},
	isbn = {9789048143580},
}

% Review of minimax and maximin techniques and computational methods. Minimax is what we want, but it is hard to compute in high dimensions (would require optimizing a Delaunay triangulation, which is exponential complexity to compute). Maximin is easier to compute (and often used in many algorithms because of this). However, minimax gives better dispersion
@article{pronzato2017minimax,
	author = {Pronzato, Luc},
	title = {Minimax and maximin space-filling designs: some properties and methods for construction},
	year = {2017},
	journal = {Journal de la Soci{\'e}t{\'e} Fran{\c{c}}aise de Statistique},
	volume = {158},
	number = {1},
	pages = {7--36},
	url = {http://www.numdam.org/item/JSFS_2017__158_1_7_0},
}

% Experiences, challenges, and techniques for integrating the blackbox optimization solvers VTDIRECT95 and QNSTOP into the parallel service architecture SORCER
@inproceedings{raghunath2017global,
	author = {Raghunath, Chaitra and Chang, Tyler H. and Watson, Layne T. and Jrad, Mohamad and Kapania, Rakesh K. and Kolonay, Raymond M.},
	title = {Global deterministic and stochastic optimization in a service oriented architecture},
	year = {2017},
	booktitle = {Proc. 2017 Spring Simulation Conference (SpringSim 2017), the 25th High Performance Computing Symposium (HPC '17)},
	numpages = {7},
	organization = {SCS},
	location = {Virginia Beach, VA, USA},
	doi = {10.22360/springsim.2017.hpc.023},
	url = {http://dl.acm.org/citation.cfm?id=3108103},
}

% PDFO: An open source modern Python implementation of Powell's derivative-free numerical optimization software suite, which is considered to be the standard (baseline) in derivative-free optimization solvers
@misc{ragonneau2021pdfo,
	author = {Ragonneau, Tom M. and Zhang, Zaikun},
	title = {{PDFO}: Cross-Platform Interfaces for {P}owell’s Derivative-Free Optimization Solvers},
	year = {2021},
	booktitle = {GitHub repository},
	number = {1.2},
	publisher = {GitHub},
	url = {https://github.com/pdfo/pdfo},
}

% The calculus of simplex gradients: a detailed analysis of simplex gradients and their properties for approximating true gradients when solving derivative-free and blackbox optimization problems
@article{regis2015calculus,
	author = {Regis, Rommel G.},
	title = {The calculus of simplex gradients},
	year = {2015},
	month = {6},
	journal = {Optimization Letters},
	volume = {9},
	number = {5},
	pages = {845--865},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11590-014-0815-x},
	url = {http://link.springer.com/10.1007/s11590-014-0815-x},
	issn = {1862-4472},
	keywords = {},
}

% Using RBF surrogates in the context of multiobjective optimization. I believe he ultimately solved the surrogate problems with NSGA-II or some other heuristic
@article{regis2016multiobjective,
	author = {Regis, Rommel G.},
	title = {Multi-objective constrained black-box optimization using radial basis function surrogates},
	year = {2016},
	month = {9},
	journal = {Journal of Computational Science},
	volume = {16},
	pages = {140--155},
	publisher = {Elsevier BV},
	doi = {10.1016/j.jocs.2016.05.013},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1877750316300904},
	issn = {1877-7503},
}

% Official publication of the scipy.stats.qmc module, which is the newly released module for performing quasi-monte carlo sampling and design-of-experiments in scipy. Scipy is an open source numerical software package which is the standard for advanced numerical methods and scientific software packages in Python. Most of scipy are wrappers for much older Fortran or C++ code, that has been highly optimized.
@article{roy2023quasimonte,
	author = {Roy, Pamphile T. and Owen, Art B. and Balandat, Maximilian and Haberland, Matt},
	title = {Quasi-Monte Carlo Methods in Python},
	year = {2023},
	month = {4},
	journal = {Journal of Open Source Software},
	volume = {8},
	number = {84},
	numpages = {5309},
	publisher = {The Open Journal},
	doi = {10.21105/joss.05309},
	url = {https://joss.theoj.org/papers/10.21105/joss.05309},
	issn = {2475-9066},
}

% The original publication of PAWS: An adaptive weighted sum and trust-regions method for solving biobjective (multiobjective) optimization problems. The key here is that the trust-regions help the weighted sum scalarization reach into nonconvex regions of the Pareto front, even though that would not normally be possible
@inproceedings{ryu2009pareto,
	author = {Ryu, Jong-hyun and Kim, Sujin and Wan, Hong},
	title = {Pareto front approximation with adaptive weighted sum method in multiobjective simulation optimization},
	year = {2009},
	month = {12},
	booktitle = {Proc. 2009 Winter Simulation Conference (WSC '09)},
	pages = {623--633},
	organization = {IEEE},
	location = {Austin, TX, USA},
	doi = {10.1109/WSC.2009.5429562},
	url = {http://ieeexplore.ieee.org/document/5429562/},
}

% The latest version of PAWS: An adaptive weighted sum and trust-regions method for solving biobjective (multiobjective) optimization problems. The key here is that the trust-regions help the weighted sum scalarization reach into nonconvex regions of the Pareto front, even though that would not normally be possible
@article{ryu2014derivativefree,
	author = {Ryu, Jong-Hyun and Kim, Sujin},
	title = {A derivative-free trust-region method for biobjective optimization},
	year = {2014},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {24},
	number = {1},
	pages = {334--362},
	publisher = {SIAM},
	doi = {10.1137/120864738},
	url = {http://epubs.siam.org/doi/10.1137/120864738},
	issn = {1052-6234},
}

% Techniques and optimal sampling criteria for performing adaptive sampling to enable downstream Gaussian process regression modeling
@article{sapsis2022optimal,
	author = {Sapsis, Themistoklis P. and Blanchard, Antoine},
	title = {Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with {Gaussian} process regression},
	year = {2022},
	month = {8},
	journal = {Philosophical Transactions of the Royal Society A},
	volume = {380},
	number = {2229},
	numpages = {20210197},
	publisher = {The Royal Society},
	doi = {10.1098/rsta.2021.0197},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0197},
	issn = {1364-503X},
}

% The SMT 2.0 paper, major improvements to the open source numerical software package (in Python) pySMT for solving multidisciplinary engineering design optimization (MDO) problems, while utilizing derivatives and providing numerical stability analysis for each surrogate model class. In SMT 2.0, support is added for hierarchical and mixed variables, and major improvements have been made to the structure, completeness, and features of the SMT library.
@article{saves2024smt,
	author = {Saves, Paul and Lafage, Rémi and Bartoli, Nathalie and Diouane, Youssef and Bussemaker, Jasper and Lefebvre, Thierry and Hwang, John T. and Morlier, Joseph and Martins, Joaquim R.R.A.},
	title = {SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
	year = {2024},
	month = {2},
	journal = {Advances in Engineering Software},
	volume = {188},
	numpages = {103571},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2023.103571},
	url = {https://www.sciencedirect.com/science/article/pii/S096599782300162X},
	issn = {0965-9978},
}

% Multiobjective molecule property optimization by optimizing processes in a continuous flow reactor (CFR) using the multiobjective evolutionary algorithm TS-EMO (thompson sampling evolutionary multiobjective optimization?)
@article{schweidtmann2018machine,
	author = {Schweidtmann, Artur M. and Clayton, Adam D. and Holmes, Nicholas and Bradford, Eric and Bourne, Richard A. and Lapkin, Alexei A.},
	title = {Machine learning meets continuous flow chemistry: Automated optimization towards the {Pareto} front of multiple objectives},
	year = {2018},
	month = {11},
	journal = {Chemical Engineering Journal},
	volume = {352},
	pages = {277--282},
	publisher = {Elsevier},
	doi = {10.1016/j.cej.2018.07.031},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1385894718312634},
	issn = {1385-8947},
}

% A survey of hypervolume indicator usage in multiobjective evolutionary optimization, mainly in terms of algorithms that use hypervolume improvement and also as a performance indicator
@article{shang2020survey,
	author = {Shang, Ke and Ishibuchi, Hisao and He, Linjun and Pang, Lie Meng},
	title = {A survey on the hypervolume indicator in evolutionary multiobjective optimization},
	year = {2020},
	month = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {25},
	number = {1},
	pages = {1--20},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2020.3013290},
	url = {https://ieeexplore.ieee.org/document/9153850/},
	issn = {1089-778X},
}

% A recent paper on the numerical performance of finite-difference-based methods for DFO. Personally, I don't think this is a viable approach given the performance of model-based methods. However, one of their experiments corroborates my experience that you can just run COBYLA on noisy and nonsmooth blackbox optimization problems, and even though it was not designed for those problems, it still does extremely well and regularly finds local minima (at least up to the noise level) -- from Nocedal's lab
@article{shi2023numerical,
	author = {Shi, Hao-Jun Michael and Xuan, Melody Qiming and Oztoprak, Figen and and, Jorge Nocedal},
	title = {On the numerical performance of finite-difference-based methods for derivative-free optimization},
	year = {2023},
	month = {3},
	journal = {Optimization Methods and Software},
	volume = {38},
	number = {2},
	pages = {289--311},
	publisher = {Taylor \& Francis},
	doi = {10.1080/10556788.2022.2121832},
	url = {https://doi.org/10.1080/10556788.2022.2121832},
	issn = {1055-6788},
}

% The EDBO software: open source Python software for performing multiobjective bayesian optimization for chemical synthesis and molecular discovery. Links a multiobjective optimization solver with the MORDRED software for getting molecular descriptors and optimizes for the desired properties
@article{shields2021bayesian,
	author = {Shields, Benjamin J. and Stevens, Jason and Li, Jun and Parasram, Marvin and Damani, Farhan and Alvarado, Jesus I. M. and Janey, Jacob M. and Adams, Rryan P. and Doyle, Abigail G.},
	title = {Bayesian reaction optimization as a tool for chemical synthesis},
	year = {2021},
	month = {2},
	journal = {Nature},
	volume = {590},
	number = {7844},
	pages = {89--96},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41586-021-03213-y},
	url = {https://www.nature.com/articles/s41586-021-03213-y},
	issn = {0028-0836},
	git = {http://github.com/b-shields/edbo},
}

% The docker image for the SUPERFISH/POISSON container + wrapper -- we used this at Argonne to obtain a portable copy of POISSON/SUPERFISH when performing particle accelerator RF gun cavity optimization and distributing work on remote systems
@misc{slepicka2020poisson,
	author = {Slepicka, Hugo},
	title = {Poisson Superfish via Docker},
	year = {2020},
	url = {https://github.com/hhslepicka/docker-poisson-superfish-nobin},
}

% Steve Smale's list of unsolved open problems in mathematics, and mostly algorithms, one of which is reliably finding basic solutions to linear programming problems
@article{smale1998mathematical,
	author = {Smale, Steve},
	title = {Mathematical problems for the next century},
	year = {1998},
	month = {3},
	journal = {The Mathematical Intelligencer},
	volume = {20},
	number = {2},
	pages = {7--15},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/bf03025291},
	url = {http://link.springer.com/10.1007/BF03025291},
	issn = {0343-6993},
	keywords = {},
}

% One of the most popular textbooks in the field of multidisciplinary engineering design optimization. The introduction provides plenty of motivation for solving computational expensive multiobjective simulation / blackbox optimization problems
@book{sobieszczanskisobieski2015multidisciplinary,
	author = {Sobieszczanski-Sobieski, Jaroslaw and Morris, Alan and Van Tooren, Michel},
	title = {Multidisciplinary Design Optimization Supported by Knowledge Based Engineering},
	year = {2015},
	publisher = {John Wiley \& Sons, Ltd.},
	address = {Chichester, UK},
	isbn = {978-1-118-49212-3},
}

% The original publication of the Sobol sequence algorithm for generating well-distributed points for in the context of good nodes for numerical integration. This is now referred to as a low-discrepancy sequence and is also used for design-of-experiments and quasi-random number generation
@article{sobol1967distribution,
	author = {Sobol, I. M.},
	title = {Distribution of points in a cube and approximate evaluation of integrals},
	year = {1967},
	month = {1},
	journal = {\v{Z}urnal Vy\v{c}islitel\cprime no\u{\i} Matematiki i Matemati\v{c}esko\u{\i} Fiziki},
	volume = {7},
	number = {4},
	pages = {784--802},
	publisher = {Elsevier BV},
	doi = {10.1016/0041-5553(67)90144-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0041555367901449},
	issn = {0044-4669},
}

% The FAIR principles of data management: Scientific data should be findable, accessible, interpretable, and reproducible
@article{stall2019make,
	author = {Stall, Shelley and Yarmey, Lynn and Cutcher-Gershenfeld, Joel and Hanson, Brooks and Lehnert, Kerstin and Nosek, Brian and Parsons, Mark and Robinson, Erin and Wyborn, Lesley},
	title = {Make scientific data {FAIR}},
	year = {2019},
	month = {6},
	journal = {Nature},
	volume = {570},
	number = {7759},
	pages = {27--29},
	publisher = {Nature Publishing Group},
	doi = {10.1038/d41586-019-01720-7},
	url = {https://www.nature.com/articles/d41586-019-01720-7},
	issn = {0028-0836},
}

% OSQP is an open source numerical software for solving quadratic programming problems from Stephen Boyd's lab
@article{stellato2020osqp,
	author = {Stellato, Bartolomeo and Banjac, Goran and Goulart, Paul and Bemporad, Alberto and Boyd, Stephen},
	title = {{OSQP}: an operator splitting solver for quadratic programs},
	year = {2020},
	month = {12},
	journal = {Mathematical Programming Computation},
	volume = {12},
	number = {4},
	pages = {637--672},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s12532-020-00179-2},
	url = {http://link.springer.com/10.1007/s12532-020-00179-2},
	issn = {1867-2949},
}

% The original publication of the weighted Chebyshev scalarization scheme for multiobjective optimization
@article{steuer1983interactive,
	author = {Steuer, Ralph E and Choo, Eng-Ung},
	title = {An interactive weighted Tchebycheff procedure for multiple objective programming},
	year = {1983},
	month = {10},
	journal = {Mathematical programming},
	volume = {26},
	number = {3},
	pages = {326--344},
	publisher = {Springer},
	doi = {10.1007/BF02591870},
	url = {http://link.springer.com/10.1007/BF02591870},
	issn = {0025-5610},
}

% The textbook on evolutionary multiobjective optimization (MOO), including common algorithms, evaluation methodologies, fundamental techniques, and test problems
@book{sv2005evolutionary,
	editor = {Abraham, Ajith and Jain, Lakhmi and Goldberg, Robert},
	title = {Evolutionary Multiobjective Optimization: Theoretical Advances and Applications},
	year = {2005},
	booktitle = {Advanced Information and Knowledge Processing},
	series = {Advanced Information and Knowledge Processing Series},
	publisher = {Springer Verlag},
	address = {London, UK},
	doi = {10.1007/1-84628-137-7},
	url = {http://link.springer.com/10.1007/1-84628-137-7},
	isbn = {1852337877},
}

% BoostDMS is numerical software library providing access to Custodio's direct search and pattern search software, including MultiGLODS and DMS, in Matlab with full parallel computing support
@article{tavares2022parallel,
	author = {Tavares, S. and Br\'as, C. P. and Cust\'odio, A. L. and Duarte, V. and Medeiros, P.},
	title = {Parallel Strategies for Direct Multisearch},
	year = {2022},
	month = {3},
	journal = {Numerical Algorithms},
	volume = {92},
	number = {3},
	pages = {1757--1788},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11075-022-01364-1},
	url = {https://link.springer.com/10.1007/s11075-022-01364-1},
	issn = {1017-1398},
}

% A trust-region + RBF surrogate-based multiobjective optimization algorithm for solving heterogeneous multiobjective optimization problems (where one or more objectives is a computationally expensive blackbox, and one or more is not). The key is to just use the derivative of all the non blackbox objectives and use the model derivative for the blackbox terms. The algorithm itself follows something like Orbit
@article{thomann2019trustregion,
	author = {Thomann, Jana and Eichfelder, Gabriele},
	title = {A Trust-Region Algorithm for Heterogeneous Multiobjective Optimization},
	year = {2019},
	month = {1},
	journal = {{SIAM} Journal on Optimization},
	volume = {29},
	number = {2},
	pages = {1017--1047},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/18m1173277},
	url = {https://epubs.siam.org/doi/10.1137/18M1173277},
	issn = {1052-6234},
}

% An open source MATLAB platform for implementing and running wide-scale comparisons against other multiobjective evolutionary algorithms on standard multiobjective test problems
@article{tian2017platemo,
	author = {Tian, Ye and Cheng, Ran and Zhang, Xingyi and Jin, Yaochu},
	title = {{PlatEMO}: A {MATLAB} Platform for Evolutionary Multi-Objective Optimization [Educational Forum]},
	year = {2017},
	month = {11},
	journal = {IEEE Computational Intelligence Magazine},
	volume = {12},
	number = {4},
	pages = {73--87},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/MCI.2017.2742868},
	url = {http://ieeexplore.ieee.org/document/8065138/},
	issn = {1556-603X},
}

% A survey of Pareto front visualization techniques in multiobjective optimization
@article{tuv sar2015visualization,
	author = {Tu{\v s}ar, Tea and Filipi{\v c}, Bogdan},
	title = {Visualization of {P}areto Front Approximations in Evolutionary Multiobjective Optimization: A Critical Review and the Prosection Method},
	year = {2015},
	month = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {19},
	number = {2},
	pages = {225--245},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TEVC.2014.2313407},
	url = {https://ieeexplore.ieee.org/document/6777535/},
	issn = {1089-778X},
}

% A robust RBF surrogate-based model, with adaptive scaling of the basis function radii to maintain numerical stability and a custom LHS sampling technique
@article{urquhart2020surrogatebased,
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	year = {2020},
	month = {3},
	journal = {Applied Soft Computing},
	volume = {88},
	numpages = {106050},
	publisher = {Elsevier BV},
	doi = {10.1016/j.asoc.2019.106050},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494619308324},
	issn = {1568-4946},
}

% The (recently open source) numerical software library SLATEC from Sandia is something of a precursor to a modern library like scipy. SLATEC provides highly optimized, numerically stable, Fortran implementations for nearly every basic numerical algorithm that one would encounter in scientific computing
@article{vandevender1982slatec,
	author = {Vandevender, Walter H. and Haskell, Karen H.},
	title = {The {SLATEC} Mathematical Subroutine Library},
	year = {1982},
	month = {9},
	journal = {SIGNUM Newsletter},
	volume = {17},
	number = {3},
	pages = {16--21},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1057594.1057595},
	url = {https://dl.acm.org/doi/10.1145/1057594.1057595},
	issn = {0163-5778},
	keywords = {},
}

% A tutorial on how to compute Latin hypercube samples (LHS) and some basic properties and ongoing research related to design-of-experiments
@article{viana2016tutorial,
	author = {Viana, Felipe AC},
	title = {A tutorial on Latin hypercube design of experiments},
	year = {2016},
	month = {7},
	journal = {Quality and reliability engineering international},
	volume = {32},
	number = {5},
	pages = {1975--1985},
	publisher = {Wiley Online Library},
	doi = {10.1002/qre.1924},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/qre.1924},
	issn = {0748-8017},
}

% SciPy official publication: Scipy is an open source numerical software package which is the standard for advanced numerical methods and scientific software packages in Python. Most of scipy are wrappers for much older Fortran or C++ code, that has been highly optimized.
@article{virtanen2020scipy,
	author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant}, Travis E. and {Haberland}, Matt and {Reddy}, Tyler and {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt}, St{\'e}fan J. and {Brett}, Matthew and {Wilson}, Joshua and {Jarrod Millman}, K. and {Mayorov}, Nikolay and {Nelson}, Andrew R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore}, Eric W. and {VanderPlas}, Jake and {Laxalde}, Denis and {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M. and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and {van Mulbregt}, Paul and {Contributors}, {SciPy 1.0}},
	title = {{SciPy} 1.0: {F}undamental Algorithms for Scientific Computing in {P}ython},
	year = {2020},
	month = {3},
	journal = {Nature Methods},
	volume = {17},
	number = {3},
	pages = {261--272},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41592-019-0686-2},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	issn = {1548-7091},
}

% Open source numerical Python software pyomo.DOE, implementing model-driven design-of-experiments generation in Pyomo
@article{wang2022pyomo.doe,
	author = {Wang, Jialu and Dowling, Alexander W.},
	title = {Pyomo.DOE: An open-source package for model-based design of experiments in Python},
	year = {2022},
	month = {12},
	journal = {AIChE Journal},
	volume = {68},
	number = {12},
	articleno = {e17813},
	publisher = {Wiley},
	doi = {10.1002/aic.17813},
	url = {https://aiche.onlinelibrary.wiley.com/doi/10.1002/aic.17813},
	issn = {0001-1541},
}

% Survey of design-of-experiments techniques and modifications for HPC system analysis, specifically related to linearly constrained and integer lattice design spaces
@article{wang2023design,
	author = {Wang, Yueyao and Xu, Li and Hong, Yili and Pan, Rong and Chang, Tyler H. and Lux, Thomas C. H. and Bernard, Jon and Watson, Layne T. and Cameron, Kirk W.},
	title = {Design strategies and approximation methods for high-performance computing variability management},
	year = {2023},
	month = {1},
	journal = {Journal of Quality Technology},
	volume = {55},
	number = {1},
	pages = {88--103},
	publisher = {Taylor \& Francis},
	doi = {10.1080/00224065.2022.2035285},
	url = {https://www.tandfonline.com/doi/full/10.1080/00224065.2022.2035285},
	issn = {0022-4065},
}

% The latest release of HOMPACK: An open source numerical software package written in Fortran 90 for solving nonlinear and polynomial systems of equations via homotopy methods.
@article{watson1997algorithm,
	author = {Watson, Layne T and Sosonkina, Maria and Melville, Robert C and Morgan, Alexander P and Walker, Homer F},
	title = {Algorithm 777: HOMPACK90: A suite of Fortran 90 codes for globally convergent homotopy algorithms},
	year = {1997},
	month = {12},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	volume = {23},
	number = {4},
	pages = {514--549},
	publisher = {ACM},
	doi = {10.1145/279232.279235},
	url = {https://dl.acm.org/doi/10.1145/279232.279235},
	issn = {0098-3500},
}

% How optimization is used to autotune the configuration of the BLAS subroutines and kernels for the ATLAS project -- since most numerical software relies on BLAS, it is often prudent to spend time optimizing BLAS configurations (such as matrix block sizes, etc.) to match machine specific values (such as cache sizes, etc.) when installing on a HPC that will have a high numerical (compute bound) workload. This can be done automatically via numerical optimization, so that users can just run the ATLAS setup scripts to configure BLAS automatically if they want an optimized installation
@article{whaley2001automated,
	author = {Whaley, R. Clint and Petitet, Antoine and Dongarra, Jack J.},
	title = {Automated empirical optimizations of software and the {ATLAS} project},
	year = {2001},
	month = {1},
	journal = {Parallel Computing},
	volume = {27},
	number = {1--2},
	pages = {3--35},
	publisher = {Elsevier BV},
	doi = {10.1016/s0167-8191(00)00087-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819100000879},
	issn = {0167-8191},
	keywords = {},
}

% The original publication of the reference point method for scalarizing multiobjective optimization problems (minimize the distance to a reference point)
@incollection{wierzbicki1999reference,
	author = {Wierzbicki, Andrzej P.},
	editor = {Gal, Tomas and Stewart, Theodor J. and Hanne, Thomas},
	title = {Reference Point Approaches},
	year = {1999},
	booktitle = {Multicriteria Decision Making: Advances in MCDM Models, Algorithms, Theory, and Applications},
	series = {International Series in Operations Research &amp; Management Science},
	pages = {237--275},
	publisher = {Springer US},
	address = {Boston, MA},
	doi = {10.1007/978-1-4615-5025-9_9},
	url = {http://link.springer.com/10.1007/978-1-4615-5025-9_9},
	isbn = {9781461372837},
	issn = {0884-8289},
}

% ORBIT: the original algorithm for solving optimization problems via sequentially minimizing RBF interpolants with a linear tail inside a sequence of trust-regions. I can't remember if it's open source, but Stefan has a high quality numerical software in MATLAB
@article{wild2008orbit,
	author = {Wild, Stefan M. and Regis, Rommel G. and Shoemaker, Christine A.},
	title = {{ORBIT:} {O}ptimization by Radial Basis Function Interpolation in Trust-Regions},
	year = {2008},
	month = {1},
	journal = {SIAM Journal on Scientific Computing},
	volume = {30},
	number = {6},
	pages = {3197--3219},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/070691814},
	url = {http://epubs.siam.org/doi/10.1137/070691814},
	issn = {1064-8275},
}

% Analysis of the convergence rate of RBF-based surrogates with linear tail (fully linear model) inside a sequence of trust regions. This is the theory used in ORBIT
@article{wild2011global,
	author = {Wild, Stefan M. and Shoemaker, Christine A.},
	title = {Global Convergence of Radial Basis Function Trust Region Derivative-Free Algorithms},
	year = {2011},
	month = {7},
	journal = {SIAM Journal on Optimization},
	volume = {21},
	number = {3},
	pages = {761--781},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/09074927X},
	url = {http://epubs.siam.org/doi/10.1137/09074927X},
	issn = {1052-6234},
}

% The POUNDERS composite blackbox / simulation optimization algorithm, which is an open source numerical software package for exploiting the sum-of-squares structure in derivative-free least squares problems. Specifically, POUNDERS models the blackbox function / simulation's outputs using a fully linear model then uses the sum-of-squares structure to get a free Hessian approximation, and achieve second-order convergence for the price of first-order convergence. Although not included, open source numerical software implementations are now available in Python and Matlab through the PyOptUs GitHub group
@incollection{wild2017solving,
	author = {Wild, Stefan M.},
	editor = {Terlaky, Tamas and Anjos, Miguel F. and Ahmed, Shabbir},
	title = {Solving Derivative-Free Nonlinear Least Squares Problems with {POUNDERS}},
	year = {2017},
	month = {4},
	booktitle = {Advances and Trends in Optimization with Engineering Applications},
	pages = {529--540},
	publisher = {SIAM},
	doi = {10.1137/1.9781611974683.ch40},
	url = {http://www.mcs.anl.gov/papers/P5120-0414.pdf},
	isbn = {978-1-611974-67-6},
}

% Hammersley's and Halton sequences -- other low-discrepancy sequences that are commonly used in design-of-experiments
@article{wong1997sampling,
	author = {Wong, Tien-Tsin and Luk, Wai-Shing and Heng, Pheng-Ann},
	title = {Sampling with {H}ammersley and {H}alton points},
	year = {1997},
	journal = {Journal of graphics tools},
	volume = {2},
	number = {2},
	pages = {9--24},
	publisher = {Taylor \& Francis},
}

% A hypervolume-based approach to a multiobjective DIRECT algorithm for multiobjective blackbox / simulation optimization
@inproceedings{wong2016hypervolumebased,
	author = {Wong, Cheryl Sze Yin and Al-Dujaili, Abdullah and Sundaram, Suresh},
	title = {Hypervolume-Based {DIRECT} for Multi-Objective Optimisation},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
	pages = {1201--1208},
	organization = {ACM},
	location = {Denver, CO, USA},
	doi = {10.1145/2908961.2931702},
	url = {https://dl.acm.org/doi/10.1145/2908961.2931702},
}

% A survey of interactive techniques and software in multiobjective optimization including the known challenges limitations
@article{xin2018interactive,
	author = {Xin, Bin and Chen, Lu and Chen, Jie and Ishibuchi, Hisao and Hirota, Kaoru and Liu, Bo},
	title = {Interactive Multiobjective Optimization: A Review of the State-of-the-Art},
	year = {2018},
	journal = {IEEE Access},
	volume = {6},
	pages = {41256--41279},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2018.2856832},
	url = {https://ieeexplore.ieee.org/document/8412189/},
	issn = {2169-3536},
}

% Multiobjective Bayesian optimization with hypervolume improvement-based acquisition
@article{yang2019multiobjective,
	author = {Yang, Kaifeng and Emmerich, Michael and Deutz, Andr{\'{e}} and B\"{a}ck, Thomas},
	title = {Multi-Objective {Bayesian} Global Optimization Using Expected Hypervolume Improvement Gradient},
	year = {2019},
	month = {2},
	journal = {Swarm and Evolutionary Computation},
	volume = {44},
	pages = {945--956},
	publisher = {Elsevier BV},
	doi = {10.1016/j.swevo.2018.10.007},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650217307861},
	issn = {2210-6502},
}

% Multiobjective Bayesian optimization with hypervolume improvement-based acquisition
@inproceedings{yang2019multipoint,
	author = {Yang, Kaifeng and Palar, Pramudita Satria and Emmerich, Michael and Shimoyama, Koji and B\"{a}ck, Thomas},
	title = {A multi-point mechanism of expected hypervolume improvement for parallel multi-objective {Bayesian} global optimization},
	year = {2019},
	month = {7},
	booktitle = {Proc. Genetic and Evolutionary Computation Conference (GECCO19)},
	organization = {ACM},
	location = {Prague Czech Republic},
	doi = {10.1145/3321707.3321784},
	url = {https://dl.acm.org/doi/10.1145/3321707.3321784},
}

% An application of multiobjective optimization for an industrial styrene reactor optimization using BiMADS
@article{yee2003multiobjective,
	author = {Yee, Amy K. Y. and Ray, Ajay K. and Rangaiah, G. P.},
	title = {Multiobjective optimization of an industrial styrene reactor},
	year = {2003},
	month = {1},
	journal = {Computers \& Chemical Engineering},
	volume = {27},
	number = {1},
	pages = {111--130},
	publisher = {Elsevier BV},
	doi = {10.1016/s0098-1354(02)00163-1},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135402001631},
	issn = {0098-1354},
	keywords = {},
}

% Solving DFT model calibrations for chemical design via active learning
@article{yuan2023active,
	author = {Yuan, Xiaoze and Zhou, Yuwei and Peng, Qing and Yang, Yong and Li, Yongwang and Wen, Xiaodong},
	title = {Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials},
	year = {2023},
	month = {1},
	journal = {Nature Computational Materials},
	volume = {9},
	number = {1},
	numpages = {12},
	publisher = {Nature Publishing Group UK London},
	doi = {10.1038/s41524-023-00967-z},
	url = {https://www.nature.com/articles/s41524-023-00967-z},
	issn = {2057-3960},
}

% PhD thesis on extracting high-dimensional (many objective) Pareto fronts including a thorough survey of such algorithms
@phdthesis{yukish2004algorithms,
	author = {Yukish, Michael},
	title = {Algorithms to identify {P}areto points in multi-dimensional data sets},
	year = {2004},
	school = {The Pennsylvania State University, Dept. of Mechanical Engineering},
	url = {https://etda.libraries.psu.edu/catalog/6336},
}

% An algorithm for solving composite sum-of-squares blackbox / simulation optimization problems by modeling the blackbox function / simulation's outputs using a fully linear model then using the sum-of-squares structure to get a free Hessian approximation
@article{zhang2010derivativefree,
	author = {Zhang, Hongchao and Conn, Andrew R. and Scheinberg, Katya},
	title = {A Derivative-Free Algorithm for Least-Squares Minimization},
	year = {2010},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {20},
	number = {6},
	pages = {3555--3576},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/09075531X},
	url = {http://epubs.siam.org/doi/10.1137/09075531X},
	issn = {1052-6234},
}

% Convergence analysis when solving composite sum-of-squares blackbox / simulation optimization problems by modeling the blackbox function / simulation's outputs using a fully linear model then using the sum-of-squares structure to get a free Hessian approximation. Basically, one can achieve second-order convergence for the price of first-order convergence
@article{zhang2012local,
	author = {Zhang, Hongchao and Conn, Andrew R.},
	title = {On the Local Convergence of a Derivative-Free Algorithm for Least-Squares Minimization},
	year = {2012},
	month = {3},
	journal = {Computational Optimization and Applications},
	volume = {51},
	number = {2},
	pages = {481--507},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-010-9367-x},
	url = {http://link.springer.com/10.1007/s10589-010-9367-x},
	issn = {0926-6003},
}

% Prima: open source reference implementation of all of Powell's numerical optimization solvers for blackbox / simulation optimization problems in modern Fortran. IMO, these should be considered the state-of-the-art and reference implementations for all blackbox optimization research
@misc{zhang2023prima,
	author = {Zhang, Zaikun},
	title = {{PRIMA: Reference Implementation for Powell's Methods with Modernization and Amelioration}},
	year = {2023},
	howpublished = {github repository},
	doi = {10.5281/zenodo.8052654},
	url = {http://www.libprima.net},
}

% An application for multiobjective optimization in the context of aircraft wing design. We are optimizing one objective that is the lift/drag ratio, and another that describes the controllability. Problem is solved using multiobjective particle swarm
@inproceedings{zhao2018multiobjective,
	author = {Zhao, Wei and Kapania, Rakesh K.},
	title = {Multiobjective Optimization of Composite Flying-wings with {SpaRibs} and Multiple Control Surfaces},
	year = {2018},
	month = {6},
	booktitle = {Proc. 2018 Multidisciplinary Analysis and Optimization Conference},
	numpages = {3424},
	organization = {AIAA},
	location = {Atlanta, GA, USA},
	doi = {10.2514/6.2018-3424},
	url = {https://arc.aiaa.org/doi/10.2514/6.2018-3424},
}

% The original publication for L-BFGS-B software, which solves bound-constrained optimization problems using a limited-memory BFGS. This is the standard implementation that is used in all L-BFGS-B codes to date, such as scipy, all machine learning codes, and most engineering codes and nonlinear systems solvers. The code is open source high-quality numerical software, written in old-style Fortran
@article{zhu1997algorithm,
	author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
	title = {Algorithm 778: {L-BFGS-B}: {F}ortran Subroutines for Large-Scale Bound-Constrained Optimization},
	year = {1997},
	month = {12},
	journal = {ACM Transactions on Mathematical Software},
	volume = {23},
	number = {4},
	pages = {550--560},
	publisher = {ACM},
	doi = {10.1145/279232.279236},
	url = {https://dl.acm.org/doi/10.1145/279232.279236},
	issn = {0098-3500},
}

% SPEA2 strength Pareto evolutionary algorithm -- an old evolutionary algorithm that was once a competitor to NSGA-II (and with significant overlap in co-authorship), but is now largely obsolete.
@article{zitzler2001spea2,
	author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
	title = {{SPEA2}: Improving the strength {Pareto} evolutionary algorithm},
	year = {2001},
	journal = {TIK-report},
	volume = {103},
	publisher = {Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH Zurich), Institut f{\"u}r Technische},
	doi = {10.3929/ethz-a-004284029},
	git = {https://github.com/manuparra/spea2},
}


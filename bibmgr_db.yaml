adams2022dakota:
  address: Albuquerque, NM, USA
  articleno: null
  authors:
  - - Brian M.
    - Adams
  - - William J.
    - Bohnhoff
  - - Keith R.
    - Dalbey
  - - Mohamed S.
    - Ebeida
  - - John P.
    - Eddy
  - - Michael S.
    - Eldred
  - - Russell W.
    - Hooper
  - - Patricia D.
    - Hough
  - - Kenneth T.
    - Hu
  - - John D.
    - Jakeman
  - - Mohammad
    - Khalil
  - - Kathryn A.
    - Maupin
  - - Jason A.
    - Monschke
  - - Elliott M.
    - Ridgeway
  - - Ahmad A.
    - Rushdi
  - - D. Thomas
    - Seidl
  - - J. Adam
    - Stephens
  - - Laura P.
    - Swiler
  - - Anh
    - Tran
  - - Justin G.
    - Winokur
  chapter: null
  descrip: The Dakota blackbox and derivative-free simulation optimization framework,
    a numerical software package (in C++) maintained by Sandia that offers support
    for AI/ML surrogate modeling, multifidelity modeling, uncertainty quantification
    (UQ), and distributed and parallel computing
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: SAND2022-6171 version 6.16
  pages: null
  publisher: Sandia National Laboratory
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  - high-performance computing
  - HPC
  - simulation
  - distributed computing
  - parallel computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - software
  - parallel programming
  - C++
  title: 'Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization,
    Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version
    6.16 User''s Manual'
  type: techreport
  url: https://dakota.sandia.gov/sites/default/files/docs/6.16.0/Users-6.16.0.pdf
  venue: null
  volume: null
  web: null
  year: 2022
agrawal2019differentiable:
  address: null
  articleno: null
  authors:
  - - Akshay
    - Agrawal
  - - Brandon
    - Amos
  - - Shane
    - Barratt
  - - Stephen
    - Boyd
  - - Steven
    - Diamond
  - - J. Zico
    - Kolter
  chapter: null
  descrip: Differentiabl convex optimization layers are introduced pytorch and tensorflow
    via an update to cvxpy, introducing differentiation through the solution to a
    convex optimization problem modeled in cvxpy. The open source numerical optimization
    software is available at github.com/cvxgrp/cvxpylayers in Python
  doi: null
  edition: null
  editors:
  - - H.
    - Wallach
  - - H.
    - Larochelle
  - - A.
    - Beygelzimer
  - - F. d\textquotesingle
    - Alch\'{e}-Buc
  - - E.
    - Fox
  - - R.
    - Garnett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - optimization
  - convex optimization
  - autograd
  - algorithmic differentiation
  - backpropagation
  - neural networks
  - software
  - open source
  - OSS
  - Python
  title: Differentiable Convex Optimization Layers
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '32'
  web: null
  year: 2019
aharonov2008adiabatic:
  address: null
  articleno: null
  authors:
  - - Dorit
    - Aharonov
  - - Wim
    - Van Dam
  - - Julia
    - Kempe
  - - Zeph
    - Landau
  - - Seth
    - Lloyd
  - - Oded
    - Regev
  chapter: null
  descrip: This paper shows the equivalence of adiabatic quantum computing and the
    gate model
  doi: 10.1137/080734479
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0036-1445
  month: '1'
  note: null
  number: '4'
  pages:
  - '755'
  - '787'
  publisher: SIAM
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  title: Adiabatic quantum computation is equivalent to standard quantum computation
  type: article
  url: http://epubs.siam.org/doi/10.1137/080734479
  venue: SIAM review
  volume: '50'
  web: null
  year: 2008
akhtar2016multi:
  address: null
  articleno: null
  authors:
  - - Taimoor
    - Akhtar
  - - Christine A.
    - Shoemaker
  chapter: null
  descrip: Using RBF surrogates to solve blackbox / derivative-free multiobjective
    optimization problems
  doi: 10.1007/s10898-015-0270-y
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '1'
  note: null
  number: '1'
  pages:
  - '17'
  - '32'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: Multi objective optimization of computationally expensive multi-modal functions
    with {RBF} surrogates and multi-rule selection
  type: article
  url: http://link.springer.com/10.1007/s10898-015-0270-y
  venue: Journal of Global Optimization
  volume: '64'
  web: null
  year: 2016
akiba2019optuna:
  address: Anchorage AK USA
  articleno: null
  authors:
  - - Takuya
    - Akiba
  - - Shotaro
    - Sano
  - - Toshihiko
    - Yanase
  - - Takeru
    - Ohta
  - - Masanori
    - Koyama
  chapter: null
  descrip: Optuna open source software for fully distributed hyperparameter optimization.
    Maintained by a private startup in Japan (named Optuna). This software provides
    high-quality distributed wrappers for many neural architecture search and generic
    optimization algorithms.
  doi: 10.1145/3292500.3330701
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '2623'
  - '2631'
  publisher: ACM
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - optimization
  - decision trees
  - Bayesian optimization
  - global optimization
  - autotuning
  - hyperparameter optimization
  - evolutionary algorithms
  - EA
  - Gaussian process
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - uncertainty quantification
  - UQ
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: 'Optuna: A next-generation hyperparameter optimization framework'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3292500.3330701
  venue: Proceedings of the 25th ACM SIGKDD international conference on knowledge
    discovery \& data mining
  volume: null
  web: null
  year: 2019
albash2018adiabatic:
  address: null
  articleno: null
  authors:
  - - Tameem
    - Albash
  - - Daniel A
    - Lidar
  chapter: null
  descrip: Review article on adiabatic quantum computing
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - 15002
  publisher: APS
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  title: Adiabatic quantum computation
  type: article
  url: https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.90.015002
  venue: Reviews of Modern Physics
  volume: '90'
  web: null
  year: 2018
aldujaili2016dividing:
  address: Vancouver, BC, Canada
  articleno: null
  authors:
  - - Abdullah
    - Al-Dujaili
  - - Sundaram
    - Suresh
  chapter: null
  descrip: Introducing an algorithm for solving multiobjective optimization (MOO)
    problems via the global optimization algorithm DIRECT. Some theory and preliminary
    results, but no software. Likely not scalable for real-world computationally expensive
    problems, due to the number of boxes that would need to be divided per iteration
    using this method. More of a theoretical foundation for a later practical algorithm
  doi: 10.1109/CEC.2016.7744246
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '3606'
  - '3613'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  title: Dividing rectangles attack multi-objective optimization
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/7744246
  venue: Proc. 2016 IEEE Congress on Evolutionary Computation (CEC '16)
  volume: null
  web: null
  year: 2016
aldujaili2016matlab:
  address: Denver, CO, USA
  articleno: null
  authors:
  - - Abdullah
    - Al-Dujaili
  - - Sundaram
    - Suresh
  chapter: null
  descrip: A numerical software package written in MATLAB -- provides a surrogate
    modeling toolbox for multiobjective optimization problems
  doi: 10.1145/2908961.2931703
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '1209'
  - '1216'
  publisher: ACM
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - surrogate modeling
  - RBFs
  title: 'A {MATLAB} toolbox for surrogate-assisted multi-objective optimization:
    A preliminary study'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/2908961.2931703
  venue: Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO
    '16)
  volume: null
  web: null
  year: 2016
alghassi2019graver:
  address: null
  articleno: null
  authors:
  - - Hedayat
    - Alghassi
  - - Raouf
    - Dridi
  - - Sridhar
    - Tayur
  chapter: null
  descrip: Quantum annealing algorithm for addition, which I suspected to be done
    with the less efficient classical addition circuit, they don't say otherwise
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - quantum computing
  title: Graver Bases via Quantum Annealing with Application to Non-Linear Integer
    Programs
  type: article
  url: https://arxiv.org/pdf/1902.04215.pdf
  venue: arXiv preprint arXiv:1902.04215
  volume: null
  web: null
  year: 2019
alizadeh2020managing:
  address: null
  articleno: null
  authors:
  - - Reza
    - Alizadeh
  - - Janet K.
    - Allen
  - - Farrokh
    - Mistree
  chapter: null
  descrip: A survey and review of surrogate modeling and response-surface modeling
    (RSM) techniques used in engineering
  doi: 10.1007/s00163-020-00336-7
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0934-9839
  month: '7'
  note: null
  number: '3'
  pages:
  - '275'
  - '298'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - surrogate modeling
  - RBFs
  title: 'Managing computational complexity using surrogate models: a critical review'
  type: article
  url: https://link.springer.com/10.1007/s00163-020-00336-7
  venue: Research in Engineering Design
  volume: '31'
  web: null
  year: 2020
amdvivadodevelopers2024vivado:
  address: null
  articleno: null
  authors:
  - - ''
    - AMD~Vivado~Developers
  chapter: null
  descrip: The official AMD Vivado docs -- Vivado is Xilinx (acquired by AMD)'s analytic
    placer, which is currently considered the state-of-the-art and only real option
    for placement and routing in industrial FPGA pnr applications
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Feb 2025'
  number: Version 2024.1
  pages: null
  publisher: AMD
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - Tcl
  title: Vivado Design Suite User Guide
  type: misc
  url: https://docs.amd.com/r/2024.1-English/ug893-vivado-ide
  venue: null
  volume: null
  web: null
  year: 2024
amos2017optnet:
  address: null
  articleno: null
  authors:
  - - Brandon
    - Amos
  - - J. Zico
    - Kolter
  chapter: null
  descrip: Brandon's original paper on differentiabl optimization layers in neural
    networks -- i.e., solving an optimization problem within a layer of a neural network
    and propogating the gradient through that solution so that the network parameters
    can still be trained
  doi: null
  edition: null
  editors:
  - - Doina
    - Precup
  - - Yee Whye
    - Teh
  git: null
  isbn: null
  issn: null
  month: 06--11 Aug
  note: null
  number: null
  pages:
  - '136'
  - '145'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - optimization
  - autograd
  - algorithmic differentiation
  - backpropagation
  - neural networks
  title: '{O}pt{N}et: Differentiable Optimization as a Layer in Neural Networks'
  type: inproceedings
  url: https://proceedings.mlr.press/v70/amos17a.html
  venue: Proceedings of the 34th International Conference on Machine Learning
  volume: '70'
  web: null
  year: 2017
amos2020algorithm:
  address: null
  articleno: null
  authors:
  - - Brandon D.
    - Amos
  - - David R.
    - Easterling
  - - Layne T.
    - Watson
  - - William I.
    - Thacker
  - - Brent S.
    - Castle
  - - Michael W.
    - Trosset
  chapter: null
  descrip: A Fortran 90 implementation of quasi-Newton stochastic optimization algorithms.
    This open source numerical software solves both determinisitc and stochastic blackbox
    optimization problems via a quasi-Newton trust-region method. It is a bit wasteful
    in terms of the number of function evaluations per iteration as it performs a
    fully Latin hypercube sampling of the trust region in each iteration, and does
    not explicitly re-use previous iterates to reduce iteration costs, like some of
    the more advanced model based methods. Still, it is extremely robust and a good
    choice in stochastic situations. Also includes a good Fortran implementation of
    Latin hypercube sampling and efficient sorting algorithms
  doi: 10.1145/3374219
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '6'
  note: null
  number: '2'
  pages:
  - 17
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - design of experiments
  - DoE
  - Latin hypercube sampling
  - LHS
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - stochastic optimization
  - convex optimization
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 1007: {QNSTOP}: {Q}uasi-{N}ewton algorithm for stochastic optimization'
  type: article
  url: https://dl.acm.org/doi/10.1145/3374219
  venue: ACM Transactions on Mathematical Software
  volume: '46'
  web: null
  year: 2020
amos2023tutorial:
  address: Hanover, MA, USA
  articleno: null
  authors:
  - - Brandon
    - Amos
  chapter: null
  descrip: Brandon introduces using neural networks and other scientific machine learning
    methods to warm-start optimization and control problems. An open source numerical
    software tutorial is available at github.com/facebookresearch/amortized-optimization-tutorial
  doi: 10.1561/2200000102
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1935-8237
  month: null
  note: null
  number: '5'
  pages:
  - 156
  publisher: Now Publishers Inc.
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - optimization
  - convex optimization
  - neural networks
  - regression
  title: Tutorial on Amortized Optimization
  type: article
  url: https://doi.org/10.1561/2200000102
  venue: Found. Trends Mach. Learn.
  volume: '16'
  web: null
  year: 2023
andersen2000mosek:
  address: null
  articleno: null
  authors:
  - - Erling D
    - Andersen
  - - Knud D
    - Andersen
  chapter: null
  descrip: The MOSEK solver is a numerical optimization software solver that is used
    in older versions of scipy.optimize to solve linear programming problems via the
    interior point method
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '197'
  - '232'
  publisher: Springer
  series: null
  tags:
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  - software
  - Python
  - Matlab
  - Go
  - Julia
  title: 'The {MOSEK} interior point optimizer for linear programming: an implementation
    of the homogeneous algorithm'
  type: inproceedings
  url: https://link.springer.com/chapter/10.1007/978-1-4757-3216-0_8
  venue: High performance optimization
  volume: null
  web: null
  year: 2000
anderson1999lapack:
  address: Philidelphia, PA, USA
  articleno: null
  authors:
  - - E.
    - Anderson
  - - Z.
    - Bai
  - - C.
    - Bischof
  - - S.
    - Blackford
  - - J.
    - Demmel
  - - J.
    - Dongarra
  - - J.
    - Du Croz
  - - A.
    - Greenbaum
  - - S.
    - Hammarling
  - - A.
    - McKenney
  - - D.
    - Sorensen
  chapter: null
  descrip: 'The user guide for the reference implementation of LAPACK: the original
    open source numerical software for all common dense linear algebra operations.'
  doi: null
  edition: '3'
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: SIAM
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - software
  - open source
  - OSS
  - Fortran
  title: '{LAPACK} Users'' Guide'
  type: book
  url: https://netlib.org/lapack/lug/
  venue: null
  volume: null
  web: null
  year: 1999
andreani2022using:
  address: null
  articleno: null
  authors:
  - - R.
    - Andreani
  - - Ana Lu{\'i}sa
    - Cust{\'o}dio
  - - M.
    - Raydan
  chapter: null
  descrip: Is derivative / gradient information counterproductive for solving MOOs?
    In this paper, it appears to make direct-search type methods perform worse by
    the metrics used. It could be that the metrics favor diversity over convergence,
    in which case one can get better diversity by taking bad evaluations. But I need
    to read more carefully to decide whether that is what's going on here
  doi: 10.1080/10556788.2022.2060971
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1055-6788
  month: '11'
  note: null
  number: '6'
  pages:
  - '2135'
  - '2156'
  publisher: Taylor \& Francis
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: Using first-order information in direct multisearch for multiobjective optimization
  type: article
  url: https://www.tandfonline.com/doi/full/10.1080/10556788.2022.2060971
  venue: Optimization Methods and Software
  volume: '37'
  web: null
  year: 2022
andresthio2025solar:
  address: null
  articleno: null
  authors:
  - - Nicolau
    - Andr\'{e}s-Thi\'{o}
  - - Charles
    - Audet
  - - Miguel
    - Diago
  - - A{\"i}men E.
    - Gheribi
  - - Sebastien
    - Le~Digabel
  - - Xavier
    - Lebeuf
  - - Mathieu
    - Lemyre~Garneau
  - - Christophe
    - Tribes
  chapter: null
  descrip: 'The SOLAR monte carlo simulation software suite contains a collection
    of ten derivative-free / blackbox / simulation optimization problem instances
    for bench-marking blackbox optimization solvers. The problems can be configured
    with one or two objectives, constrained or unconstrained (including hidden constraints),
    discrete or continuous, single or multi-fidelity, and with varying levels of stochasticity.
    The numerical simulation code models a solar power plant''s yield as a function
    of various design factors. The open source C++ software with Python bindings is
    available at: github.com/bbopt/solar'
  doi: 10.1007/s11081-024-09952-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1389-4420
  month: '3'
  note: null
  number: null
  pages: null
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - stochastic optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Python
  - C++
  title: 'solar: A solar thermal power plant simulator for blackbox optimization benchmarking'
  type: article
  url: https://link.springer.com/10.1007/s11081-024-09952-x
  venue: Optimization and Engineering
  volume: null
  web: null
  year: 2025
andriyash2016boosting:
  address: null
  articleno: null
  authors:
  - - Evgeny
    - Andriyash
  - - Zhengbing
    - Bian
  - - Fabian
    - Chudak
  - - Marshall
    - Drew-Brook
  - - Andrew D
    - King
  - - William G
    - Macready
  - - Aidan
    - Roy
  chapter: null
  descrip: D-Wave article describing the multiplication circuit
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: D-Wave
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Boosting integer factoring performance via quantum annealing offsets
  type: techreport
  url: https://www.dwavesys.com/sites/default/files/14-1002A_B_tr_Boosting_integer_factorization_via_quantum_annealing_offsets.pdf
  venue: null
  volume: null
  web: null
  year: 2016
applegate2021practical:
  address: null
  articleno: null
  authors:
  - - David
    - Applegate
  - - Mateo
    - Diaz
  - - Oliver
    - Hinder
  - - Haihao
    - Lu
  - - Miles
    - Lubin
  - - Brendan
    - O\textquotesingle Donoghue
  - - Warren
    - Schudy
  chapter: null
  descrip: 'Introducing the open source PDLP solver (now part of Google OR tools)
    for solving large scale linear programming problems, where the constraint matrices
    would never fit in memory using first-order (matrix free) descent methods. The
    algorithm is based on PDHG, which is an improvement to ADMM specialized for solving
    LPs. The main contributions of this solver and its predecessor PDHG are adaptive
    step-sizes and preconditioning to prevent oscillatory motions plus adpaptive an
    adaptive restart procedures and presolve to prevent stalling out. This solver
    is not as fast simplex or interior point methods on problems that do fit in memory,
    but is extremely scalable and robust on larger problems. Obviously, it cannot
    give a basic solution since first-order methods are always approximate (with order
    10^-8 accuracy). Open source code is available through Google OR tools: github.com/google/or-tools'
  doi: null
  edition: null
  editors:
  - - M.
    - Ranzato
  - - A.
    - Beygelzimer
  - - Y.
    - Dauphin
  - - P.S.
    - Liang
  - - J. Wortman
    - Vaughan
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '20243'
  - '20257'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  - high dimension
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - C
  - Python
  - Julia
  title: Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2021/file/a8fbbd3b11424ce032ba813493d95ad7-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '34'
  web: null
  year: 2021
astudillo2021thinking:
  address: Phoenix, Arizona
  articleno: '2'
  authors:
  - - Raul
    - Astudillo
  - - Peter I.
    - Frazier
  chapter: null
  descrip: 'Applying grey-box bayesian optimization tutorial: using Bayesian optimization
    on structured problems, where a blackbox function is composed with an algebraic
    function, just like with ParMOO and Jeff''s GOOMBAH paper. Tutorial performed
    using BoTorch'
  doi: 10.1109/WSC52266.2021.9715343
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - 15
  publisher: IEEE
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - Gaussian process
  title: 'Thinking inside the box: a tutorial on grey-box bayesian optimization'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9715343
  venue: Proc. 2021 Winter Simulation Conference (WSC 2021)
  volume: null
  web: null
  year: 2021
audet2008multiobjective:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Gilles
    - Savard
  - - Walid
    - Zghal
  chapter: null
  descrip: BiMADS -- a biobjective direct search / generalized pattern search via
    the MADS algorithm with an adaptive weighting scheme to trace the Pareto front
    from one end to the other. The numerical software implementation is part of the
    NOMAD software package (written in C++)
  doi: 10.1137/060677513
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '1'
  pages:
  - '188'
  - '210'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  title: Multiobjective optimization through a series of single-objective formulations
  type: article
  url: http://epubs.siam.org/doi/10.1137/060677513
  venue: SIAM Journal on Optimization
  volume: '19'
  web: null
  year: 2008
audet2008nonsmooth:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Vincent
    - B\'echard
  - - Sebastien
    - Le~Digabel
  chapter: null
  descrip: Introducing the MADS algorithm of variable neighborhood search plus a generalized
    pattern search adapted to a mesh. Also includes the STYRENE benchmark problem
    and application for derivative-free blackbox and simulation optimization algorithms.
    The objective is to maximize the net present value subject to several process
    and economic constraints. The open source C++ numerical simulation code that defines
    the problem is available at github.com/bbopt/styrene
  doi: 10.1007/s10898-007-9234-1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '6'
  note: null
  number: '2'
  pages:
  - '299'
  - '318'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - constrained optimization
  - benchmarking
  - software
  - open source
  - OSS
  - C++
  title: Nonsmooth optimization through Mesh Adaptive Direct Search and Variable Neighborhood
    Search
  type: article
  url: http://link.springer.com/10.1007/s10898-007-9234-1
  venue: Journal of Global Optimization
  volume: '41'
  web: null
  year: 2008
audet2009progressive:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - John E.
    - Dennis
  chapter: null
  descrip: The progressive barrier penalty for nonlinear blackbox optimization methods.
    Basically adds a progressive penalty for violating constraints based on the distance
    to feasibility
  doi: 10.1137/070692662
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '1'
  pages:
  - '445'
  - '472'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  title: A Progressive Barrier for Derivative-Free Nonlinear Programming
  type: article
  url: http://epubs.siam.org/doi/10.1137/070692662
  venue: SIAM Journal on Optimization
  volume: '20'
  web: null
  year: 2009
audet2010mesh:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Gilles
    - Savard
  - - Walid
    - Zghal
  chapter: null
  descrip: Multi-MADS -- a multiobjective direct search / generalized pattern search
    via MADS algorithm, using normal boundary intersection (NBI) for adaptive weighting
    Also includes a multiobjective (three objective) variation of the STYRENE benchmark
    for derivative-free blackbox and simulation optimization problems. The STYRENE
    benchmark problem and application for derivative-free blackbox and simulation
    optimization algorithms. The objective is to maximize the net present value subject
    to several process and economic constraints. The open source C++ numerical simulation
    code that defines the problem is available at github.com/bbopt/styrene
  doi: 10.1016/j.ejor.2009.11.010
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0377-2217
  month: '8'
  note: null
  number: '3'
  pages:
  - '545'
  - '556'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - constrained optimization
  - benchmarking
  title: A mesh adaptive direct search algorithm for multiobjective optimization
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0377221709008601
  venue: European Journal of Operational Research
  volume: '204'
  web: null
  year: 2010
audet2017derivativefree:
  address: Charm, Switzerland
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Warren
    - Hare
  chapter: null
  descrip: Book on fundamental methods, terminology, and theory in blackbox and derivative-free
    optimization (DFO) -- covers topics such as definitions of blackbox functions,
    heuristics, classical methods, positive bases and minimum spanning sets, generalized
    pattern search, and direct search, fully linear and quadratic models, model-drive
    descent and trust-region methods and ensuring model quality, general surrogate
    modeling, constraints, and multiobjective basics.
  doi: 10.1007/978-3-319-68913-5
  edition: null
  editors: []
  git: null
  isbn: '9783319689128'
  issn: 1431-8598
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer International
  series: Springer Series in Operations Research and Financial Engineering
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - constrained optimization
  - surrogate modeling
  title: Derivative-free and blackbox optimization
  type: book
  url: http://link.springer.com/10.1007/978-3-319-68913-5
  venue: Springer Series in Operations Research and Financial Engineering
  volume: null
  web: null
  year: 2017
audet2021performance:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Jean
    - Bigeon
  - - Dominique
    - Cartier
  - - "S\xE9bastien Le"
    - Digabel
  - - Ludovic
    - Salomon
  chapter: null
  descrip: 'A thorough survey of over 50 commonly used performance indicators in multiobjective
    optimization -- key takeaways: performance metrics can measure different properties
    of an algorithm, such as whether it is converging to the true Pareto front, the
    coverage of the true Pareto front, and the average diversity of solutions. One
    of the only metrics that is monotonic (i.e., cannot become worse when a solution
    contains a previous solution) is the hypervolume indicator, which is the standard
    in evolutionary algorithms'
  doi: 10.1016/j.ejor.2020.11.016
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0377-2217
  month: '7'
  note: null
  number: '2'
  pages:
  - '397'
  - '422'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Performance indicators in multiobjective optimization
  type: article
  url: https://www.sciencedirect.com/science/article/pii/S0377221720309620
  venue: European Journal of Operational Research
  volume: '292'
  web: null
  year: 2021
audet2021stochastic:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Kwassi Joseph
    - Dzahini
  - - Michael
    - Kokkolaras
  - - Sebastien
    - Le~Digabel
  chapter: null
  descrip: 'Official StoMADS reference: StoMADS is the stochastic version of the MADS
    (generalized pattern search) optimization algorithm and software. The open source
    StoMADS numerical software is available at github.com/bbopt/StoMADS. It is implemented
    in MATLAB. The main contribution of StoMADS is to calculate how many resamples
    and shrinking step sizes are needed to guarantee convergence of the MADS algorithm
    on problems where the blackbox function / simulation evaluation is stochastic'
  doi: 10.1007/s10589-020-00249-0
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '5'
  note: null
  number: '1'
  pages:
  - '1'
  - '34'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - stochastic optimization
  - software
  - open source
  - OSS
  - Matlab
  title: Stochastic mesh adaptive direct search for blackbox optimization using probabilistic
    estimates
  type: article
  url: https://doi.org/10.1007/s10589-020-00249-0
  venue: Computational Optimization and Applications
  volume: '79'
  web: null
  year: 2021
audet2022algorithm:
  address: null
  articleno: '35'
  authors:
  - - Charles
    - Audet
  - - S\'ebastien
    - Le Digabel
  - - Viviane
    - Rochon Montplaisir
  - - Christophe
    - Tribes
  chapter: null
  descrip: NOMAD v4 -- open source numerical software package (in C++) for blackbox
    and derivative-free optimization via the MADS algorithms. After publication, they
    have added support for multiobjective optimization, mixed variables, nonlinear
    constraints, etc. Great example of high-impact open source numerical and optimization
    software. Improvements over NOMAD v3 include improvements to fundamental algorithms,
    coding practices, release process, and general project structure to support continuous
    research and development into the future
  doi: 10.1145/3544489
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '9'
  note: null
  number: '3'
  pages:
  - 22
  publisher: ACM
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - C++
  - Python
  - parallel programming
  title: '{Algorithm 1027}: {NOMAD} Version 4: Nonlinear Optimization with the {MADS}
    Algorithm'
  type: article
  url: https://dl.acm.org/doi/10.1145/3544489
  venue: ACM Transactions on Mathematical Software
  volume: '48'
  web: null
  year: 2022
audet2023general:
  address: null
  articleno: null
  authors:
  - - Charles
    - Audet
  - - Edward
    - Hall{\'e}-Hannan
  - - S{\'e}bastien
    - Le Digabel
  chapter: null
  descrip: 'Handling categorical/integer/mixed variables in blackbox optimization:
    This is the method used to perform hyperparameter tuning of neural-networks and
    other AI models via MADS. In general, they decompose variables into meta variables
    (which determine whether other variables are active or not, such as the number
    of layers in the network which can deactivate variables associated with inactive
    layers), categorical variables (which either need to be embedded somehow or can
    be explored in an unordered manner via direct search / generalized pattern search),
    and finally standard variables which includes both continuous and relaxed integer
    variables'
  doi: 10.1007/s43069-022-00180-6
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2662-2556
  month: '2'
  note: null
  number: '1'
  pages:
  - 12
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  title: A general mathematical framework for constrained mixed-variable blackbox
    optimization problems with meta and categorical variables
  type: inproceedings
  url: https://link.springer.com/10.1007/s43069-022-00180-6
  venue: Operations Research Forum
  volume: '4'
  web: null
  year: 2023
aurenhammer2013voronoi:
  address: Hackensack, NJ, USA
  articleno: null
  authors:
  - - Franz
    - Aurenhammer
  - - Rolf
    - Klein
  - - Der-Tsai
    - Lee
  chapter: null
  descrip: A very thorough textbook on properties and algorithms for computing Voronoi
    tesselations and Delaunay triangulations, and the duality relationship between
    them. Covers basic and advanced properties of each and their duality, basic algorithms
    for computing each in two and three dimensions and their time and space complexities,
    all basic algorithms for computing each in high dimensions, and finally some algorithms
    and definitions for working in generalized metric spaces. Finally, some common
    applications of each are mentioned
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: World Scientific Publishing Co.
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  - high dimension
  title: Voronoi diagrams and {D}elaunay triangulations
  type: book
  url: https://www.worldscientific.com/worldscibooks/10.1142/8685?srsltid=AfmBOoqYyCEcMxI7JQyJKonU2CZAy-dXYFcqnxJkrlJCSsAMHLbMPI70#t=aboutBook
  venue: null
  volume: null
  web: null
  year: 2013
ba2016layer:
  address: null
  articleno: null
  authors:
  - - Jimmy Lei
    - Ba
  - - Jamie Ryan
    - Kiros
  - - Geoffrey E.
    - Hinton
  chapter: null
  descrip: 'Introducing layer normalization: as opposed to batch normalization, where
    we normalize each dimension to a Gaussian (with trainable mean and standard deviation)
    within each batch, we instead normalize either the inputs to or outputs of each
    layer. This normalization helps stabilize the neural network layer inputs during
    training, so that adjusting the weights to an earlier layer doesn''t completely
    invalidate the weights at a later layer within a single step of training, and
    allows the weights in each layer to adjust gradually and together'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - benchmarking
  - classification
  - neural networks
  - CNNs
  - regularization
  - overfitting
  - optimization
  title: Layer Normalization
  type: inproceedings
  url: https://arxiv.org/abs/1607.06450v1
  venue: Proceedings of the 30th International Conference on Neural Information Processing
    Systems (NIPS 2016) Deep Learning Symposium
  volume: null
  web: null
  year: 2016
balandat2020botorch:
  address: null
  articleno: null
  authors:
  - - Maximilian
    - Balandat
  - - Brian
    - Karrer
  - - Daniel
    - Jiang
  - - Samuel
    - Daulton
  - - Ben
    - Letham
  - - Andrew G
    - Wilson
  - - Eytan
    - Bakshy
  chapter: null
  descrip: 'BoTorch: Modular bayesian optimization framework and numerical software
    package. Uses Ax for deploying A/B testing applications. Uses pytorch for autograd,
    and uses monte carlo sampling and kernel reparameterization tricks in order to
    efficiently evaluate composite objective and acquisition functions and non gaussian
    kernels. Great example of high-impact open source numerical software and optimization
    software'
  doi: null
  edition: null
  editors:
  - - H.
    - Larochelle
  - - M.
    - Ranzato
  - - R.
    - Hadsell
  - - M.F.
    - Balcan
  - - H.
    - Lin
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '21524'
  - '21538'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - high-performance computing
  - HPC
  - autograd
  - algorithmic differentiation
  - backpropagation
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - surrogate modeling
  - Gaussian process
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - uncertainty quantification
  - UQ
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: '{BoTorch}: A Framework for Efficient {M}onte-{C}arlo {B}ayesian Optimization'
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '33'
  web: null
  year: 2020
balaprakash2018deephyper:
  address: Bengaluru, India
  articleno: null
  authors:
  - - Prasanna
    - Balaprakash
  - - Michael
    - Salim
  - - Thomas D.
    - Uram
  - - Venkat
    - Vishwanath
  - - Stefan M.
    - Wild
  chapter: null
  descrip: Prasanna and Stefan's original DeepHyper publication -- DeepHyper is an
    open source extreme-scale distributed optimization package, designed to scale
    to Argonne's exascale HPCs. Able to train ensembles of neural networks with diverse
    architectures at scale, with both single and multiobjective hyperparameter tuning
    support
  doi: 10.1109/hipc.2018.00014
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '42'
  - '51'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - optimization
  - decision trees
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - uncertainty quantification
  - UQ
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  - MPI
  title: '{DeepHyper}: Asynchronous hyperparameter search for deep neural networks'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8638041
  venue: IEEE 25th international conference on high performance computing (HiPC)
  volume: null
  web: null
  year: 2018
balay2022petsctao:
  address: Lemont, IL, USA
  articleno: null
  authors:
  - - Satish
    - Balay
  - - Shrirang
    - Abhyankar
  - - Mark F.
    - Adams
  - - Steven
    - Benson
  - - Jed
    - Brown
  - - Peter
    - Brune
  - - Kris
    - Buschelman
  - - Emil
    - Constantinescu
  - - Lisandro
    - Dalcin
  - - Alp
    - Dener
  - - Victor
    - Eijkhout
  - - William D.
    - Gropp
  - - V\'{a}clav
    - Hapla
  - - Tobin
    - Isaac
  - - Pierre
    - Jolivet
  - - Dmitry
    - Karpeev
  - - Dinesh
    - Kaushik
  - - Matthew G.
    - Knepley
  - - Fande
    - Kong
  - - Scott
    - Kruger
  - - Dave A.
    - May
  - - Lois Curfman
    - McInnes
  - - Richard Tran
    - Mills
  - - Lawrence
    - Mitchell
  - - Todd
    - Munson
  - - Jose E.
    - Roman
  - - Karl
    - Rupp
  - - Patrick
    - Sanan
  - - Jason
    - Sarich
  - - Barry F.
    - Smith
  - - Stefano
    - Zampini
  - - Hong
    - Zhang
  - - Hong
    - Zhang
  - - Junchao
    - Zhang
  chapter: null
  descrip: The PETSc user's guide. I haven't used it but PETSc is a widely-used C++
    numerical software library and linear algebra / iterative algorithms framework
    developed at Argonne and used for implementing many well-known iterative solvers,
    especially in the area of CFD. This is a great example of high-impact open source
    numerical software and best practices in open source scientific software. Now
    ships together with TAO, a similar simulation optimization software package
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: ANL-21/39 - Revision 3.17
  pages: null
  publisher: Argonne National Laboratory
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - computational linear algebra
  - optimization
  - simulation optimization
  - convex optimization
  - software
  - open source
  - OSS
  - C
  - C++
  title: '{PETSc/TAO} Users Manual'
  type: techreport
  url: https://petsc.org/release/docs/manual/manual.pdf
  venue: null
  volume: null
  web: null
  year: 2022
ball1992sensitivity:
  address: null
  articleno: null
  authors:
  - - Keith
    - Ball
  - - Natarajan
    - Sivakumar
  - - Joseph D
    - Ward
  chapter: null
  descrip: An analysis showing that the condition number of the RBF (and GP) interpolation
    matrices grow (and lowering bounding them) as the minimum separation distance
    shrinks. I.e., when data gets too dense, the RBF conditioning is guaranteed to
    become arbitrarily bad.
  doi: 10.1007/bf01203461
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0176-4276
  month: '12'
  note: null
  number: '4'
  pages:
  - '401'
  - '426'
  publisher: Springer
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - approximation theory
  title: On the sensitivity of radial basis interpolation to minimal data separation
    distance
  type: article
  url: http://link.springer.com/10.1007/BF01203461
  venue: Constructive Approximation
  volume: '8'
  web: null
  year: 1992
bambade2022proxqp:
  address: New York, United States
  articleno: null
  authors:
  - - Antoine
    - Bambade
  - - Sarah
    - El-Kazdadi
  - - Adrien
    - Taylor
  - - Justin
    - Carpentier
  chapter: null
  descrip: 'Official PROX-QP paper -- the best open source QP solver according to
    several benchmarks. Open source C++ code is availabe through the proxsuite github:
    https://github.com/Simple-Robotics/proxsuite and a Python interface is available
    through CvxPy'
  doi: 10.15607/rss.2022.xviii.040
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: June
  note: null
  number: null
  pages: null
  publisher: 'Robotics: Science and Systems Foundation'
  series: null
  tags:
  - computational geometry
  - projection
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - Python
  - C++
  title: '{PROX-QP}: Yet another Quadratic Programming Solver for Robotics and beyond'
  type: inproceedings
  url: https://hal.inria.fr/hal-03683733
  venue: 'RSS 2022 - Robotics: Science and Systems'
  volume: null
  web: null
  year: 2022
bandyopadhyay2008simulated:
  address: null
  articleno: null
  authors:
  - - Sanghamitra
    - Bandyopadhyay
  - - Sriparna
    - Saha
  - - Ujjwal
    - Maulik
  - - Kalyanmoy
    - Deb
  chapter: null
  descrip: AMOSA algorithm -- apparently this is a widely-known standard in multiobjective
    simulated annealing because reviewers regularly ask me to cite this. But I've
    never met anyone who uses this and I can't find the software anywhere. The algorithm
    seems very reasonable though
  doi: 10.1109/TEVC.2007.900837
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1941-0026
  month: '6'
  note: null
  number: '3'
  pages:
  - '269'
  - '283'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - simulated annealing
  - SA
  title: 'A Simulated Annealing-Based Multiobjective Optimization Algorithm: {AMOSA}'
  type: article
  url: http://ieeexplore.ieee.org/document/4358775
  venue: IEEE Transactions on Evolutionary Computation
  volume: '12'
  web: null
  year: 2008
bansal2022jahsbench201:
  address: null
  articleno: null
  authors:
  - - Archit
    - Bansal
  - - Danny
    - Stoll
  - - Maciej
    - Janowski
  - - Arber
    - Zela
  - - Frank
    - Hutter
  chapter: null
  descrip: 'JAHS-Bench-201: The latest test suite of benchmark problems for neural
    architecture search. The baseline is random search, but you can solve the problems
    in their parameterized search space with any optimization algorithm, record the
    number of true function / simulation evaluations (i.e., networks trained) and
    submit this to the JAHS-Bench leaderboards on GitHub. This is a good representative
    test problem for NAS. Both single and multiobjective benchmarks are provided,
    also most problems can be run in both single or multifidelity evaluation modes'
  doi: null
  edition: null
  editors:
  - - S.
    - Koyejo
  - - S.
    - Mohamed
  - - A.
    - Agarwal
  - - D.
    - Belgrave
  - - K.
    - Cho
  - - A.
    - Oh
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '38788'
  - '38802'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - benchmarking
  - software
  - Python
  title: '{JAHS-Bench-201}: A Foundation For Research On Joint Architecture And Hyperparameter
    Search'
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2022/file/fd78f2f65881c1c7ce47e26b040cf48f-Paper-Datasets_and_Benchmarks.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '35'
  web: null
  year: 2022
barahona1982computational:
  address: null
  articleno: null
  authors:
  - - Francisco
    - Barahona
  chapter: null
  descrip: Proof that solving the Ising model is classically NP-hard
  doi: 10.1088/0305-4470/15/10/028
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0305-4470
  month: '10'
  note: null
  number: '10'
  pages:
  - 3241
  publisher: IOP Publishing
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  title: On the computational complexity of Ising spin glass models
  type: article
  url: https://iopscience.iop.org/article/10.1088/0305-4470/15/10/028
  venue: 'Journal of Physics A: Mathematical and General'
  volume: '15'
  web: null
  year: 1982
"barbagonz\xE1lez2018jmetalsp":
  address: null
  articleno: null
  authors:
  - - "Crist\xF3bal"
    - "Barba-Gonz\xE1lez"
  - - "Jos\xE9"
    - "Garc\xEDa-Nieto"
  - - Antonio J.
    - Nebro
  - - "Jos\xE9 A."
    - Cordero
  - - Juan J.
    - Durillo
  - - Ismael
    - Navas-Delgado
  - - "Jos\xE9 F."
    - Aldana-Montes
  chapter: null
  descrip: An open source numerical software library for solving multiobjective optimization
    problems in java in real-time via heuristics. The authors combine jMetal with
    data streaming via Apache Spark to solve distributed multiobjective optimization
    problems with streaming data in real-time
  doi: 10.1016/j.asoc.2017.05.004
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1568-4946
  month: '8'
  note: null
  number: null
  pages:
  - '737'
  - '748'
  publisher: Elsevier BV
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - optimization
  - multiobjective optimization
  - evolutionary algorithm
  - EA
  - software
  - open source
  - OSS
  - Java
  - parallel programming
  title: '{jMetalSP}: A framework for dynamic multi-objective big data optimization'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S1568494617302557
  venue: Applied Soft Computing
  volume: '69'
  web: null
  year: 2018
barber1996quickhull:
  address: null
  articleno: null
  authors:
  - - C. Bradford
    - Barber
  - - David P.
    - Dobkin
  - - Hannu
    - Huhdanpaa
  chapter: null
  descrip: The Quickhull algorithm and corresponding numerical software package is
    one of the best algorithms for computing convex hulls in high dimensions. Since
    Delaunay triangulations can be obtained by lifting to d+1 dimensional parabola
    and computing its lower convex hull, Quickhull can also be used to obtain Delaunay
    triangulations and convex hulls. This is the standard and baseline for high-dimensional
    Delaunay triangulation, although it seldom scales past 6-7 dimensions before a
    memory failure occurs, unless the data set is extremely small (exponential time
    and space complexity in dimension). However, for problems that Quickhull can solve,
    it's often the fastest and most robust solution. This is also the official reference
    for the authors' corresponding open source C++ software, which is the default
    method for Delaunay triangulation and piecewise linear interpolation in scipy.spatial
    and scipy.interpolate, respectively
  doi: 10.1145/235815.235821
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '12'
  note: null
  number: '4'
  pages:
  - '469'
  - '483'
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - algorithms
  - convex hull
  - high dimension
  - software
  - open source
  - OSS
  - C
  - C++
  title: The {Q}uickhull algorithm for convex hulls
  type: article
  url: https://dl.acm.org/doi/10.1145/235815.235821
  venue: ACM Transactions on Mathematical Software
  volume: '22'
  web: null
  year: 1996
barker2022introducing:
  address: null
  articleno: null
  authors:
  - - Michelle
    - Barker
  - - Neil P.
    - Chue Hong
  - - Daniel S.
    - Katz
  - - Anna-Lena
    - Lamprecht
  - - Carlos
    - Martinez-Ortiz
  - - Fotis
    - Psomopoulos
  - - Jennifer
    - Harrow
  - - Leyla Jael
    - Castro
  - - Morane
    - Gruenpeter
  - - Paula Andrea
    - Martinez
  - - Tom
    - Honeyman
  chapter: null
  descrip: The FAIR principles for open source scientific software, data, source code,
    and experiments should by findable (via DOIs or other), accessible (clear purpose
    and metadata), interoperable (should use standard interfaces, data formats, and
    schemas), and reusable (well documented, understandable, and not overly specialized
    to an unnecessarilly niche use-case). These are good principles for any open source
    software development practices
  doi: 10.1038/s41597-022-01710-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2052-4463
  month: '10'
  note: null
  number: '1'
  pages:
  - 622
  publisher: Nature Publishing Group
  series: null
  tags:
  - high-performance computing
  - HPC
  - software
  - open source
  - OSS
  title: Introducing the {FAIR} Principles for research software
  type: article
  url: https://www.nature.com/articles/s41597-022-01710-x
  venue: Scientific Data
  volume: '9'
  web: null
  year: 2022
belkin2018overfitting:
  address: Montr{\'e}al, Canada
  articleno: null
  authors:
  - - Mikhail
    - Belkin
  - - Daniel
    - Hsu
  - - Partha P.
    - Mitra
  chapter: null
  descrip: Publication showing that (a) interpolating classification and regression
    data in order to make predictions is often a reasonable thing to do especially
    in high dimensions (b) Delaunay interpolation is the correct way to do so and
    (c) using Delaunay interpolation on high dimensional noisy classification data
    yields predictions whose misclassification risk can be bounded by twice the Bayes
    risk (theoretical lowest possible risk for the data given) in the limit as the
    dimension increases
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '2306'
  - '2317'
  publisher: Curran Associates Inc.
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - overfitting
  - Delaunay triangulation
  - high dimension
  - interpolation
  - regression
  - classification
  - approximation theory
  title: Overfitting or perfect fitting? {R}isk bounds for classification and regression
    rules that interpolate
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2018/hash/e22312179bf43e61576081a2f250f845-Abstract.html
  venue: Proceedings of the 32nd International Conference on Neural Information Processing
    Systems (NIPS'18)
  volume: null
  web: null
  year: 2018
belkin2021fit:
  address: null
  articleno: null
  authors:
  - - Mikhail
    - Belkin
  chapter: null
  descrip: A massive review article on the magic of just interpolating noisy machine
    learning and deep learning data in high dimensions, and some theorems on why this
    is acceptable and even desirable as the dimension increases. Delaunay interpolation
    is spotlighted as one of the interpolation methods to use
  doi: 10.1017/S0962492921000039
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0962-4929
  month: '5'
  note: null
  number: null
  pages:
  - '203'
  - '248'
  publisher: Cambridge University Press
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - overfitting
  - Delaunay triangulation
  - high dimension
  - interpolation
  - regression
  - classification
  - approximation theory
  title: 'Fit without fear: remarkable mathematical phenomena of deep learning through
    the prism of interpolation'
  type: article
  url: https://www.cambridge.org/core/product/identifier/S0962492921000039/type/journal_article
  venue: Acta Numerica
  volume: '30'
  web: null
  year: 2021
bengio2013representation:
  address: null
  articleno: null
  authors:
  - - Yoshua
    - Bengio
  - - Aaron
    - Courville
  - - Pascal
    - Vincent
  chapter: null
  descrip: A review article on representation learning and unifying the ideas of representation
    learning, density estimation, and manifold learning under a single geometric umbrella.
    The authors show broadly how representation learning has been the key to the performance
    of neural networks across many fields, often allowing us to break the curse of
    dimensionality. Additionally, they discuss the general requirements (priors) about
    a problem / function for representation learning to be an effective approach.
  doi: 10.1109/TPAMI.2013.50
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0162-8828
  month: '8'
  note: null
  number: '8'
  pages:
  - '1798'
  - '1828'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - neural networks
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  - regression
  - classification
  title: 'Representation Learning: A Review and New Perspectives'
  type: article
  url: http://ieeexplore.ieee.org/document/6472238/
  venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
  volume: '35'
  web: null
  year: 2013
benitezhidalgo2019jmetalpy:
  address: null
  articleno: null
  authors:
  - - Antonio
    - Ben{\'i}tez-Hidalgo
  - - Antonio J.
    - Nebro
  - - Jos{\'e}
    - Garc{\'i}a-Nieto
  - - Izaskun
    - Oregi
  - - ''
    - Ser
  chapter: null
  descrip: jMetalPy -- a Python framework for solving MOOs with EAs -- this open source
    numerical software is a Python implementation of jMetal, with some improvements
    to code quality and new features for better open source software development and
    parallelism
  doi: 10.1016/j.swevo.2019.100598
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2210-6502
  month: '12'
  note: null
  number: null
  pages:
  - 100598
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - software
  - open source
  - OSS
  - Python
  - Java
  - parallel programming
  title: '{jMetalPy}: A {P}ython framework for multi-objective optimization with metaheuristics'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S2210650219301397
  venue: Swarm and Evolutionary Computation
  volume: '51'
  web: null
  year: 2019
bergstra2011algorithms:
  address: null
  articleno: null
  authors:
  - - James
    - Bergstra
  - - R\'{e}mi
    - Bardenet
  - - Yoshua
    - Bengio
  - - Bal\'{a}zs
    - K\'{e}gl
  chapter: null
  descrip: Original tree-parzen estimator (TPE) reference. The authors observe that
    the structure of most hyperparameter optimization problems is tree-like in that
    the values of certain parameters are only relevant given the choices of earlier
    parameters. This leads them to train a statistical distribution over the tree
    of decisions, which will allow them to traverse the tree with high probability
    of sampling good models. They show that this is better than Bayesian optimization
    with Gaussian processes and random search. They also have the insight (still holds
    true today) that for these problems, it is difficult to do better than random
    search
  doi: null
  edition: null
  editors:
  - - J.
    - Shawe-Taylor
  - - R.
    - Zemel
  - - P.
    - Bartlett
  - - F.
    - Pereira
  - - K.Q.
    - Weinberger
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - decision trees
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  title: Algorithms for Hyper-Parameter Optimization
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '24'
  web: null
  year: 2011
berkemeier2021derivativefree:
  address: null
  articleno: null
  authors:
  - - Manuel
    - Berkemeier
  - - Sebastian
    - Peitz
  chapter: null
  descrip: A numerical algorithm for multiobjective descent, using RBF surrogates
    + trust regions. Builds heavily off of Stefan's PhD thesis (ORBIT)
  doi: 10.3390/mca26020031
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2297-8747
  month: '4'
  note: null
  number: '2'
  pages:
  - 31
  publisher: Multidisciplinary Digital Publishing Institute
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: Derivative-Free Multiobjective Trust Region Descent Method Using Radial Basis
    Function Surrogate Models
  type: article
  url: https://www.mdpi.com/2297-8747/26/2/31
  venue: Mathematical and Computational Applications
  volume: '26'
  web: null
  year: 2021
betz1997vpr:
  address: Berlin, Heidelberg
  articleno: null
  authors:
  - - Vaughn
    - Betz
  - - Jonathan
    - Rose
  chapter: null
  descrip: 'Original VPR paper discussing their simulated-annealing placement and
    its cost function. As far as I can tell, this placement approach has been improved
    but not fundamentally changed in the years since original publication. VPR is
    now shipped as part of the open source software package VTR: at github.com/verilog-to-routing/vtr-verilog-to-routing'
  doi: null
  edition: null
  editors:
  - - Wayne
    - Luk
  - - Peter Y. K.
    - Cheung
  - - Manfred
    - Glesner
  git: null
  isbn: 978-3-540-69557-8
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '213'
  - '222'
  publisher: Springer Berlin Heidelberg
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - C++
  - Verilog
  title: '{VPR}: a new packing, placement and routing tool for {FPGA} research'
  type: inproceedings
  url: https://link.springer.com/chapter/10.1007/3-540-63465-7_226
  venue: Field-Programmable Logic and Applications
  volume: null
  web: null
  year: 1997
beume2009complexity:
  address: null
  articleno: null
  authors:
  - - Nicola
    - Beume
  - - Carlos M.
    - Fonseca
  - - Manuel
    - Lopez-Ibanez
  - - Luis
    - Paquete
  - - Jan
    - Vahrenhold
  chapter: null
  descrip: Proof that the complexity of calculating the hypervolume indicator with
    o objectives is exponential. Roughly the same reasons that calculating simplices
    in an o-dimensional Delaunay triangulation or computing the facets of an o-dimensional
    convex hull are exponential
  doi: 10.1109/TEVC.2009.2015575
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1941-0026
  month: '10'
  note: null
  number: '5'
  pages:
  - '1075'
  - '1082'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: On the complexity of computing the hypervolume indicator
  type: article
  url: http://ieeexplore.ieee.org/document/5208224
  venue: IEEE Transactions on Evolutionary Computation
  volume: '13'
  web: null
  year: 2009
biamonte2017quantum:
  address: null
  articleno: null
  authors:
  - - Jacob
    - Biamonte
  - - Peter
    - Wittek
  - - Nicola
    - Pancotti
  - - Patrick
    - Rebentrost
  - - Nathan
    - Wiebe
  - - Seth
    - Lloyd
  chapter: null
  descrip: A paper summarizing the early advances in quantum machine learning
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '7671'
  pages:
  - 195
  publisher: Nature Publishing Group
  series: null
  tags:
  - quantum computing
  title: Quantum machine learning
  type: article
  url: https://www.nature.com/articles/nature23474
  venue: Nature
  volume: '549'
  web: null
  year: 2017
bian2010towards:
  address: Monterey, California, USA
  articleno: null
  authors:
  - - Huimin
    - Bian
  - - Andrew C.
    - Ling
  - - Alexander
    - Choong
  - - Jianwen
    - Zhu
  chapter: null
  descrip: Comparison between VPR placement runtime and modern ASIC-based analytic
    placement runtimes. Conclusion is that simulated annealing is more accurate, but
    analytic placement is faster
  doi: 10.1145/1723112.1723140
  edition: null
  editors: []
  git: null
  isbn: '9781605589114'
  issn: null
  month: '2'
  note: null
  number: null
  pages:
  - 10
  publisher: Association for Computing Machinery
  series: FPGA '10
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - mixed-variable optimization
  - simulated annealing
  - SA
  title: Towards scalable placement for {FPGA}s
  type: inproceedings
  url: https://doi.org/10.1145/1723112.1723140
  venue: Proceedings of the 18th Annual ACM/SIGDA International Symposium on Field
    Programmable Gate Arrays
  volume: null
  web: null
  year: 2010
biedron2019fun3d:
  address: Hampton, VA, USA
  articleno: null
  authors:
  - - Robert T.
    - Biedron
  - - Jan Renee
    - Carlson
  - - Joseph M.
    - Derlaga
  - - Peter A.
    - Gnoffo
  - - Dana P.
    - Hammond
  - - William T.
    - Jones
  - - Bill
    - Kleb
  - - Elizabeth M.
    - Lee-Rausch
  - - Eric J.
    - Nielson
  - - Michael A.
    - Park
  - - Christopher L.
    - Rumsey
  - - James L.
    - Thomas
  - - Kyle B.
    - Thompson
  - - William A.
    - Wood
  chapter: null
  descrip: NASA's FUN3D CFD solver. This is one of the oldest and standard numerical
    softwares for solving CFD problems. Written in mostly Fortran 90. Uses a form
    of the problem that yields the adjoints, which can be used to optimize structures
    in fewer steps and perform sensitivity analyses. The kernel uses an iterative
    solver to solve a massive block-sparse linear system (I think derived from the
    weak form). Some a priori multiobjective optimization solvers are described in
    Section 9.9
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '{NASA} {T}echnical {M}emorandum ({TM}) 2019-220416'
  pages: null
  publisher: NASA Langley Research Center
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - autograd
  - algorithmic differentiation
  - backpropagation
  - optimization
  - multiobjective optimization
  - simulation optimization
  - autotuning
  - software
  - Fortran
  - parallel programming
  - CUDA
  - MPI
  title: '{FUN3D Manual}: 13.6'
  type: techreport
  url: https://fun3d.larc.nasa.gov/papers/FUN3D_Manual-13.6.pdf
  venue: null
  volume: null
  web: null
  year: 2019
bigeon2020dmultimads:
  address: null
  articleno: null
  authors:
  - - Jean
    - Bigeon
  - - S{\'e}bastien
    - Le Digabel
  - - Ludovic
    - Salomon
  chapter: null
  descrip: 'DMulti-MADS: Improved Multi-MADS using direct search / generalized pattern
    search plus some improvements to the Multi-MADS algorithm -- I need to re-read
    this to remember what the improvements were'
  doi: 10.1007/s10589-021-00272-9
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '6'
  note: null
  number: '2'
  pages:
  - '301'
  - '338'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  title: '{DM}ulti-{MADS}: {M}esh adaptive direct multisearch for blackbox multiobjective
    optimization'
  type: article
  url: https://link.springer.com/10.1007/s10589-021-00272-9
  venue: Computational Optimization and Applications
  volume: '79'
  web: null
  year: 2020
biscani2020parallel:
  address: null
  articleno: null
  authors:
  - - Francesco
    - Biscani
  - - Dario
    - Izzo
  chapter: null
  descrip: pagmo/pygmo - Parallel frameworks for solving multiobjective optimization
    problems (MOO) in Java and Python. Great example of open source numerical software,
    published in JOSS
  doi: 10.21105/joss.02338
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2475-9066
  month: '9'
  note: null
  number: '53'
  pages:
  - 2338
  publisher: The Open Journal
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: 'A parallel global multiobjective framework for optimization: pagmo'
  type: article
  url: https://joss.theoj.org/papers/10.21105/joss.02338
  venue: Journal of Open Source Software
  volume: '5'
  web: null
  year: 2020
blackford1997scalapack:
  address: null
  articleno: null
  authors:
  - - L. Susan
    - Blackford
  - - Jaeyoung
    - Choi
  - - Andy
    - Cleary
  - - Eduardo
    - D'Azevedo
  - - James
    - Demmel
  - - Inderjit
    - Dhillon
  - - Jack
    - Dongarra
  - - Sven
    - Hammarling
  - - Greg
    - Henry
  - - Antoine
    - Petitet
  - - K.
    - Stanley
  - - D.
    - Walker
  - - R. C.
    - Whaley
  chapter: null
  descrip: 'The ScaLAPACK user''s guide: A highly parallel and scalable open source
    implementation of the LAPACK software, for solving massive scale numerical linear
    algebra systems on distributed systems'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: SIAM
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  - computational linear algebra
  - software
  - open source
  - OSS
  - Fortran
  - parallel programming
  - MPI
  title: '{ScaLAPACK} Users'' Guide'
  type: book
  url: https://www.netlib.org/scalapack/
  venue: null
  volume: '4'
  web: null
  year: 1997
blank2020pymoo:
  address: null
  articleno: null
  authors:
  - - Julian
    - Blank
  - - Kalyanmoy
    - Deb
  chapter: null
  descrip: pymoo is an open source software package implementing NSGA-II, NSGA-III,
    and many other multiobjective evolutionary algorithms (MOEAs), plus extensions
    for handling things such as categorical variables. This is a well-maintained and
    well-documented open-source numerical software package. It is maintained by the
    lab of the original NSGA-II author, and therefore could be considered the official
    NSGA-II implementation. all source code in Python
  doi: 10.1109/ACCESS.2020.2990567
  edition: null
  editors: []
  git: http://github.com/anyoptimization/pymoo
  isbn: null
  issn: 2169-3536
  month: null
  note: null
  number: null
  pages:
  - '89497'
  - '89509'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - mixed-variable optimization
  - evolutionary algorithms
  - EA
  - evolutionary algorithm
  - software
  - open source
  - OSS
  - Python
  title: '{pymoo}: Multi-Objective Optimization in {Python}'
  type: article
  url: https://ieeexplore.ieee.org/document/9078759
  venue: IEEE Access
  volume: '8'
  web: null
  year: 2020
boissonnat2009incremental:
  address: Aarhus, Denmark
  articleno: null
  authors:
  - - Jean-Daniel
    - Boissonnat
  - - Olivier
    - Devillers
  - - Samuel
    - Hornus
  chapter: null
  descrip: A method for constructing Delaunay triangulations incrementally in medium
    to high dimensions. The key is to only store the Delaunay graph instead of the
    entire triangulation in order to get around storage issues. Then, simplices are
    either retrieved from a cache or reconstructed upon request. This dodges the curse
    of dimensionality in the spatial sense, but the time complexity of constructing
    the graph is still exponential since they do so by computing every simplex in
    the triangulation and storing the edges. This is the method used in CGAL, a standard
    numerical software package for using computational geometry data structures and
    algorithms in perfect precision (via symbolic arithmetic). CGAL is a header-only
    open source C++ library
  doi: 10.1145/1542362.1542403
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - '208'
  - '216'
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - high dimension
  title: Incremental construction of the {D}elaunay triangulation and the {D}elaunay
    graph in medium dimension
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/1542362.1542403
  venue: Proceedings of the Twenty-fifth Annual Symposium on Computational Geometry
    (SCG '09)
  volume: null
  web: null
  year: 2009
boixo2014evidence:
  address: null
  articleno: null
  authors:
  - - Sergio
    - Boixo
  - - Troels F
    - R{\o}nnow
  - - Sergei V
    - Isakov
  - - Zhihui
    - Wang
  - - David
    - Wecker
  - - Daniel A
    - Lidar
  - - John M
    - Martinis
  - - Matthias
    - Troyer
  chapter: null
  descrip: Evidence in support that quantum annealing will eventually be better than
    using simulated annealing
  doi: 10.1038/nphys2900
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1745-2473
  month: '3'
  note: null
  number: '3'
  pages:
  - 218
  publisher: Nature Publishing Group
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Evidence for quantum annealing with more than one hundred qubits
  type: article
  url: https://www.nature.com/articles/nphys2900
  venue: Nature Physics
  volume: '10'
  web: null
  year: 2014
bollapragada2020optimization:
  address: null
  articleno: null
  authors:
  - - Raghu
    - Bollapragada
  - - Matt
    - Menickelly
  - - Witold
    - Nazarewicz
  - - Jared
    - O'Neal
  - - Paul-Gerhard
    - Reinhard
  - - Stefan M.
    - Wild
  chapter: null
  descrip: Methodolgies for calibrating the Fayans EDF model to experimental data.
    Data is expensive and limited and the model itself is computationally expensive,
    so this is a classical inverse problem. The problem is actually multiobective
    because the data themselves come from various categories representing different
    types of observations, and the standard deviations for each of these observables
    is not known. Could be configured as a 3 or 9-objective problem
  doi: 10.1088/1361-6471/abd009
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0954-3899
  month: '2'
  note: null
  number: '2'
  pages:
  - 24001
  publisher: IOP Publishing
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - simulation optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - regression
  title: Optimization and supervised machine learning methods for fitting numerical
    physics models without derivatives
  type: article
  url: https://iopscience.iop.org/article/10.1088/1361-6471/abd009
  venue: 'Journal of Physics G: Nuclear and Particle Physics'
  volume: '48'
  web: null
  year: 2020
boothby2018nextgeneration:
  address: null
  articleno: null
  authors:
  - - Kelly
    - Boothby
  - - Paul
    - Bunyk
  - - Jack
    - Raymond
  - - Aidan
    - Roy
  chapter: null
  descrip: Whitepaper on the D-Wave Pegasus topology
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '22'
  pages:
  - 28
  publisher: D-Wave
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Next-generation topology of D-Wave quantum processors
  type: techreport
  url: https://www.dwavesys.com/sites/default/files/14-1026A-C_Next-Generation-Topology-of-DW-Quantum-Processors.pdf
  venue: null
  volume: null
  web: null
  year: 2018
borle2019analyzing:
  address: null
  articleno: null
  authors:
  - - Ajinkya
    - Borle
  - - Samuel J
    - Lomonaco
  chapter: null
  descrip: Quantum Annealing algorithm for solving Linear Least Squares equations
  doi: 10.1007/978-3-030-10564-8_23
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '289'
  - '301'
  publisher: Springer
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Analyzing the quantum annealing approach for solving linear least squares
    problems
  type: inproceedings
  url: null
  venue: International Workshop on Algorithms and Computation (WALCOM 2019)
  volume: null
  web: null
  year: 2019
bouhlel2019python:
  address: null
  articleno: null
  authors:
  - - Mohamed Amine
    - Bouhlel
  - - John T.
    - Hwang
  - - Nathalie
    - Bartoli
  - - "R\xE9mi"
    - Lafage
  - - Joseph
    - Morlier
  - - Joaquim R.R.A.
    - Martins
  chapter: null
  descrip: The open source numerical software package (in Python) pySMT. This is a
    surrogate modeling and Bayesian optimization toolbox for solving multidisciplinary
    engineering design optimization (MDO) problems, while utilizing derivatives and
    providing numerical stability analysis for each surrogate model class.
  doi: 10.1016/j.advengsoft.2019.03.005
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0965-9978
  month: '9'
  note: null
  number: null
  pages:
  - '102'
  - '662'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  - Gaussian process
  - software
  - open source
  - OSS
  - Python
  title: A {P}ython surrogate modeling framework with derivatives
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0965997818309360
  venue: Advances in Engineering Software
  volume: '135'
  web: null
  year: 2019
bowyer1981computing:
  address: null
  articleno: null
  authors:
  - - Adrian
    - Bowyer
  chapter: null
  descrip: The Bowyer-Watson algorithm is one of the first algorithms for computing
    Delaunay triangulations in arbitrary dimensions. It is not particularly scalable,
    but a first step toward thinking about Delaunay triangulation in more than 3D.
    Was published by both Bowyer and Watson in the same issue of the same journal,
    with a footnote from the publisher that they both submitted at the same time and
    after investigation, it was determined that this was purely coincidental and no
    plagiarism was involved. Therefore, both papers were published together and both
    authors are credited equally for discovery
  doi: 10.1093/comjnl/24.2.162
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0010-4620
  month: '2'
  note: null
  number: '2'
  pages:
  - '162'
  - '166'
  publisher: Oxford University Press (OUP)
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: Computing {D}irichlet tessellations
  type: article
  url: https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/24.2.162
  venue: The Computer Journal
  volume: '24'
  web: null
  year: 1981
boyd2004convex:
  address: null
  articleno: null
  authors:
  - - Stephen P
    - Boyd
  - - Lieven
    - Vandenberghe
  chapter: null
  descrip: The famous textbook on convex optimization (from which I learned most concepts)
    covering concepts such as basic convexity definitions and theorems, basic algorithms
    and optimality conditions, handling constraints, Lagrangian duality, multiobjective
    optimization basics, gradient descent and newton's method, sequential quadratic
    programming, linear programming, and a few applications and modeling basics
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Cambridge university press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - optimization
  - computational geometry
  - algorithms
  - projection
  - multiobjective optimization
  - constrained optimization
  - linear programming
  - quadratic programming
  - QP
  - LP
  - convex optimization
  - scientific machine learning
  - SciML
  title: Convex optimization
  type: book
  url: https://stanford.edu/~boyd/cvxbook/
  venue: null
  volume: null
  web: null
  year: 2004
bradbury2018jax:
  address: null
  articleno: null
  authors:
  - - J.
    - Bradbury
  - - ''
    - others
  chapter: null
  descrip: The recommended citation for the jax software project -- one of my personal
    favorite open source numerical software in Python. Performs autograd (or algorithmic
    differentiation) in either forward or reverse mode, is strongly typed, can act
    as a drop-in replacement for numpy, and can be just-in-time (jit) compiled for
    massive speedups
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Jul 2024'
  number: 0.3.13
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - autograd
  - algorithmic differentiation
  - backpropagation
  - open source
  - high-performance computing
  - HPC
  - computational linear algebra
  - scientific machine learning
  - SciML
  - software
  - OSS
  - Python
  - CUDA
  title: '{JAX}: composable transformations of {P}ython+{N}um{P}y programs'
  type: misc
  url: http://github.com/google/jax
  venue: null
  volume: null
  web: null
  year: 2018
bradford2018efficient:
  address: null
  articleno: null
  authors:
  - - Eric
    - Bradford
  - - Artur M.
    - Schweidtmann
  - - Alexei
    - Lapkin
  chapter: null
  descrip: A Multiobjective Bayesian optimization algorithm, very similar to ParEGO
    -- this algorithm uses the Gaussian process surrogates with NSGA-II to solve the
    problem. However, spectral sampling and thompson sampling are then employed to
    subselect a diverse set of candidates for batch evaluation. The resulting algorithm
    is called TSEMO
  doi: 10.1007/s10898-018-0609-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '6'
  note: null
  number: '2'
  pages:
  - '407'
  - '438'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - evolutionary algorithm
  - EA
  - Gaussian process
  title: Efficient multiobjective optimization employing {Gaussian} processes, spectral
    sampling and a genetic algorithm
  type: article
  url: http://link.springer.com/10.1007/s10898-018-0609-2
  venue: Journal of Global Optimization
  volume: '71'
  web: null
  year: 2018
bras2020use:
  address: null
  articleno: null
  authors:
  - - Carmo P.
    - Br{\'a}s
  - - Ana Lu{\'\i}sa
    - Cust{\'o}dio
  chapter: null
  descrip: A study on utilizing polynomial surrogate models during multiobjective
    direct search and generalized pattern search techniques
  doi: 10.1007/s10589-020-00233-8
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '12'
  note: null
  number: '3'
  pages:
  - '897'
  - '918'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - surrogate modeling
  title: On the use of polynomial models in multiobjective directional direct search
  type: article
  url: https://link.springer.com/10.1007/s10589-020-00233-8
  venue: Computational Optimization and Applications
  volume: '77'
  web: null
  year: 2020
bratley1988algorithm:
  address: null
  articleno: null
  authors:
  - - Paul
    - Bratley
  - - Bennett L.
    - Fox
  chapter: null
  descrip: The original TOMS open source numerical software code implementing Sobol
    sequence (low discrepancy sequence) generation in Fortran 90. Apparently there
    is a bug or limitation to this code, fixed by Joe et al. 2003 in their TOMS Remark
    on 659, and subsequent publication of a new generator used in Scipy
  doi: 10.1145/42288.214372
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '3'
  note: null
  number: '1'
  pages:
  - 13
  publisher: Association for Computing Machinery
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  - Sobol sequence
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 659: Implementing Sobol''s Quasirandom Sequence Generator'
  type: article
  url: https://doi.org/10.1145/42288.214372
  venue: ACM Transactions on Mathematical Software
  volume: '14'
  web: null
  year: 1988
bringmann2013approximation:
  address: null
  articleno: null
  authors:
  - - Karl
    - Bringmann
  - - Tobias
    - Friedrich
  chapter: null
  descrip: Analysis of hypervolume indicator as proxy for solution set quality --
    results for 2-objectives only, this paper shows that the hypervolume indicator
    is the best single indicator we have, but with some caveates
  doi: 10.1016/j.artint.2012.09.005
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0004-3702
  month: '2'
  note: null
  number: 0004-3702
  pages:
  - '265'
  - '290'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Approximation quality of the hypervolume indicator
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0004370212001178
  venue: Artificial Intelligence
  volume: '195'
  web: null
  year: 2013
brockman2016openai:
  address: null
  articleno: null
  authors:
  - - Greg
    - Brockman
  - - Vicki
    - Cheung
  - - Ludwig
    - Pettersson
  - - Jonas
    - Schneider
  - - John
    - Schulman
  - - Jie
    - Tang
  - - Wojciech
    - Zaremba
  chapter: null
  descrip: The largest repository of open source AI, machine learning, and control
    benchmark problems, maintained by OpenAI. This was the primary benchmark problem
    environment for all reinforcement learning researchers not affiliated with another
    company with their own private environments (such as Meta and Google). Still available
    at github.com/openai/gym
  doi: null
  edition: null
  editors: []
  git: https://github.com/openai/gym
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv:1606.01540
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - benchmarking
  - open source
  - scientific machine learning
  - SciML
  - software
  - OSS
  - Python
  title: OpenAI Gym
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2016
bronstein2021geometric:
  address: null
  articleno: null
  authors:
  - - Michael M.
    - Bronstein
  - - Joan
    - Bruna
  - - Taco
    - Cohen
  - - Petar
    - "Veli\u010Dkovi\u0107"
  chapter: null
  descrip: The new classic textbook on geometric deep learning -- this book (by the
    creator of the field) covers how all of representation learning can be broken
    down into exploiting symmetries, stabilities, and invariances in data, through
    the lens of group theory. For example, a convolutional layer exploits a symmetry
    between equivalent groups of images; and a max pooling layer exploits a scale
    invariance in image size/resolution; and when we talk about feature encodings,
    we are often looking for mappings that maintain stability (i.e., similar feature
    vectors remain close together after encoding).
  doi: 10.48550/arXiv.2104.13478
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.LG
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - neural networks
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  title: 'Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges'
  type: book
  url: null
  venue: null
  volume: null
  web: null
  year: 2021
brown2020language:
  address: null
  articleno: null
  authors:
  - - Tom
    - Brown
  - - Benjamin
    - Mann
  - - Nick
    - Ryder
  - - Melanie
    - Subbiah
  - - Jared D
    - Kaplan
  - - Prafulla
    - Dhariwal
  - - Arvind
    - Neelakantan
  - - Pranav
    - Shyam
  - - Girish
    - Sastry
  - - Amanda
    - Askell
  - - Sandhini
    - Agarwal
  - - Ariel
    - Herbert-Voss
  - - Gretchen
    - Krueger
  - - Tom
    - Henighan
  - - Rewon
    - Child
  - - Aditya
    - Ramesh
  - - Daniel
    - Ziegler
  - - Jeffrey
    - Wu
  - - Clemens
    - Winter
  - - Chris
    - Hesse
  - - Mark
    - Chen
  - - Eric
    - Sigler
  - - Mateusz
    - Litwin
  - - Scott
    - Gray
  - - Benjamin
    - Chess
  - - Jack
    - Clark
  - - Christopher
    - Berner
  - - Sam
    - McCandlish
  - - Alec
    - Radford
  - - Ilya
    - Sutskever
  - - Dario
    - Amodei
  chapter: null
  descrip: The official GPT-3 report, showing that scaling the same techniques from
    the GPT-1/2 models up to a 175 billion of parameter model is sufficient to provide
    state-of-the-art zero- and one-shot performance on a wide variety of benchmark
    problems. The fine-tuning of GPT-3 would become the original ChatGPT
  doi: null
  edition: null
  editors:
  - - H.
    - Larochelle
  - - M.
    - Ranzato
  - - R.
    - Hadsell
  - - M.F.
    - Balcan
  - - H.
    - Lin
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1877'
  - '1901'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - transformers
  - LLMs
  title: Language Models are Few-Shot Learners
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '33'
  web: null
  year: 2020
buhmann2000radial:
  address: null
  articleno: null
  authors:
  - - M. D.
    - Buhmann
  chapter: null
  descrip: Large review paper covering all aspects of RBFs, their interpolation error
    bounds, their convergence rates, and numerical computation for various kernel
    functions.
  doi: 10.1017/S0962492900000015
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0962-4929
  month: '1'
  note: null
  number: null
  pages:
  - '1'
  - '38'
  publisher: Cambridge University Press
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - approximation theory
  title: Radial basis functions
  type: article
  url: https://www.cambridge.org/core/product/identifier/S0962492900000015/type/journal_article
  venue: Acta Numerica
  volume: '9'
  web: null
  year: 2000
cai2014practical:
  address: null
  articleno: null
  authors:
  - - Jun
    - Cai
  - - William G
    - Macready
  - - Aidan
    - Roy
  chapter: null
  descrip: Finding minor graphs (a minor of G is a subgraph H that can be obtained
    by deleting edges). This algorithm is (was) used to find physical embeddings on
    D-Wave systems via the minorminer.find_embedding tool
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: A practical heuristic for finding graph minors
  type: article
  url: https://arxiv.org/abs/1406.2741
  venue: arXiv preprint arXiv:1406.2741
  volume: null
  web: null
  year: 2014
cameron2019moana:
  address: null
  articleno: null
  authors:
  - - Kirk W.
    - Cameron
  - - Ali
    - Anwar
  - - Yue
    - Cheng
  - - Li
    - Xu
  - - Bo Ananth Uday
    - Li
  - - Jon
    - Bernard
  - - Chandler
    - Jearls
  - - Thomas
    - Lux
  - - Yili
    - Hong
  - - Layne T.
    - Watson
  - - Ali R.
    - Butt
  chapter: null
  descrip: Key findings from the VarSys project on modeling HPC performance variability
    with surrogates and RSM, and using these models to inform decision making through
    visualizations, optimization, and otherwise -- a good real-world example of how
    modeling, interpolation, optimization, and data science can come together to produce
    actionable results (in the field of HPC performance tuning)
  doi: 10.1109/tpds.2019.2892129
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1045-9219
  month: '8'
  note: null
  number: '8'
  pages:
  - '1843'
  - '1856'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - performance modeling
  title: '{MOANA}: {M}odeling and analyzing {I/O} variability in parallel system experimental
    design'
  type: article
  url: https://ieeexplore.ieee.org/document/8631172
  venue: IEEE Transactions on Parallel and Distributed Systems
  volume: '30'
  web: null
  year: 2019
campana2018multiobjective:
  address: null
  articleno: null
  authors:
  - - Emilio Fortunato
    - Campana
  - - Matteo
    - Diez
  - - Giampaolo
    - Liuzzi
  - - Stefano
    - Lucidi
  - - Riccardo
    - Pellegrini
  - - Veronica
    - Piccialli
  - - Francesco
    - Rinaldi
  - - Andrea
    - Serani
  chapter: null
  descrip: The open source numerical software MODIR is proposed here. MODIR can be
    used to solve multiobjective blackbox optimization problems via a multiobjective
    variant of the DIRECT blackbox algorithm (direct search method). The motivating
    application is a multidisciplinary shiphull engineering design problem
  doi: 10.1007/s10589-017-9955-0
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '9'
  note: null
  number: '1'
  pages:
  - '53'
  - '72'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  title: A multi-objective {DIRECT} algorithm for ship hull optimization
  type: article
  url: http://link.springer.com/10.1007/s10589-017-9955-0
  venue: Computational Optimization and Applications
  volume: '71'
  web: null
  year: 2018
cao2017performance:
  address: Vancouver, Canada
  articleno: null
  authors:
  - - Zhen
    - Cao
  - - Vasily
    - Tarasov
  - - Hari Prasath
    - Raman
  - - Dean
    - Hildebrand
  - - Erez
    - Zadok
  chapter: null
  descrip: An experimental and statistical study (very similar to the VarSys project)
    on measuring and modeling the performance variability in modern storage stacks.
    They concluded that block allocation strategies cause performance variability
    in Ext4-HDD configurations, based on statistical techniques such as Latin hypercube
    design of experiments over their parameter space and computing the relative standard
    deviations at each configuration.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: 978-1-931971-36-2
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '329'
  - '344'
  publisher: USENIX Association
  series: null
  tags:
  - high-performance computing
  - HPC
  - performance modeling
  title: On the performance variation in modern storage stacks
  type: inproceedings
  url: https://www.usenix.org/conference/fast17/technical-sessions/presentation/cao
  venue: Proceedings of the 15th USENIX Conference on File and Storage Technologies
    (FAST '17)
  volume: null
  web: null
  year: 2017
capps2016iozone:
  address: null
  articleno: null
  authors:
  - - Don
    - Capps
  - - Carol
    - Capps
  - - Darren
    - Sawyer
  - - Jerry
    - Lohr
  - - George
    - Dowding
  - - Gary
    - Little
  - - Terry
    - Capps
  - - Robin
    - Miller
  - - Sorin
    - Faibish
  - - Raymond
    - Wang
  - - Tanmay
    - Waghmare
  - - Yansheng
    - Zhang
  - - Vernon
    - Miller
  - - Nick
    - Principe
  - - Zach
    - Jones
  - - Udayan
    - Bapat
  - - William
    - Norcott
  - - Isom
    - Crawford
  - - Kirby
    - Collins
  - - Al
    - Slater
  - - Scott
    - Rhine
  - - Mike
    - Wisner
  - - Ken
    - Goss
  - - Steve
    - Landherr
  - - Brad
    - Smith
  - - Mark
    - Kelly
  - - Alain
    - CYR
  - - Randy
    - Dunlap
  - - Mark
    - Montague
  - - Dan
    - Million
  - - Gavin
    - Brebner
  - - Jean-Marc
    - Zucconi
  - - Jeff
    - Blomberg
  - - Benny
    - Halevy
  - - Dave
    - Boone
  - - Erik
    - Habbinga
  - - Kris
    - Strecker
  - - Walter
    - Wong
  - - Joshua
    - Root
  - - Fabrice
    - Bacchella
  - - Zhenghua
    - Xue
  - - Qin
    - Li
  - - Darren
    - Sawyer
  - - Vangel
    - Bojaxhi
  - - Ben
    - England
  - - Vikentsi
    - Lapa
  - - Alexey
    - Skidanov
  chapter: null
  descrip: Official IOZone reference -- this benchmarking software can be used to
    measure the read, write, strided-read, strided-write, first read, first write,
    etc throughputs on a given system. In the VarSys project, we used this to measure
    IO performance (which we then used to calculate performance variability) for various
    system configurations on our Moana cluster.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: January
  note: 'Last accessed: Nov 2016'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - benchmarking
  - performance modeling
  - software
  title: '{IOzone} Filesystem Benchmark'
  type: misc
  url: www.iozone.org
  venue: null
  volume: null
  web: null
  year: 2016
caron2024qpbenchmark:
  address: null
  articleno: null
  authors:
  - - "St\xE9phane"
    - Caron
  - - Akram
    - Zaki
  - - Pavel
    - Otta
  - - Daniel
    - "Arnstr\xF6m"
  - - Justin
    - Carpentier
  - - Fengyu
    - Yang
  - - Pierre-Alexandre
    - Leziart
  chapter: null
  descrip: A really nice suite of quadratic programming (QP) benchmark problems
  doi: null
  edition: null
  editors: []
  git: https://github.com/qpsolvers/qpbenchmark
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 29, 2025'
  number: 2.4.0
  pages: null
  publisher: GitHub
  series: null
  tags:
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Python
  title: 'qpbenchmark: Benchmark for quadratic programming solvers available in {Python}'
  type: misc
  url: null
  venue: GitHub repository
  volume: null
  web: null
  year: 2024
chandy1997parallel:
  address: Austin, TX, USA
  articleno: null
  authors:
  - - J. A.
    - Chandy
  - - P.
    - Banerjee
  chapter: null
  descrip: A hybrid simulated annealing/partitioning based placer algorithm with parallel
    terminal assignment. They are able to recursively divide the problem with partitioning
    for a few levels then perform placement on the subproblems with simulated annealing
  doi: 10.1109/ICCD.1997.628930
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '621'
  - '627'
  publisher: IEEE Comput. Soc
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - FPGA
  title: A parallel circuit-partitioned algorithm for timing driven cell placement
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/628930/
  venue: Proceedings International Conference on Computer Design VLSI in Computers
    and Processors
  volume: null
  web: null
  year: 1997
chang2016gpu:
  address: Virginia Beach, VA, USA
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  chapter: null
  descrip: My bachelor's thesis on how to use and measuring the overhead of using
    NVIDIA's CUDA MPS contol daemon to distribute small matrix-vector multiplication
    kernels between available resources on HPC systems
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Department of Computer Science, Virginia Wesleyan University
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - GPU computing
  - CUDA
  title: '{GPU} Saturation for Multiple Matrix-Vector Multiplications'
  type: Bachelor's Thesis
  url: null
  venue: null
  volume: null
  web: null
  year: 2016
chang2018computing:
  address: St. Petersburg, FL, USA
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Tyler C. H.
    - Lux
  - - Sharath
    - Raghvendra
  - - Bo
    - Li
  - - Li
    - Xu
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: An algorithm paper on how to compute the umbrella neighborhood of a single
    point in the Delaunay triangulation. The idea is to sample a single vertex, generate
    a single simplex using DelaunaySparse, then pivot across open facets without dropping
    the center point. We pivot until we've closed all open facets (therefore, we need
    to keep track of all open facets). This algorithm works beautifully for points
    in general position, but often fails for degenerate sets since we may sample points
    from conflicting triangulations, therefore failing to close all facets. It was
    abandoned in favor of computing the Delaunay graph when creating VTMOP.
  doi: 10.1109/secon.2018.8479003
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '4'
  note: null
  number: null
  pages:
  - 8
  publisher: IEEE
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  title: Computing the umbrella neighbourhood of a vertex in the {D}elaunay triangulation
    and a single {V}oronoi cell in arbitrary dimension
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8479003
  venue: Proceedings of IEEE SoutheastCon 2018
  volume: null
  web: null
  year: 2018
chang2018polynomial:
  address: Richmond, KY, USA
  articleno: '12'
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Thomas C. H.
    - Lux
  - - Bo
    - Li
  - - Li
    - Xu
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: The official publication for the DelaunaySparse algorithm -- but not the
    software. This is where the most thorough algorithm analysis and proof of correctness
    is published -- implementation details and handilng numerical degeneracy are briefly
    touched on but not detailed (and not yet finalized) until the submission of the
    TOMS paper
  doi: 10.1145/3190645.3190680
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '3'
  note: null
  number: null
  pages: null
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - high dimension
  title: A polynomial time algorithm for multivariate interpolation in arbitrary dimension
    via the {D}elaunay triangulation
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3190645.3190680
  venue: Proceedings of the 2018 ACM Southeast Conference (ACMSE '18)
  volume: null
  web: null
  year: 2018
chang2018predicting:
  address: Baltimore, MD, USA
  articleno: '2'
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Thomas C. H.
    - Lux
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Li
    - Xu
  - - Godmar
    - Back
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: Interpolation errors and runtimes and an early version of the DelaunaySparse
    algorithm is applied to a HPC performance modeling application
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: SCS
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - high-performance computing
  - HPC
  title: Predicting system performance by interpolation using a high-dimensional {D}elaunay
    triangulation
  type: inproceedings
  url: https://par.nsf.gov/servlets/purl/10111451
  venue: Proceedings of the 2018 Spring Simulation Conference (SpringSim 2018), the
    26th High Performance Computing Symposium (HPC '18)
  volume: null
  web: null
  year: 2018
chang2019experimental:
  address: null
  articleno: null
  authors:
  - - Chia Cheng
    - Chang
  - - Arjun
    - Gambhir
  - - Travis S
    - Humble
  - - Shigetoshi
    - Sota
  - - Aram W.
    - Harrow
  - - Avinatan
    - Hassidim
  - - Seth
    - Lloyd
  - - Yi\ifmmode \breve{g}\else \u{g}\fi{}it
    - Suba\ifmmode \mbox{\c{s}}\else \c{s}\fi{}\ifmmode \imath \else \i \fi{}
  - - Rolando D.
    - Somma
  - - Davide
    - Orsucci
  - - Jingwei
    - Wen
  - - Xiangyu
    - Kong
  - - Shijie
    - Wei
  - - Bixue
    - Wang
  - - Tao
    - Xin
  - - Guilu
    - Long
  chapter: null
  descrip: Quantum Annealing algorithm for solving polynomial systems of equations
  doi: 10.1103/PhysRevA.99.012320
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2469-9926
  month: Jan
  note: null
  number: '1'
  pages:
  - 5
  publisher: Nature Publishing Group
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Experimental realization of quantum algorithms for a linear system inspired
    by adiabatic quantum computing
  type: article
  url: https://link.aps.org/doi/10.1103/PhysRevA.99.012320
  venue: Phys. Rev. A
  volume: '99'
  web: null
  year: 2019
chang2020algorithm:
  address: New York, NY, USA
  articleno: '38'
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Thomas C. H.
    - Lux
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: The DelaunaySparse software, demonstrates how to calculate simplices from
    a Delauay triangulation in very high dimensions scalably (and in parallel) using
    a highly customized simplex method like solver. The resulting Fortran numerical
    software is fully open source with a C and Python interface
  doi: 10.1145/3422818
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '12'
  note: null
  number: '4'
  pages:
  - 20
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - high dimension
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - computational linear algebra
  - interpolation
  - software
  - open source
  - OSS
  - Fortran
  - Python
  - C
  - parallel programming
  - OpenMP
  title: 'Algorithm 1012: {DELAUNAYSPARSE}: Interpolation via a Sparse Subset of the
    {D}elaunay Triangulation in Medium to High Dimensions'
  type: article
  url: https://dl.acm.org/doi/10.1145/3422818
  venue: ACM Trans. Math. Softw.
  volume: '46'
  web: null
  year: 2020
chang2020managing:
  address: Fairfax, VA, USA
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Jeffrey
    - Larson
  - - Layne T.
    - Watson
  - - Thomas C. H.
    - Lux
  chapter: null
  descrip: Paper on the challenges of integrating VTMOP into the libEnsemble parallel
    computing Python software library at Argonne
  doi: 10.22360/SpringSim.2020.HPC.001
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 31
  publisher: SCS
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - multiobjective optimization
  - simulation optimization
  - blackbox optimization
  - derivative-free optimization
  - DFO
  title: Managing computationally expensive blackbox multiobjective optimization problems
    using {libEnsemble}
  type: inproceedings
  url: https://dl.acm.org/doi/abs/10.5555/3408207.3408245
  venue: Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance
    Computing Symposium (HPC '20)
  volume: null
  web: null
  year: 2020
chang2020mathematical:
  address: null
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  chapter: null
  descrip: My PhD thesis, including multiobjective optimization techniques, algorithm,
    performance analysis, and software review; description of VTMOP, running parallel
    simulations, integrating with libE. Also scientific machine learning via Delaunay
    interpolation and algorithms and proofs for doing so. Several applications related
    to HPC performance modeling and autotuning.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Virginia Tech, Dept. of Computer Science
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - algorithms
  - high dimension
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - parallel computing
  - performance modeling
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - computational linear algebra
  - software
  - parallel programming
  - Fortran
  - Python
  - OpenMP
  - MPI
  title: Mathematical Software for Multiobjective Optimization Problems
  type: phdthesis
  url: https://vtechworks.lib.vt.edu/handle/10919/98915
  venue: null
  volume: null
  web: null
  year: 2020
chang2020multiobjective:
  address: Orlando, FL, USA
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Jeffrey
    - Larson
  - - Layne T.
    - Watson
  chapter: null
  descrip: A study on the multiobjective optimization of the LINPACK benchmark's config
    files on the leadership class HPC Bebop at Argonne National Laboratory. We used
    VTMOP but some modifications were required to ensure that mixed variables were
    properly handled. Some of the techniques that we used here inspired me to provide
    automatic support in ParMOO. Ultimately, we achieve 3x reduction in performance
    variability without sacrificing max/mean throughput.
  doi: 10.1109/WSC48552.2020.9383875
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '3081'
  - '3092'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - benchmarking
  - performance modeling
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - autotuning
  - surrogate modeling
  title: Multiobjective optimization of the variability of the high-performance {LINPACK}
    solver
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9383875
  venue: Proc. 2020 Winter Simulation Conference (WSC 2020)
  volume: null
  web: null
  year: 2020
chang2022algorithm:
  address: null
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Jeffrey
    - Larson
  - - Nicole
    - Neveu
  - - William I.
    - Thacker
  - - Shubhangi
    - Deshpande
  - - Thomas C. H.
    - Lux
  chapter: null
  descrip: 'Publication of my second open source numerical software package: VTMOP
    a Fortran software for solving blackbox multiobjective optimization problems.
    Uses surrogate modeling (RSM), with an adaptive weighting scheme, within a trust
    region framework. The motivating application is a particle accelerator tuning
    problem at SLAC'
  doi: 10.1145/3529258
  edition: null
  editors: []
  git: https://github.com/Libensemble/libe-community-examples/tree/main/vtmop
  isbn: null
  issn: 0098-3500
  month: '9'
  note: null
  number: '3'
  pages:
  - 36
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - design of experiments
  - DoE
  - adaptive sampling
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - software
  - open source
  - OSS
  - Fortran
  - OpenMP
  - MPI
  title: '{Algorithm 1028}: {VTMOP}: Solver for Blackbox Multiobjective Optimization
    Problems'
  type: article
  url: https://dl.acm.org/doi/10.1145/3529258
  venue: '{ACM} Transactions on Mathematical Software'
  volume: '48'
  web: null
  year: 2022
chang2023framework:
  address: Kigali, Rwanda
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Jakob R.
    - Elias
  - - Stefan M.
    - Wild
  - - Santanu
    - Chaudhuri
  - - Joseph A.
    - Libera
  chapter: null
  descrip: Using ParMOO software with the MDML software (wrapper on Apache Kafka with
    automatic data logging and analysis dashboard) in order to optimize chemical manufacturing
    processes in a wet-lab environment. The kafka querries are sent directly to a
    continuous flow reactor (CFR) through a smart-lab setup in the MERF at Argonne.
    Through this setup, ParMOO is able to automatically steer the solvents, bases,
    temperatures, flow rates, and mixing ratios of a complex chemical manufacturing
    process in order to produce optimized yields and purities -- achieving multi-hundred-fold
    improvement over the previous manual process
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 10
  publisher: null
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  title: 'A framework for fully autonomous design of materials via multiobjective
    optimization and active learning: challenges and next steps'
  type: inproceedings
  url: https://openreview.net/forum?id=8KJS7RPjMqG
  venue: 11th International Conference on Learning Representations (ICLR 2023), Workshop
    on Machine Learning for Materials (ML4Materials)
  volume: null
  web: null
  year: 2023
chang2023parmoo:
  address: null
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Stefan M.
    - Wild
  chapter: null
  descrip: The ParMOO JOSS article -- ParMOO is my open source numerical software
    package and library ParMOO, written in Python, which can be used for implementing
    custom solvers for multiobjective simulation optimization problems, while supporting
    mixed variables, non linear constraints, and diverse and custom surrogte models,
    and composite structures where some components of the problem are blackbox, but
    the rest are algebraic. In later releases, ParMOO also supports interactive visualization
    of results via Plotly + Dash, parallel and distributed function evaluations via
    libEnsemble, and automatic gradient calculations and just-in-time compilation
    via jax
  doi: 10.21105/joss.04468
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2475-9066
  month: '2'
  note: null
  number: '82'
  pages:
  - 4468
  publisher: The Open Journal
  series: null
  tags:
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - distributed computing
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: '{ParMOO}: A {P}ython library for parallel multiobjective simulation optimization'
  type: article
  url: https://joss.theoj.org/papers/10.21105/joss.04468
  venue: Journal of Open Source Software
  volume: '8'
  web: null
  year: 2023
chang2024parmoo:
  address: Lemont, IL, USA
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Stefan M.
    - Wild
  - - Hyrum
    - Dickinson
  chapter: null
  descrip: The ParMOO docs -- ParMOO is my open source numerical software package
    and library ParMOO, written in Python, which can be used for implementing custom
    solvers for multiobjective simulation optimization problems, while supporting
    mixed variables, non linear constraints, and diverse and custom surrogte models,
    and composite structures where some components of the problem are blackbox, but
    the rest are algebraic. In later releases, ParMOO also supports interactive visualization
    of results via Plotly + Dash, parallel and distributed function evaluations via
    libEnsemble, and automatic gradient calculations and just-in-time compilation
    via jax
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: Version 0.4.1
  pages: null
  publisher: Argonne National Laboratory
  series: null
  tags:
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - distributed computing
  - parallel computing
  - autograd
  - algorithmic differentiation
  - backpropagation
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  - high dimension
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  - MPI
  title: '{ParMOO}: {P}ython library for parallel multiobjective simulation optimization'
  type: techreport
  url: https://parmoo.readthedocs.io/en/latest
  venue: null
  volume: null
  web: null
  year: 2024
chang2024remark:
  address: null
  articleno: '12'
  authors:
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Sven
    - Leyffer
  - - Thomas C. H.
    - Lux
  - - Hussain M. J.
    - Almohri
  chapter: null
  descrip: An update to DelaunaySparse where we use BQPD to solve the problem of projection
    onto the convex hull. We rigorously show that this is the only method that achieves
    a 100% success rate among open source quadratic programming / NNLS solvers for
    this size and shape of problem. Then we demonstrate robustness on synthetic test
    problems and real applications from computer security
  doi: 10.1145/3656581
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '6'
  note: null
  number: '2'
  pages:
  - 8
  publisher: Association of Computing Machinery
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - projection
  - convex hull
  - high dimension
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  title: 'Remark on {Algorithm} 1012: Computing projections with large data sets'
  type: article
  url: https://dl.acm.org/doi/10.1145/3656581
  venue: ACM Transactions on Mathematical Software
  volume: '50'
  web: null
  year: 2024
chang2025designing:
  address: null
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Stefan M.
    - Wild
  chapter: null
  descrip: The ParMOO IJOC article describing the design of the ParMOO software, motivation,
    and providing examples of how ParMOO can be used to solve common scientific problems
    more efficiently with low effort -- ParMOO is my open source numerical software
    package and library ParMOO, written in Python, which can be used for implementing
    custom solvers for multiobjective simulation optimization problems, while supporting
    mixed variables, non linear constraints, and diverse and custom surrogte models,
    and composite structures where some components of the problem are blackbox, but
    the rest are algebraic. In later releases, ParMOO also supports interactive visualization
    of results via Plotly + Dash, parallel and distributed function evaluations via
    libEnsemble, and automatic gradient calculations and just-in-time compilation
    via jax
  doi: 10.1287/ijoc.2023.0250
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1091-9856
  month: '3'
  note: null
  number: null
  pages: null
  publisher: Institute for Operations Research and the Management Sciences (INFORMS)
  series: null
  tags:
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - distributed computing
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: Designing a Framework for Solving Multiobjective Simulation Optimization
    Problems
  type: article
  url: https://pubsonline.informs.org/doi/10.1287/ijoc.2023.0250
  venue: INFORMS Journal on Computing
  volume: null
  web: null
  year: 2025
chang2025leveraging:
  address: null
  articleno: '113726'
  authors:
  - - Tyler H.
    - Chang
  - - Andrew K.
    - Gillette
  - - Romit
    - Maulik
  chapter: null
  descrip: Our SciML paper analyzing and comparing the usefulness of error bounds
    of various interpolation methods for scientific machine learning applications.
    Ultimately, we show that the Delaunay bounds are most useful (I personally suspect
    it has something to do with less smooth "low order methods" being more robust
    for noisy data) and we demonstrate how they can be used for interpretability and
    verification on an aircraft design application (predicting lift/drag ratios for
    various airfoils)
  doi: 10.1016/j.jcp.2025.113726
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0021-9991
  month: '3'
  note: null
  number: null
  pages:
  - 23
  publisher: Elsevier BV
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - representation learning
  - Delaunay triangulation
  - RBFs
  - Gaussian process
  - high dimension
  - dimension reduction
  - interpolation
  - regression
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Leveraging Interpolation Models and Error Bounds for Verifiable Scientific
    Machine Learning
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0021999125000099
  venue: Journal of Computational Physics
  volume: '524'
  web: null
  year: 2025
chang2025repository:
  address: null
  articleno: null
  authors:
  - - Tyler H.
    - Chang
  - - Stefan M.
    - Wild
  chapter: null
  descrip: This is the IJOC ParMOO repository DOI -- this is an archive of the software
    experiments for obtaining our test problems and reproducing our experimental results
    on those test problems with customized ParMOO solvers.
  doi: 10.1287/ijoc.2023.0250.cd
  edition: null
  editors: []
  git: https://github.com/INFORMSJoC/2023.0250
  isbn: null
  issn: 1091-9856
  month: '3'
  note: 'Last accessed: May 1, 2025'
  number: null
  pages: null
  publisher: Institute for Operations Research and the Management Sciences (INFORMS)
  series: null
  tags:
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - distributed computing
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - mixed-variable optimization
  - surrogate modeling
  - RBFs
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  title: Repository for ``Designing a Framework for Solving Multiobjective Simulation
    Optimization Problems''
  type: misc
  url: https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0250
  venue: INFORMS Journal on Computing
  volume: null
  web: null
  year: 2025
chard2020funcx:
  address: Stockholm, Sweden
  articleno: null
  authors:
  - - Ryan
    - Chard
  - - Yadu
    - Babuji
  - - Zhuozhao
    - Li
  - - Tyler
    - Skluzacek
  - - Anna
    - Woodard
  - - Ben
    - Blaiszik
  - - Ian
    - Foster
  - - Kyle
    - Chard
  chapter: null
  descrip: The funcX and updated Globus publication on the techniques and benefits
    of using a function-as-a-service (FaaS) framework to perform scientific experimentation
    at Argonne and other labs. funcX and Globus are scientific software products for
    performing distributed function evaluations and parallel computing that started
    at Argonne, and spun off into independent companies
  doi: 10.1145/3369583.3392683
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - '65'
  - '76'
  publisher: ACM
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  - Kubernetes
  - Slurm
  - PBS
  title: '{funcX}: A federated function serving fabric for science'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3369583.3392683
  venue: Proc. 29th International Symposium on High-Performance Parallel and Distributed
    Computing (HPDC '20)
  volume: null
  web: null
  year: 2020
chen2004optimal:
  address: null
  articleno: null
  authors:
  - - Long
    - Chen
  - - Jin-chao
    - Xu
  chapter: null
  descrip: A great article on the optimality of Delaunay triangulations for interpolation.
    The authors show that the Delaunay triangulation can be equivalently defined as
    the interpolation mesh that minimizes interpolation error for the perfect quadratic
    function f(x) = <x, x>. Then, they explore how we can define new metrics to yield
    a new Delaunay triangulation that is optimal for an arbitrary function. Ultimatey,
    it is hard to compute such Delaunay triangulations, so they explore relaxations
    and definitions of "nearly optimal" Delaunay triangulations.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '2'
  pages:
  - '299'
  - '308'
  publisher: null
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  - regression
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Optimal {D}elaunay triangulations
  type: article
  url: https://www.math.uci.edu/~chenlong/Papers/Chen.L%3BXu.J2004.pdf
  venue: Journal of Computational Mathematics
  volume: '22'
  web: null
  year: 2004
chen2016xgboost:
  address: San Francisco, California, USA
  articleno: null
  authors:
  - - Tianqi
    - Chen
  - - Carlos
    - Guestrin
  chapter: null
  descrip: The publication for the XGBoost numerical software. XGBoost can be used
    to efficiently compute optimized gradient boosted decision trees on massive datasets
    via a fully-distributed algorithm that can be configured to run on Hadoop, SGE,
    and MPI. It can also be run on NVIDIA GPUs using CUDA. The software is fully open-source
    and written in highly optimized C++, though everyone uses it through its Python
    interface. The download is available at github.com/dmlc/xgboost. Most data science
    competition winners use XGBoost for tabular data
  doi: 10.1145/2939672.2939785
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '8'
  note: null
  number: null
  pages:
  - '785'
  - '794'
  publisher: ACM
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - decision trees
  - open source
  - high-performance computing
  - HPC
  - distributed computing
  - GPU computing
  - CUDA
  - scientific machine learning
  - SciML
  - regression
  - classification
  - software
  - OSS
  - Python
  - C++
  - Kubernetes
  - Spark
  title: '{XGBoost}: A Scalable Tree Boosting System'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/2939672.2939785
  venue: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining (KDD '16)
  volume: null
  web: null
  year: 2016
chen2017fpga:
  address: Irvine, CA
  articleno: null
  authors:
  - - Shih-Chun
    - Chen
  - - Yao-Wen
    - Chang
  chapter: null
  descrip: Survey of common FPGA placement and routing algorithms/techniques, mentioning
    partition-based placement, analytic placement, and simulated annealing placement
    as various viable techniques ranging from least computationally expensive/least
    accurate to most expensive/most accurate. They also mention the different stages
    in a moder analytic placer (since analytical placement is the current state-of-the-art),
    which includes packing and netlist optimizations, global floorplanning and global
    placement (via quadratic programming), legalization (similar to integer/categorical
    binning), and detailed placement (typically via simulated annealing)
  doi: 10.1109/ICCAD.2017.8203878
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '11'
  note: null
  number: null
  pages:
  - '914'
  - '921'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - mixed-variable optimization
  - constrained optimization
  - simulated annealing
  - SA
  title: '{FPGA} placement and routing'
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/8203878/
  venue: 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)
  volume: null
  web: null
  year: 2017
chen2021evaluating:
  address: null
  articleno: null
  authors:
  - - Mark
    - Chen
  - - Jerry
    - Tworek
  - - Heewoo
    - Jun
  - - Qiming
    - Yuan
  - - Henrique Ponde De Oliveira
    - Pinto
  - - Jared
    - Kaplan
  - - Harri
    - Edwards
  - - Yuri
    - Burda
  - - Nicholas
    - Joseph
  - - Greg
    - Brockman
  - - ''
    - others
  chapter: null
  descrip: 'HumanEval: OpenAI''s publicly available test set of function headers and
    docstrings + unit tests for evaluating the performance of generative AI models
    and LLMs that generate Python code from function docstings. They also train a
    code generation model from GitHub data and evaluate its performance on their test
    set with mixed results. The challenge appears to be having the LLM understand
    long multi-step docstrings and binding the results of operations to variables
    (and remembering those variables in the future!). Other popular benchmarks for
    code generation involve mostly just having the AI answer questions on Codebench
    and Codefore. The HumanEval benchmark is available with a Python interface at:
    github.com/openai/human-eval'
  doi: 10.48550/arXiv.2107.03374
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2107.03374
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - benchmarking
  - open source
  title: Evaluating large language models trained on code
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2021
chen2023integrated:
  address: Orlando, FL, USA
  articleno: null
  authors:
  - - Gongxiaohui
    - Chen
  - - Tyler H.
    - Chang
  - - John
    - Power
  - - Chungunag
    - Jing
  chapter: null
  descrip: Publication of our work on multiobjective shape optimization of the RF-gun
    cavity for the Argonne wakefield accelerator using ParMOO with the POISSON/SUPERFISH
    simulation software
  doi: 10.48550/arXiv.2311.09415
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 2
  publisher: null
  series: null
  tags:
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - simulation
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: An Integrated Multi-Physics Optimization Framework for Particle Accelerator
    Design
  type: inproceedings
  url: null
  venue: Proc. 2023 Winter Simulation Conference (WSC 2023), Industrial Applications
    Track
  volume: null
  web: null
  year: 2023
cheney2009course:
  address: Providence, RI, USA
  articleno: null
  authors:
  - - Elliott W.
    - Cheney
  - - William A.
    - Light
  chapter: null
  descrip: 'The classic textbook by Cheney and Light on the fundamentals of approximation
    theory for multivariate functions -- topics include: basics of interpolation,
    approximation theory, and linear operators; multivariate polynomials, their interpolation
    nodes, and error kernels; selecting good polynomial interpolants via Newton and
    Lagrange type methods; positive-definite functions, kernel interpretations, and
    good kernels for interpolation; basis functions, orthonormal bases, common bases
    for interpolation and convergence rates; Chebyshev nodes; B-splines, Box splines,
    and thin-plate splines; and basics of artificial neural networks. Other topics
    include wavelets, orthogonal projection algorithms, Hilbert spaces, and reproducing
    kernel Hilbert spaces (RKHS).'
  doi: 10.1090/gsm/101
  edition: null
  editors: []
  git: null
  isbn: '9780821847985'
  issn: 1065-7339
  month: '1'
  note: null
  number: null
  pages: null
  publisher: AMS
  series: Graduate Studies in Mathematics
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - RBFs
  - high dimension
  - interpolation
  - regression
  - approximation theory
  title: A Course in Approximation Theory
  type: book
  url: http://www.ams.org/gsm/101
  venue: Graduate Studies in Mathematics
  volume: null
  web: null
  year: 2009
cheng2012delaunay:
  address: Boca Raton, FL, USA
  articleno: null
  authors:
  - - Siu-Wing
    - Cheng
  - - Tamal K.
    - Dey
  - - Jonathan R.
    - Shewchuk
  chapter: null
  descrip: Another textbook on Delaunay triangulations in the context of mesh generation.
    This book is less thorough than the book by Aurenhammer in terms of theory, but
    a bit more complete in its coverage and analysis of modern algorithms, including
    the often forgotten gift-wrapping algorithm, which forms the basis for several
    operations in DelaunaySparse. This book also has some interesting algorithms and
    analysis that are specific to mesh generation.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: CRC Press
  series: Computer and Information Science Series
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: Delaunay Mesh Generation
  type: book
  url: https://people.eecs.berkeley.edu/~jrs/meshbook.html
  venue: null
  volume: null
  web: null
  year: 2012
chiang2024chatbot:
  address: null
  articleno: null
  authors:
  - - Wei-Lin
    - Chiang
  - - Lianmin
    - Zheng
  - - Ying
    - Sheng
  - - Anastasios Nikolas
    - Angelopoulos
  - - Tianle
    - Li
  - - Dacheng
    - Li
  - - Banghua
    - Zhu
  - - Hao
    - Zhang
  - - Michael
    - Jordan
  - - Joseph E.
    - Gonzalez
  - - Ion
    - Stoica
  chapter: null
  descrip: Official reference for the LLM Chatbot Arena website where humans go to
    ask questions and rank their preference for various chatbots, generating a leaderboard
    for the "best" current chatbot. This is still considered the most reliable way
    to rank LLM performance on real problems. The website is https://chat.lmsys.org/
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  title: '{Chatbot Arena}: An Open Platform for Evaluating {LLM}s by Human Preference'
  type: inproceedings
  url: https://openreview.net/forum?id=3MW8GKNyzI
  venue: Forty-first International Conference on Machine Learning (ICML)
  volume: null
  web: null
  year: 2024
chollet2015keras:
  address: null
  articleno: null
  authors:
  - - Fran\c{c}ois
    - Chollet
  - - ''
    - others
  chapter: null
  descrip: The Keras docs -- great and highly impactful open source Python software,
    needs no introduction. A simplified interface for quickly building neural networks
    and other deep learning models with various backends frameworks such as Tensorflow,
    jax, and Pytorch.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Nov 2023'
  number: null
  pages: null
  publisher: GitHub
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - open source
  - neural networks
  - scientific machine learning
  - SciML
  - regression
  - classification
  - software
  - OSS
  - Python
  title: Keras
  type: GitHub repository
  url: https://keras.io
  venue: null
  volume: null
  web: null
  year: 2015
choudhary2019polynomialsized:
  address: null
  articleno: null
  authors:
  - - Aruni
    - Choudhary
  - - Michael
    - Kerber
  - - Sharath
    - Raghvendra
  chapter: null
  descrip: An algorithm for approximating Cech complexes (a type of simplicial complex)
    in high dimensions
  doi: 10.1007/s00454-017-9951-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0179-5376
  month: '1'
  note: null
  number: '1'
  pages:
  - '42'
  - '80'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - algorithms
  - high dimension
  title: Polynomial-sized topological approximations using the permutahedron
  type: article
  url: http://link.springer.com/10.1007/s00454-017-9951-2
  venue: Discrete \& Computational Geometry
  volume: '61'
  web: null
  year: 2019
chowdhary2024pyoed:
  address: New York, NY, USA
  articleno: '11'
  authors:
  - - Abhijit
    - Chowdhary
  - - Shady E.
    - Ahmed
  - - Ahmed
    - Attia
  chapter: null
  descrip: pyOED an open source Python numerical software library for performing optimal
    experimental design, e.g, for sensor placement at Argonne
  doi: 10.1145/3653071
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '6'
  note: null
  number: '2'
  pages:
  - 22
  publisher: Association for Computing Machinery
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  - software
  - open source
  - OSS
  - Python
  title: '{PyOED}: An Extensible Suite for Data Assimilation and Model-Constrained
    Optimal Design of Experiments'
  type: article
  url: https://dl.acm.org/doi/10.1145/3653071
  venue: ACM Transactions on Mathematical Software
  volume: '50'
  web: null
  year: 2024
christianson2022traditional:
  address: null
  articleno: null
  authors:
  - - Ryan B
    - Christianson
  - - Ryan M
    - Pollyea
  - - Robert B
    - Gramacy
  chapter: null
  descrip: A paper on the similarities and differences between kriging and Gaussian
    processes -- in practice, I see very little difference other than that people
    often perform more performance optimizations on Gaussian processes and consider
    Kriging to be a bit more strict
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Wiley Online Library
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - regression
  - uncertainty quantification
  - UQ
  title: Traditional kriging versus modern Gaussian processes for large-scale mining
    data
  type: article
  url: https://arxiv.org/abs/2207.10138
  venue: 'Statistical Analysis and Data Mining: The ASA Data Science Journal'
  volume: null
  web: null
  year: 2022
christie2000interpretation:
  address: null
  articleno: null
  authors:
  - - P.
    - Christie
  - - D.
    - Stroobandt
  chapter: null
  descrip: Various applications of Rent's rule, including usage as a partitioning
    cost function that would optimize partitions to match the rent coefficient of
    the underlying hardware
  doi: 10.1109/92.902258
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1063-8210
  month: '12'
  note: null
  number: '6'
  pages:
  - '639'
  - '648'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  title: The interpretation and application of Rent's rule
  type: article
  url: http://ieeexplore.ieee.org/document/902258/
  venue: IEEE Transactions on Very Large Scale Integration (VLSI) Systems
  volume: '8'
  web: null
  year: 2000
chugh2020scalarizing:
  address: Glasgow, United Kingdom
  articleno: null
  authors:
  - - Tinkle
    - Chugh
  chapter: null
  descrip: A really thorough review of most common scalarization functions used in
    multiobjective optimization with a focus on Bayesian optimization. Provides all
    scalarization functions, their equations, and a few adaptive schemes very concisely,
    then presents numerical results on DTLZ2 benchmarks
  doi: 10.1109/cec48606.2020.9185706
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '1'
  - '8'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - scalarization
  title: Scalarizing functions in Bayesian multiobjective optimization
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9185706/
  venue: 2020 IEEE Congress on Evolutionary Computation (CEC)
  volume: null
  web: null
  year: 2020
cignoni1998dewall:
  address: null
  articleno: null
  authors:
  - - Paolo
    - Cignoni
  - - Claudio
    - Montani
  - - Roberto
    - Scopigno
  chapter: null
  descrip: The DeWall algorithm is a divide-and-conquer algorithm for computing Delaunay
    triangulations in arbitrary dimensions. The idea is to build a wall of Delaunay
    simplices bisecting the space using a variation of the gift-wrapping algorithm.
    Then, each half can be triangulated separately (and possibly in parallel) an attempt
    to reproduce these results and parallelize the algorithm was what originally spawned
    the idea for DelaunaySparse
  doi: 10.1016/s0010-4485(97)00082-1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0010-4485
  month: '4'
  note: null
  number: '5'
  pages:
  - '333'
  - '341'
  publisher: Elsevier BV
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: '{DeWall}: A fast divide and conquer {D}elaunay triangulation algorithm in
    {$\mathbb{E}^d$}'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0010448597000821
  venue: Computer-Aided Design
  volume: '30'
  web: null
  year: 1998
cocchi2018implicit:
  address: null
  articleno: null
  authors:
  - - Guido
    - Cocchi
  - - Giampaolo
    - Liuzzi
  - - Alessandra
    - Papini
  - - Marco
    - Sciandrone
  chapter: null
  descrip: An algorithm for multiobjective implicit filtering (MOIF). Not mentioned
    here, but the open source numerical software for MOIF on the author's GitHub is
    often cited via this paper
  doi: 10.1007/s10589-017-9953-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '3'
  note: null
  number: '2'
  pages:
  - '267'
  - '296'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: An implicit filtering algorithm for derivative-free multiobjective optimization
    with box constraints
  type: article
  url: http://link.springer.com/10.1007/s10589-017-9953-2
  venue: Computational Optimization and Applications
  volume: '69'
  web: null
  year: 2018
cocchi2020augmented:
  address: null
  articleno: null
  authors:
  - - Guido
    - Cocchi
  - - Matteo
    - Lapucci
  chapter: null
  descrip: Building a multiobjective augmented Lagrangian -- basically, use a standard
    augmented Lagrangian penalty but apply it to all components of the objective.
    There is a proof that this will work. I don't use this method, but I use the same
    trick with the progressive barrier of Audet all the time and usually cite both
    papers
  doi: 10.1007/s10589-020-00204-z
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '9'
  note: null
  number: '1'
  pages:
  - '29'
  - '56'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - convex optimization
  - constrained optimization
  title: An augmented {Lagrangian} algorithm for multi-objective optimization
  type: article
  url: https://link.springer.com/10.1007/s10589-020-00204-z
  venue: Computational Optimization and Applications
  volume: '77'
  web: null
  year: 2020
conn2008geometry:
  address: null
  articleno: null
  authors:
  - - Andrew R
    - Conn
  - - Katya
    - Scheinberg
  - - Lu{\'\i}s N
    - Vicente
  chapter: null
  descrip: Andrew Conn's landmark paper on interpolation dataset geometry -- leads
    to the definition of sets being "well-poised" for interpolation, meaning that
    when the interpolation set's geometry meats some local geometric conditions (basically
    bounded away from singularity), then the resulting interpolant's error (and gradient
    / hessian errors) can be bounded and the resulting models can be used to perform
    gradient descent or SQP within a trust-region framework with guaranteed convergence
  doi: 10.1007/s10107-006-0073-5
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0025-5610
  month: '6'
  note: null
  number: 1-2
  pages:
  - '141'
  - '172'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - interpolation
  - approximation theory
  title: Geometry of interpolation sets in derivative free optimization
  type: article
  url: http://link.springer.com/10.1007/s10107-006-0073-5
  venue: Mathematical programming
  volume: '111'
  web: null
  year: 2008
conn2009introduction:
  address: Philadelphia, PA, USA
  articleno: null
  authors:
  - - Andrew R.
    - Conn
  - - Katya
    - Scheinberg
  - - Luis N.
    - Vicente
  chapter: null
  descrip: Conn and Scheinberg book on DFO -- describes the geometry of good interpolation
    sets, linear and quadratic interpolants and how to use them for derivative-free
    gradient descent and SQP frameworks, bounds on interpolation and gradient errors
    of various models, and how to efficiently restore good geometry when the optimization
    algorithm samples points in a subspace
  doi: 10.1137/1.9780898718768
  edition: null
  editors: []
  git: null
  isbn: '9780898716689'
  issn: null
  month: '1'
  note: null
  number: null
  pages: null
  publisher: SIAM
  series: MPS-SIAM Series on Optimization
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: Introduction to derivative-free optimization
  type: book
  url: http://epubs.siam.org/doi/book/10.1137/1.9780898718768
  venue: null
  volume: null
  web: null
  year: 2009
cooper2020pymoso:
  address: null
  articleno: null
  authors:
  - - Kyle
    - Cooper
  - - Susan R.
    - Hunter
  chapter: null
  descrip: 'PyMOSO: an open source Python numerical software library for solving multiobjective
    simulation optimization problems with integer and discrete variables via direct
    search / pattern search like techniques'
  doi: 10.1287/ijoc.2019.0902
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1091-9856
  month: '4'
  note: null
  number: '4'
  pages:
  - '1101'
  - '1108'
  publisher: Institute for Operations Research and the Management Sciences (INFORMS)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - pattern search
  - mixed-variable optimization
  - stochastic optimization
  - software
  - open source
  - OSS
  - Python
  title: '{PyMOSO}: {S}oftware for multi-objective simulation optimization with {R-PERLE}
    and {R-MinRLE}'
  type: article
  url: http://pubsonline.informs.org/doi/10.1287/ijoc.2019.0902
  venue: INFORMS Journal on Computing
  volume: '32'
  web: null
  year: 2020
costa2018rbfopt:
  address: null
  articleno: null
  authors:
  - - Alberto
    - Costa
  - - Giacomo
    - Nannicini
  chapter: null
  descrip: RBFOpt an open source library for solving single-objective blackbox optimization
  doi: 10.1007/s12532-018-0144-7
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1867-2949
  month: '12'
  note: null
  number: '4'
  pages:
  - '597'
  - '629'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  - software
  - open source
  - OSS
  title: '{RBFOpt}: an open-source library for black-box optimization with costly
    function evaluations'
  type: article
  url: http://link.springer.com/10.1007/s12532-018-0144-7
  venue: Mathematical Programming Computation
  volume: '10'
  web: null
  year: 2018
cristescu2015surrogatebased:
  address: null
  articleno: null
  authors:
  - - Cristina
    - Cristescu
  - - Joshua
    - Knowles
  chapter: null
  descrip: 'ParEGO latest code and update introducing algorithmic updates, improved
    software quality, and (I think) some parallel computing -- ParEGO is the first
    open source numerical multiobjective bayesian optimization software package and
    written in C++. It is basically EGO (the original Bayesian optimization software
    using expected improvment acquisition) plus augmented Lagrangian scalarization.
    Available here: github.com/CristinaCristescu/ParEGO_Eigen'
  doi: null
  edition: null
  editors: []
  git: https://github.com/CristinaCristescu/ParEGO_Eigen
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - scalarization
  - Gaussian process
  - software
  - open source
  - OSS
  - C++
  - parallel programming
  title: 'Surrogate-based multiobjective optimization: {ParEGO} update and test'
  type: inproceedings
  url: https://www.cs.bham.ac.uk/~jdk/UKCI-2015.pdf
  venue: Workshop on Computational Intelligence (UKCI)
  volume: '770'
  web: null
  year: 2015
custodio2011direct:
  address: null
  articleno: null
  authors:
  - - Ana Lu\'isa
    - Cust\'odio
  - - Jose F. A.
    - Madeira
  - - A. Ismael F.
    - Vaz
  - - Lu\'is N.
    - Vicente
  chapter: null
  descrip: Direct multisearch (DMS) is one of Custodio's earlier multiobjective direct
    search algorithms, which I think is a precursor to MutiGLODS. The numerical MATLAB
    software can be obtained by contacting her lab, but I'm not sure if they still
    distribute it as part of BoostDFO
  doi: 10.1137/10079731x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '7'
  note: null
  number: '3'
  pages:
  - '1109'
  - '1140'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  title: Direct Multisearch for Multiobjective Optimization
  type: article
  url: http://epubs.siam.org/doi/10.1137/10079731X
  venue: SIAM Journal on Optimization
  volume: '21'
  web: null
  year: 2011
custodio2018multiglods:
  address: null
  articleno: null
  authors:
  - - Ana Lu\'isa
    - Cust{\'{o}}dio
  - - Jose F. A.
    - Madeira
  chapter: null
  descrip: The MultiGLODS numerical software package is written in MATLAB and used
    for solving multiobjective optimization problems via direct search / pattern search
    with a clever restart algorithm for selecting directions to explore in order obtain
    good coverage of the Pareto front. I believe this version also can use polynomial
    surrogates to pre-select good search directions and filter out unneeded blackbox
    function / simulation evaluations. It is now part of the BoostDFO MATLAB numerical
    software toolkit, obtainable from contacting Custodio
  doi: 10.1007/s10898-018-0618-1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '10'
  note: null
  number: '2'
  pages:
  - '323'
  - '345'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - surrogate modeling
  title: '{MultiGLODS}: global and local multiobjective optimization using direct
    search'
  type: article
  url: http://link.springer.com/10.1007/s10898-018-0618-1
  venue: Journal of Global Optimization
  volume: '72'
  web: null
  year: 2018
dalal2008low:
  address: Leuven, Belgium
  articleno: null
  authors:
  - - Ishaan L.
    - Dalal
  - - Deian
    - Stefan
  - - Jared
    - Harwayne-Gidansky
  chapter: null
  descrip: Applications of low-discrepancy sequences in Monte carlo simulation
  doi: 10.1109/ASAP.2008.4580163
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: ''
  pages:
  - '108'
  - '113'
  publisher: IEEE
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  title: Low discrepancy sequences for {M}onte {C}arlo simulations on reconfigurable
    platforms
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/4580163
  venue: 2008 International Conference on Application-Specific Systems, Architectures
    and Processors
  volume: ''
  web: null
  year: 2008
dandurand2016quadratic:
  address: null
  articleno: null
  authors:
  - - Brian
    - Dandurand
  - - Margaret M.
    - Wiecek
  chapter: null
  descrip: Description and proof of coverage for a quadratic scalarization scheme
    for scalarizing multiobjective optimization problems
  doi: 10.1007/s00291-016-0453-z
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0171-6468
  month: '10'
  note: null
  number: '4'
  pages:
  - '1071'
  - '1096'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Quadratic scalarization for decomposed multiobjective optimization
  type: article
  url: http://link.springer.com/10.1007/s00291-016-0453-z
  venue: '{OR} Spectrum'
  volume: '38'
  web: null
  year: 2016
dantzig1998linear:
  address: Princeton, NJ, USA
  articleno: null
  authors:
  - - George B.
    - Dantzig
  chapter: null
  descrip: Dantzig's original (landmark) textbook on solving linear programming problems
    via the simplex method. This was obviously a landmark achievement in how to solve
    linear programming problems and more generally in the field of numerical optimization
  doi: null
  edition: '11'
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Princeton University Press
  series: Princeton Landmarks in Mathematics and Physics
  tags:
  - computational geometry
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  title: Linear Programming and Extensions
  type: book
  url: https://d1wqtxts1xzle7.cloudfront.net/56278680/Libro_Linear_Programming_George_Dantzig-libre.pdf?1523307505=&response-content-disposition=inline%3B+filename%3DLinear_Programming_and_Extensions.pdf&Expires=1746491165&Signature=WQRD07CTKkhpfjxG1R6Kb2tSq0cRnDUia1ETKdgTQX2wbUxpA2p7ZGudVpOpbsKgUZzsKL-U3CddGBaVVSTr~TSLwPadmYe8xHRVZ4KqyB~ms5zyu08vntJ0V-pRNY0sws9H~ktLJTgoABlZMkoYDA23Dbrh07yQqukyaqHsDuoTEZRzng6AIqN7CXO1KW2M4J~rS-M1mmM3bdTSMAoWPozK7Suea-HJPd7QbCMq2hB0JY5mhhi6nUHa6zIQVmjTCcPPdnX9O4lYgYPQgOBiMlIJ5yhYolhlHKXMA~2-g3rbpe4kqJXIEqICSWPByh72uohGvRJkDgUX-CkBw7FZNA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA
  venue: null
  volume: null
  web: null
  year: 1998
das1998normalboundary:
  address: null
  articleno: null
  authors:
  - - Indraneel
    - Das
  - - John E.
    - Dennis
  chapter: null
  descrip: The normal boundary intersection (NBI) method was one of the first adaptive
    scalarization schemes for multiobjective optimization problems. It uses linear
    scalarization but does so adaptively using the angle of the intersecting vector
    at a target point to set the weights in order to adaptively fill in gaps on the
    Pareto front
  doi: 10.1137/S1052623496307510
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '8'
  note: null
  number: '3'
  pages:
  - '631'
  - '657'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: 'Normal-boundary intersection: A new method for generating the {P}areto surface
    in nonlinear multicriteria optimization problems'
  type: article
  url: http://epubs.siam.org/doi/10.1137/S1052623496307510
  venue: SIAM Journal on Optimization
  volume: '8'
  web: null
  year: 1998
das2005quantum:
  address: null
  articleno: null
  authors:
  - - Arnab
    - Das
  - - Bikas K
    - Chakrabarti
  chapter: null
  descrip: Textbook on quantum annealing
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer Science \& Business Media
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Quantum annealing and related optimization methods
  type: book
  url: https://www.researchgate.net/publication/252247914_Quantum_Annealing_and_Related_Optimization_Methods
  venue: null
  volume: '679'
  web: null
  year: 2005
dash2024optimizing:
  address: Hamburg, Germany
  articleno: null
  authors:
  - - Sajal
    - Dash
  - - Isaac R
    - Lyngaas
  - - Junqi
    - Yin
  - - Xiao
    - Wang
  - - Romain
    - Egele
  - - J. Austin
    - Ellis
  - - Matthias
    - Maiterth
  - - Guojing
    - Cong
  - - Feiyi
    - Wang
  - - Prasanna
    - Balaprakash
  chapter: null
  descrip: Paper on how the HPC Frontier at ORNL was configured to train trillion-parameter
    large language models (LLMs). There is a really nice discussion of the model architectures
    and sizes, and the memory requirements of each. There is also a nice discussion
    of parallel pipelines and model vs data sharding. Then they discuss their code
    bases and software stacks. Finally, they perform a hyperparameter optimization
    with DeepHyper to determine optimal block sizes and pipeline overlapping configurations.
  doi: 10.23919/ISC.2024.10528939
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '5'
  note: null
  number: null
  pages:
  - '1'
  - '11'
  publisher: IEEE
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - transformers
  - optimization
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  title: Optimizing Distributed Training on Frontier for Large Language Models
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/10528939/
  venue: ISC High Performance 2024 Research Paper Proceedings (39th International
    Conference)
  volume: null
  web: null
  year: 2024
datta2016surrogateassisted:
  address: null
  articleno: null
  authors:
  - - Rituparna
    - Datta
  - - Rommel G.
    - Regis
  chapter: null
  descrip: An early paper on using surrogate models to reduce the cost (in terms of
    true simulation / blackbox function evaluations) when using multiobjective evolutionary
    algorithm to solve computationally expensive blackbox and simulation optimization
    problems
  doi: 10.1016/j.eswa.2016.03.044
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0957-4174
  month: '9'
  note: null
  number: null
  pages:
  - '270'
  - '284'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - evolutionary algorithms
  - EA
  title: A surrogate-assisted evolution strategy for constrained multi-objective optimization
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0957417416301452
  venue: Expert Systems with Applications
  volume: '57'
  web: null
  year: 2016
dattani2019quadratization:
  address: null
  articleno: null
  authors:
  - - Nike
    - Dattani
  chapter: null
  descrip: Article applying quantum AND gate to perform reduction by substitution,
    section 5, page 44
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Quadratization in discrete optimization and quantum mechanics
  type: article
  url: https://arxiv.org/abs/1901.04405
  venue: arXiv preprint arXiv:1901.04405
  volume: null
  web: null
  year: 2019
daulton2020differentiable:
  address: null
  articleno: null
  authors:
  - - Samuel
    - Daulton
  - - Maximilian
    - Balandat
  - - Eytan
    - Bakshy
  chapter: null
  descrip: A technique for differentiating expected hypervolume improvement EHVI (and
    its monte carlo variant qEHVI), which can be used as the acquisition function
    for solving multiobjective blackbox optimization problems with BoTorch
  doi: null
  edition: null
  editors:
  - - H.
    - Larochelle
  - - M.
    - Ranzato
  - - R.
    - Hadsell
  - - M.F.
    - Balcan
  - - H.
    - Lin
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '9851'
  - '9864'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - autograd
  - backpropagation
  - scalarization
  title: Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective
    {B}ayesian Optimization
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2020/file/6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '33'
  web: null
  year: 2020
daulton2021parallel:
  address: null
  articleno: null
  authors:
  - - Samuel
    - Daulton
  - - Maximilian
    - Balandat
  - - Eytan
    - Bakshy
  chapter: null
  descrip: The first paper on performing parallel Bayesian optimization using the
    expected hypervolume improvement acquisition function in BoTorch
  doi: null
  edition: null
  editors:
  - - M.
    - Ranzato
  - - A.
    - Beygelzimer
  - - Y.
    - Dauphin
  - - P.S.
    - Liang
  - - J. Wortman
    - Vaughan
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '2187'
  - '2200'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - stochastic optimization
  - scalarization
  - Gaussian process
  title: Parallel {B}ayesian Optimization of Multiple Noisy Objectives with Expected
    Hypervolume Improvement
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '34'
  web: null
  year: 2021
dauphin2014identifying:
  address: null
  articleno: null
  authors:
  - - Yann N.
    - Dauphin
  - - Razvan
    - Pascanu
  - - Caglar
    - Gulcehre
  - - Kyunghyun
    - Cho
  - - Surya
    - Ganguli
  - - Yoshua
    - Bengio
  chapter: null
  descrip: Interesting paper on why it is generally OK to use local optimizers when
    solving non convex optimization problems in high-dimensional spaces. In general,
    in high-dimensional spaces, almost every critical point will be a saddle point
    with high probability. Therefore, first-order methods tend to perform very well
    on these problems as they converge quickly but are not attracted to saddle points
    and therefore tend to find the global optimum in the limit. The analysis of the
    probability that a critical point will be a saddle point is based on a spectral
    analysis of the hessian at each critical point other than the global minimum/maximum
    -- all of the eigenvalues must be positive or negative for the critical point
    to be a local minima / maxima, and the probability of this occurring decays as
    the number of eigenvalues grows with the dimension of the Hessian. The authors
    also experimentally validate these claims by extracting critical points from the
    loss landscapes of single layer MLPs trained on down-sampled versions of MNIST
    and CIFAR-10.
  doi: null
  edition: null
  editors:
  - - Z.
    - Ghahramani
  - - M.
    - Welling
  - - C.
    - Cortes
  - - N.
    - Lawrence
  - - K.Q.
    - Weinberger
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - optimization
  - convex optimization
  - stochastic optimization
  - global optimization
  - high dimension
  - scientific machine learning
  - SciML
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '27'
  web: null
  year: 2014
de2008tracedriven:
  address: Tsukuba, Japan
  articleno: null
  authors:
  - - Pradipta
    - De
  - - Ravi
    - Kothari
  - - Vijay
    - Mann
  chapter: null
  descrip: An emulation framework for HPC performance modeling and predicting true
    performance under variability.
  doi: 10.1109/clustr.2008.4663776
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '9'
  note: null
  number: null
  pages:
  - '232'
  - '241'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - performance modeling
  title: A trace-driven emulation framework to predict scalability of large clusters
    in presence of {OS} jitter
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/4663776
  venue: Proceedings of the 2008 IEEE International Conference on Cluster Computing
  volume: null
  web: null
  year: 2008
dean2008mapreduce:
  address: New York, NY, USA
  articleno: null
  authors:
  - - Jeffrey
    - Dean
  - - Sanjay
    - Ghemawat
  chapter: null
  descrip: Original publication of MapReduce from Google research, the distributed
    computing paradigm that drives all Hadoop clusters. This was the most common distributed
    computing programming and filesystem in the 2010s, and still persist today. The
    idea is that each computation over massive datasets is posed as a sequence of
    map and reduce operations, where the map performs some simple operation on the
    data (which can be massively parallelized) and the reduce provides a way for two
    data items to be reduced into a single item. By applying map and reduce over and
    over on all the data, a computation on a massive fully distributed dataset can
    be performed in parallel with a logarithmic number of sequential steps and without
    ever holding all the data on one machine
  doi: 10.1145/1327452.1327492
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0001-0782
  month: '1'
  note: null
  number: '1'
  pages:
  - '107'
  - '113'
  publisher: Association for Computing Machinery
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  title: 'MapReduce: simplified data processing on large clusters'
  type: article
  url: https://doi.org/10.1145/1327452.1327492
  venue: Communications of the ACM
  volume: '51'
  web: null
  year: 2008
dean2013tail:
  address: null
  articleno: null
  authors:
  - - Jeffrey
    - Dean
  - - Luiz Andr'e
    - Barroso
  chapter: null
  descrip: A review paper on how HPC performance variability causes issues at scale
    in Google's data centers, particularly, since requests generally have to wait
    on the longest running process (i.e., the performance variability "tail") and
    so the mean performance time can be somewhate meaningless
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '2'
  pages:
  - '74'
  - '80'
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - performance modeling
  title: The tail at scale
  type: article
  url: https://research.google/pubs/the-tail-at-scale
  venue: Communications of the ACM
  volume: '56'
  web: null
  year: 2013
deanl2019press:
  address: null
  articleno: null
  authors: []
  chapter: null
  descrip: The exascale computing project press release
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: March
  note: 'Last accessed: April 28, 2020'
  number: null
  pages: null
  publisher: '{U.S.\ Department of Energy, Argonne National Laboratory}'
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - software
  - parallel programming
  - MPI
  - CUDA
  title: 'Press Release: {U.S.\ Department of Energy and Intel} to deliver first exascale
    supercomputer'
  type: misc
  url: https://www.anl.gov/article/us-department-of-energy-and-intel-to-deliver-first-exascale-supercomputer
  venue: null
  volume: null
  web: null
  year: 2019
deb2002fast:
  address: null
  articleno: null
  authors:
  - - Kalyanmoy
    - Deb
  - - Amrit
    - Pratap
  - - Sameer
    - Agarwel
  - - T.
    - Meyarivan
  chapter: null
  descrip: 'The original NSGA-II paper: a multiobjective evolutionary algorithm (MOEA)
    that scales well and performs extremely well in practice. The main contribution
    is a fast method for performing nondominated sorting so that the authors can ensure
    all efficient points persist in every generation. This method is generally considered
    to be the baseline in multiobjective optimization. While the algorithm is a simple
    heuristic that is extremely wasteful in terms of the number of true blackbox function
    / simulation evaluations, it performs extremely well in practice by the hypervolume
    indicator. I have found that it is very difficult to obtain better performance
    than NSGA-II on both benchmark and real-world problems in the limit, unless you
    have some "secret sauce" to exploit for your particular problem'
  doi: 10.1109/4235.996017
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '4'
  note: null
  number: '2'
  pages:
  - '182'
  - '197'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  - evolutionary algorithms
  - EA
  title: 'A fast and elitist multiobjective genetic algorithm: {NSGA-II}'
  type: article
  url: http://ieeexplore.ieee.org/document/996017
  venue: IEEE Transactions on Evolutionary Computation
  volume: '6'
  web: null
  year: 2002
deb2002scalable:
  address: Honolulu, HI, USA
  articleno: null
  authors:
  - - Kalyanmoy
    - Deb
  - - Lothar
    - Thiele
  - - Marco
    - Laumanns
  - - Eckart
    - Zitzler
  chapter: null
  descrip: The DTLZ test problems are the standard test problems used in all multiobjective
    evolutionary optimization papers. They are algebraic test problems that can scale
    to as many objectives and variables as one desires. Each problem also has a pathological
    property that makes it extremely difficult or degenerate for multiobjective optimization
    algorithms. This maeks the suite as a whole extremely convenient for testing,
    scaling, and evaluating results. However, several of the problems are so difficult
    that no solvers can reliably solve them. Additionally, all the problems have the
    property that the last "n" variables are essentially unused, with their optimum
    being 0.5 and not changing as we move across the Pareto front, which could be
    unrealistic for certain problems
  doi: 10.1109/CEC.2002.1007032
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '825'
  - '830'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - global optimization
  - benchmarking
  - high dimension
  - evolutionary algorithms
  - EA
  title: Scalable multi-objective optimization test problems
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/1007032
  venue: Proc. 2002 IEEE Congress on Evolutionary Computation (CEC '02)
  volume: '1'
  web: null
  year: 2002
deb2013evolutionary:
  address: null
  articleno: null
  authors:
  - - Kalyanmoy
    - Deb
  - - Himanshu
    - Jain
  chapter: null
  descrip: 'The orgiinal NSGA-III paper part 1: a multiobjective evolutionary algorithm
    similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many
    objectives by having the user provide a collection of well-spaced reference points
    and optimizing toward those'
  doi: 10.1109/TEVC.2013.2281535
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '8'
  note: null
  number: '4'
  pages:
  - '577'
  - '601'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - high dimension
  - evolutionary algorithms
  - EA
  title: 'An evolutionary many-objective optimization algorithm using reference-point-based
    nondominated sorting approach, part {I}: solving problems with box constraints'
  type: article
  url: http://ieeexplore.ieee.org/document/6600851
  venue: IEEE Transactions on Evolutionary Computation
  volume: '18'
  web: null
  year: 2013
deberg2008computational:
  address: Santa Clara, CA, USA
  articleno: null
  authors:
  - - Mark
    - de Berg
  - - Otfried
    - Cheong
  - - Marc
    - van Kreveld
  - - Mark
    - Overmars
  chapter: null
  descrip: A common (standard) textbook on computational geometry covering all basic
    definitions, data structures, algorithms, proofs of correctness, time and space
    complexity analyses, and applications. This is the textbook that I used in grad
    school when learning computational geometry
  doi: null
  edition: '3'
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer-Verlag, TELOS
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  - projection
  - convex hull
  title: 'Computational Geometry: Algorithms and Applications'
  type: book
  url: https://cimec.org.ar/foswiki/pub/Main/Cimec/GeometriaComputacional/DeBerg_-_Computational_Geometry_-_Algorithms_and_Applications_2e.pdf
  venue: null
  volume: null
  web: null
  year: 2008
deboor1978practical:
  address: New York, NY, USA
  articleno: null
  authors:
  - - Carl R.
    - de Boor
  chapter: null
  descrip: DeBoor's classic textbook on splines and B-spline interpolation, covering
    polynomial interpolation, divided differences, and error kernels; node spacing
    and chebyshev nodes; piecewise linear and polynomial interpolation; piecewise
    polynomials as combination of basis functions; generalized B-spline via a recursive
    definition; algorithms for stably building and evaluating B-Splines; knot and
    node placement and error rates and diminishing returns; The Schoenberg-Whitney
    Theorem and B-splines as RKHS kernels; multivariate B-splines as tensor products;
    applications; and Fortran subroutines for all steps in the process.
  doi: 10.2307/2006241
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0025-5718
  month: jan
  note: null
  number: '149'
  pages: null
  publisher: Springer-Verlag
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - RBFs
  - high dimension
  - interpolation
  - regression
  - approximation theory
  title: A Practical Guide to Splines
  type: book
  url: https://www.researchgate.net/publication/200744645_A_Practical_Guide_to_Spline
  venue: Applied Mathematical Sciences
  volume: '34'
  web: null
  year: 1978
delaunay1934sur:
  address: null
  articleno: null
  authors:
  - - B.
    - Delaunay
  chapter: null
  descrip: Original reference for Delaunay triangulations -- I have not read this,
    it's in French, but this is the proper way to cite the original idea for Delaunay
    triangulations
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '6'
  pages: null
  publisher: null
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  title: Sur la sph\'ere vide
  type: article
  url: https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=im&paperid=4937&option_lang=eng
  venue: 'Bull. Acad. Science USSR VII: Class. Sci. Math.,'
  volume: null
  web: null
  year: 1934
deng2009imagenet:
  address: Miami, FL
  articleno: null
  authors:
  - - Jia
    - Deng
  - - Wei
    - Dong
  - - Richard
    - Socher
  - - Li-Jia
    - Li
  - - Kai
    - Li
  - - Li
    - Fei-Fei
  chapter: null
  descrip: One of the largest most difficult image classification benchmark problems
    from the 2010s, which drove a lot of advancement. Was a bit too difficult for
    the average researcher to solve since it required a lot of training resources
    to get good performance, but drove advancement in developing and training large
    (deep) neural networks with many layers and millions of parameters
  doi: 10.1109/CVPR.2009.5206848
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: ''
  pages:
  - '248'
  - '255'
  publisher: IEEE
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - benchmarking
  - neural networks
  - classification
  title: '{ImageNet}: A large-scale hierarchical image database'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/5206848/
  venue: 2009 IEEE Conference on Computer Vision and Pattern Recognition
  volume: ''
  web: null
  year: 2009
deryck2022generic:
  address: null
  articleno: null
  authors:
  - - Tim
    - De Ryck
  - - Siddhartha
    - Mishra
  chapter: null
  descrip: A framework for deriving error bounds for neural networks in scientific
    machine learning applications, specifically for physics-informed neural networks
    (PINNs)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '10945'
  - '10958'
  publisher: null
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - PINNs
  - regression
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Generic bounds on the approximation error for physics-informed (and) operator
    learning
  type: article
  url: https://proceedings.neurips.cc/paper_files/paper/2022/file/46f0114c06524debc60ef2a72769f7a9-Paper-Conference.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '35'
  web: null
  year: 2022
deshpande2011data:
  address: null
  articleno: null
  authors:
  - - Shubhangi
    - Deshpande
  - - Layne T.
    - Watson
  - - Jiang
    - Shu
  - - Naren
    - Ramakrishnan
  chapter: null
  descrip: 'A study of various surrogate models for optimizing with WBCSim design
    tool which simulates and analyzes the behavior of wood-based composites: in summary,
    Shepard''s method (locally linear) performed best in this analysis, and therefore
    was chosen as the main surrogate model in Deshpande''s 2016 algorithm (above).
    It is hypothesized that Shepard''s method''s ability to extrapolate (typically
    an undesirable feature in interpolation) makes it good for surrogate modeling
    for optimization, which requires extrapolation based on local trends'
  doi: 10.1007/s00366-010-0192-8
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0177-0667
  month: '7'
  note: null
  number: '3'
  pages:
  - '211'
  - '223'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: Data driven surrogate-based optimization in the problem solving environment
    {WBCSim}
  type: article
  url: http://link.springer.com/10.1007/s00366-010-0192-8
  venue: Engineering with Computers
  volume: '27'
  web: null
  year: 2011
deshpande2016multiobjective:
  address: null
  articleno: null
  authors:
  - - Shubhangi
    - Deshpande
  - - Layne T.
    - Watson
  - - Robert A.
    - Canfield
  chapter: null
  descrip: An algorithm for solving blackbox multiobjective optimization problems
    via trust region descent, using locally linear (shepard's method) surrogate models,
    and a multiobjective variant of DIRECT. Also, the authors propose a novel adaptive
    weighting scheme within the trust regions. The motivating application is an aircraft
    design optimization problem.
  doi: 10.1080/10556788.2015.1048861
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1055-6788
  month: '1'
  note: null
  number: '1'
  pages:
  - '110'
  - '133'
  publisher: Informa UK Limited
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  - surrogate modeling
  title: Multiobjective optimization using an adaptive weighting scheme
  type: article
  url: http://www.tandfonline.com/doi/full/10.1080/10556788.2015.1048861
  venue: Optimization Methods and Software
  volume: '31'
  web: null
  year: 2016
devillers2001walking:
  address: Medford, MA, USA
  articleno: null
  authors:
  - - Olivier
    - Devillers
  - - Sylvain
    - Pion
  - - Monique
    - Teillaud
  chapter: null
  descrip: One of (if not the) early publication on how to perform various simplex
    walks (such as a visibility walk) within a triangulation in order to locate target
    simplices, after the triangulation has already been computed and stored in a standard
    data structure (such as a simplex list)
  doi: 10.1145/378583.378643
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - '106'
  - '114'
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: Walking in a triangulation
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/378583.378643
  venue: Proceedings of the Seventeenth Annual Symposium on Computational Geometry
    (SCG '01)
  volume: null
  web: null
  year: 2001
devlin2019bert:
  address: Minneapolis, Minnesota
  articleno: null
  authors:
  - - Jacob
    - Devlin
  - - Ming-Wei
    - Chang
  - - Kenton
    - Lee
  - - Kristina
    - Toutanova
  chapter: null
  descrip: BERT was one of the first transformer-based neural network architectures
    for solving language-related tasks, such as language translations, at Google.
    This was also by far the largest model of its time. BERT paved the way for modern
    large language models, probably moreso than the Attention is all you need paper
  doi: 10.18653/v1/N19-1423
  edition: null
  editors:
  - - Jill
    - Burstein
  - - Christy
    - Doran
  - - Thamar
    - Solorio
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '4171'
  - '4186'
  publisher: Association for Computational Linguistics
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - transformers
  - neural networks
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - scientific machine learning
  - SciML
  title: '{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  type: inproceedings
  url: https://aclanthology.org/N19-1423/
  venue: 'Proceedings of the 2019 Conference of the North {A}merican Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)'
  volume: null
  web: null
  year: 2019
dhariwal2017openai:
  address: null
  articleno: null
  authors:
  - - Prafulla
    - Dhariwal
  - - Christopher
    - Hesse
  - - Oleg
    - Klimov
  - - Alex
    - Nichol
  - - Matthias
    - Plappert
  - - Alec
    - Radford
  - - John
    - Schulman
  - - Szymon
    - Sidor
  - - Yuhuai
    - Wu
  - - Peter
    - Zhokhov
  chapter: null
  descrip: 'A repository of baseline models for reinforcement learning algorithms.
    OpenAI encourages researchers to compare their models against these baselines.
    All of the provided RL models are open source and available for download in Python
    from: github.com/openai/baselines'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 2025'
  number: null
  pages: null
  publisher: GitHub
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - open source
  - neural networks
  title: '{OpenAI} Baselines'
  type: GitHub repository
  url: https://github.com/openai/baselines
  venue: null
  volume: null
  web: null
  year: 2017
diamond2016cvxpy:
  address: null
  articleno: null
  authors:
  - - Steven
    - Diamond
  - - Stephen
    - Boyd
  chapter: null
  descrip: CVXPY is an open source Python optimization and modeling language for solving
    convex optimization problems in a disciplined way (meaning that we ensure convexity
    through hard rules on the problem definition). From the lab of Stephen Boyd
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '83'
  pages:
  - '1'
  - '5'
  publisher: null
  series: null
  tags:
  - computational geometry
  - optimization
  - linear programming
  - quadratic programming
  - LP
  - QP
  - convex optimization
  - constrained optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - Python
  title: '{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization'
  type: article
  url: http://jmlr.org/papers/v17/15-408.html
  venue: Journal of Machine Learning Research
  volume: '17'
  web: null
  year: 2016
domahidi2013ecos:
  address: Z{\"u}rich, Switzerland
  articleno: null
  authors:
  - - Alexander
    - Domahidi
  - - Eric
    - Chu
  - - Stephen
    - Boyd
  chapter: null
  descrip: ECOS is an open source numerical software for solving second-order cone
    optimization problems, from the lab of Stephen Boyd. In my experience, this software
    is the best tool from Boyd's lab and the most robust to degeneracy
  doi: 10.23919/ECC.2013.6669541
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '3071'
  - '3076'
  publisher: IEEE
  series: null
  tags:
  - computational geometry
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - C
  title: '{ECOS}: {A}n {SOCP} solver for embedded systems'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/6669541
  venue: European Control Conference (ECC)
  volume: null
  web: null
  year: 2013
dong2020nasbench201:
  address: null
  articleno: null
  authors:
  - - Xuanyi
    - Dong
  - - Yi
    - Yang
  chapter: null
  descrip: NAS-Bench-201 introduces a cell-based neural architecture search space
    representation (i.e., problem embedding) that is used in many neural architecture
    search softwares circa ~2022. This includes the benchmark problems in JAHS-Bench-201
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - benchmarking
  title: '{NAS-Bench-201}: Extending the Scope of Reproducible Neural Architecture
    Search'
  type: inproceedings
  url: https://openreview.net/forum?id=HJxyZkBKDr
  venue: 8th International Conference on Learning Representations (ICLR 2020)
  volume: null
  web: null
  year: 2020
dongarra2003linpack:
  address: null
  articleno: null
  authors:
  - - Jack J.
    - Dongarra
  - - Piotr
    - Luszczek
  - - Antoine
    - Petitet
  chapter: null
  descrip: Summary article on the history of the LINPACK of benchmark (the standard
    benchmark for HPC performance tuning and evaluation), which defines the HPC Top
    500 list
  doi: 10.1002/cpe.728
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1532-0626
  month: '8'
  note: null
  number: '9'
  pages:
  - '803'
  - '820'
  publisher: Wiley
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - benchmarking
  - software
  - Fortran
  - parallel programming
  title: 'The {LINPACK} benchmark: past, present, and future'
  type: article
  url: https://onlinelibrary.wiley.com/doi/10.1002/cpe.728
  venue: 'Concurrency and Computation: Practice and Experience'
  volume: '15'
  web: null
  year: 2003
dridi2017prime:
  address: null
  articleno: null
  authors:
  - - Raouf
    - Dridi
  - - Hedayat
    - Alghassi
  chapter: null
  descrip: Another quantum annealing-based prime factorization article
  doi: 10.1038/srep43048
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2045-2322
  month: '2'
  note: null
  number: '1'
  pages:
  - 43048
  publisher: Nature Publishing Group
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Prime factorization using quantum annealing and computational algebraic geometry
  type: article
  url: https://www.nature.com/articles/srep43048
  venue: Scientific reports
  volume: '7'
  web: null
  year: 2017
dubey2021performance:
  address: null
  articleno: null
  authors:
  - - Anshu
    - Dubey
  - - Lois Curfman
    - McInnes
  - - Rajeev
    - Thakur
  - - Erik W.
    - Draeger
  - - Thomas
    - Evans
  - - Timothy C.
    - Germann
  - - William E.
    - Hart
  chapter: null
  descrip: Achieving performance portability of parallel codes in the exascale computing
    project (ECP) across various GPU-based architectures, which is challenging since
    different GPU vendors use different GPU programming libraries (e.g., CUDA for
    NVIDIA vs HIPP for AMD) -- high-quality re-usable open source software and software
    hardware codesign are cited as important issues from the software perspective,
    as well as performance portability (i.e., port between HIPP, CUDA, OpenMP, and
    SYCL/DPC++ for on-node parallelism)
  doi: 10.1109/MCSE.2021.3098231
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1521-9615
  month: '9'
  note: null
  number: '5'
  pages:
  - '46'
  - '54'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - GPU computing
  - CUDA
  - software
  - MPI
  - OpenMP
  - parallel programming
  title: 'Performance Portability in the {E}xascale {C}omputing {P}roject: Exploration
    Through a Panel Series'
  type: article
  url: https://ieeexplore.ieee.org/document/9495114
  venue: Computing in Science \& Engineering
  volume: '23'
  web: null
  year: 2021
dubois2024lengthcontrolled:
  address: null
  articleno: null
  authors:
  - - Yann
    - Dubois
  - - Bal{\'a}zs
    - Galambosi
  - - Percy
    - Liang
  - - Tatsunori B
    - Hashimoto
  chapter: null
  descrip: 'Latest AlpacaEval paper introducing length control to the evaluation pipeline,
    which prevents models from being able to game the evaluation system by producing
    longer outputs. AlpacaEval is a standard English-language LLM evaluation benchmark,
    which ranks and evaluates how well a LLM is capable of following instructions
    and completing basic tasks using the Alpaca model (another LLM) to judge performance
    instead of a human judge. This particular benchmark has a 98% correlation with
    the LLM Chatbot Arena without requiring human participants and using minimal OpenAI
    credits to run. Open source Python interface at: github.com/tatsu-lab/alpaca_eval'
  doi: 10.48550/arXiv.2404.04475
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2404.04475
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - benchmarking
  title: 'Length-Controlled {AlpacaEval}: A Simple Way to Debias Automatic Evaluators'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2024
duchi2011adaptive:
  address: null
  articleno: null
  authors:
  - - John
    - Duchi
  - - Elad
    - Hazan
  - - Yoram
    - Singer
  chapter: null
  descrip: The original paper for AdaGrad (adaptive subgradient method) which replaced
    the subgradient method with an adaptive estimate for the gradient, where each
    component of the gradient is rescaled by an adaptive estimate for the standard
    deviation in that direction based on previous iterates. This adaptive estimate
    for standard deviation in each axis-aligned direction serves as a diagonal approximation
    to the Hessian matrix, giving second-order like properties to the method and greatly
    improving the practical convergence. AdaGrad was very popular and considered the
    state-of-the-art optimization algorithm for training neural networks upon its
    initial release, but was quickly replaced by Adam, which added a Nesterov momentum
    esque smoothing to this adaptive gradient estimation in order to further improve
    convergence rates on nonsmooth, highly stochastic, and ill-conditioned problems
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '61'
  pages:
  - '2121'
  - '2159'
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - optimization
  - stochastic optimization
  - convex optimization
  - scientific machine learning
  - SciML
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
  type: article
  url: http://jmlr.org/papers/v12/duchi11a.html
  venue: Journal of Machine Learning Research
  volume: '12'
  web: null
  year: 2011
dunlop2008use:
  address: Edinburgh, UK
  articleno: null
  authors:
  - - Dominic
    - Dunlop
  - - Sebastien
    - Varrette
  - - Pascal
    - Bouvry
  chapter: null
  descrip: An example of using genetic algorithms for autotuning HPC libraries (such
    as BLAS and LAPACK)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '105'
  - '113'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - autotuning
  - evolutionary algorithms
  - EA
  title: On the use of a genetic algorithm in high performance computing benchmark
    tuning
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/4667550
  venue: Proceedings of the 2008 International Symposium on Performance Evaluation
    of Computer and Telecommunication Systems
  volume: null
  web: null
  year: 2008
dunning2017jump:
  address: null
  articleno: null
  authors:
  - - Iain
    - Dunning
  - - Joey
    - Huchette
  - - Miles
    - Lubin
  chapter: null
  descrip: The JuMP modeling language in Julia -- a modeling language for modeling
    and solving linear and nonlinear programming (optimization) problems in Julia.
    The implementation is an open source numerical software
  doi: 10.1137/15M1020575
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0036-1445
  month: '1'
  note: null
  number: '2'
  pages:
  - '295'
  - '320'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - optimization
  - linear programming
  - LP
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - julia
  title: '{JuMP}: A Modeling Language for Mathematical Optimization'
  type: article
  url: https://epubs.siam.org/doi/10.1137/15M1020575
  venue: SIAM Review
  volume: '59'
  web: null
  year: 2017
durillo2011jmetal:
  address: null
  articleno: null
  authors:
  - - Juan J.
    - Durillo
  - - Antonio J.
    - Nebro
  chapter: null
  descrip: jMetal is an open source numerical software library implementing multiobjective
    optimization solvers in Java. Last I checked, most of the solvers were heuristic
    methods such as evolutionary algorithms and/or simulated annealing. This is widely
    used in some fields of engineering
  doi: 10.1016/j.advengsoft.2011.05.014
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0965-9978
  month: '10'
  note: null
  number: '10'
  pages:
  - '760'
  - '771'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - evolutionary algorithms
  - EA
  - simulated annealing
  - SA
  - software
  - open source
  - OSS
  - Java
  title: '{jMetal}: A {J}ava framework for multi-objective optimization'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0965997811001219
  venue: Advances in Engineering Software
  volume: '42'
  web: null
  year: 2011
dzahini2025class:
  address: null
  articleno: null
  authors:
  - - Kwassi Joseph
    - Dzahini
  - - Stefan M.
    - Wild
  chapter: null
  descrip: A careful analysis of Johnson-Lindenstrauss transforms, and how we can
    sample dimensions from hashing matrices to ensure that the resulting random subspace
    method converges into the true optimum in the limit
  doi: 10.1137/23M1605661
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0895-4798
  month: '3'
  note: null
  number: '1'
  pages:
  - '416'
  - '438'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - high dimension
  title: "A Class of Sparse {Johnson\u2013Lindenstrauss} Transforms and Analysis of\
    \ their Extreme Singular Values"
  type: article
  url: https://doi.org/10.1137/23M1605661
  venue: SIAM Journal on Matrix Analysis and Applications
  volume: '46'
  web: null
  year: 2025
eckman2023simopt:
  address: Linthicum, MD, USA
  articleno: null
  authors:
  - - David J.
    - Eckman
  - - Shane G.
    - Henderson
  - - Sara
    - Shashaani
  chapter: null
  descrip: SimOpt is a really nice suite of real-world and real-world inspired simulation
    optimization problems. They are mostly stochastic. This is the standard benchmark
    suite in the operations research and business operations community. In order to
    publish work in those venues, one needs to perform well on these problems. The
    paper details how SimOpt has been refactored to provide new problems, features,
    reproducability, and better interfaces in the latest patch. The open source Python
    software is available at github.com/simopt-admin/simopt
  doi: 10.1287/ijoc.2023.1273
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1526-5528
  month: '3'
  note: null
  number: '2'
  pages:
  - '495'
  - '508'
  publisher: INFORMS
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - stochastic optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Python
  title: '{SimOpt}: A Testbed for Simulation-Optimization Experiments'
  type: article
  url: https://pubsonline.informs.org/doi/10.1287/ijoc.2023.1273
  venue: INFORMS Journal on Computing
  volume: '35'
  web: null
  year: 2023
edelsbrunner1989acyclicity:
  address: Saarbruchen, West Germany
  articleno: null
  authors:
  - - Herbert
    - Edelsbrunner
  chapter: null
  descrip: In a general triangulation, most simplex walks including a visibility walk
    are not guaranteed to converge. I.e., due to poor geometry one could get stuck
    in an infinite cycle walking in circles. This is related to how Dantzig's simplex
    method can also get caught in infinite cycles. Edelsbrunner proves that in the
    special case of Delaunay triangulations, this actually can't happen and so the
    convergence of the visibility walk is guaranteed
  doi: 10.1145/73833.73850
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '145'
  - '151'
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  title: An acyclicity theorem for cell complexes in d dimensions
  type: inproceedings
  url: http://portal.acm.org/citation.cfm?doid=73833.73850
  venue: Proceedings of the Fifth Annual Symposium on Computational Geometry (SCG
    '89)
  volume: null
  web: null
  year: 1989
edelsbrunner1990simulation:
  address: null
  articleno: null
  authors:
  - - Herbert
    - Edelsbrunner
  - - Ernst Peter
    - M{\"u}cke
  chapter: null
  descrip: 'Edelsbrunner''s famous simulation of simplicity: introducing minimal perturbations
    to real-world datasets to ensure general position. This allows us to use non robust
    computational geometry algorithms without worrying about handling degeneracy.
    The method is a bit too expensive for high-dimensional problems, but standard
    for lower-dimensional problems.'
  doi: 10.1145/77635.77639
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0730-0301
  month: '1'
  note: null
  number: '1'
  pages:
  - '66'
  - '104'
  publisher: ACM New York, NY, USA
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  title: 'Simulation of simplicity: a technique to cope with degenerate cases in geometric
    algorithms'
  type: article
  url: https://dl.acm.org/doi/10.1145/77635.77639
  venue: ACM Transactions on Graphics (tog)
  volume: '9'
  web: null
  year: 1990
egele2022autodeuq:
  address: Montreal, QC, Canada
  articleno: null
  authors:
  - - Romain
    - Egele
  - - Romit
    - Maulik
  - - Krishnan
    - Raghavan
  - - Bethany
    - Lusch
  - - Isabelle
    - Guyon
  - - Prasanna
    - Balaprakash
  chapter: null
  descrip: AutoDEUQ paper proposing that an ensemble of neural networks can be a more
    powerful and scalable tool for uncertainty quantification than a Bayesian neural
    network. They trained AutoDEUQ using DeepHyper in multiobjective mode. I don't
    think the code is available, but an example can be found on the DeepHyper GitHub
  doi: 10.1109/icpr56361.2022.9956231
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '8'
  note: null
  number: null
  pages:
  - '1908'
  - '1914'
  publisher: IEEE
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - Gaussian process
  - regression
  - uncertainty quantification
  - UQ
  title: '{AutoDEUQ}: Automated deep ensemble with uncertainty quantification'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9956231/
  venue: 2022 26th International Conference on Pattern Recognition (ICPR)
  volume: null
  web: null
  year: 2022
egele2023parallel:
  address: null
  articleno: null
  authors:
  - - Romain
    - Egele
  - - Tyler H.
    - Chang
  - - Yixuan
    - Sun
  - - Venkatram
    - Vishwanath
  - - Prasanna
    - Balaprakash
  chapter: null
  descrip: Joint work with the DeepHyper team to bring multiobjective optimization
    to DeepHyper. Focusing on the challenges of keeping the optimizer focused on interesting
    regions of the Pareto front and how to rescale objectives so that traditional
    scalarization approaches can work
  doi: 10.48550/arXiv.2309.14936
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.LG
  series: null
  tags:
  - optimization
  - decision trees
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - autotuning
  - hyperparameter optimization
  - scalarization
  title: Parallel multi-objective hyperparameter optimization with uniform normalization
    and bounded objectives
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2023
eggensperger2021hpobench:
  address: null
  articleno: null
  authors:
  - - Katharina
    - Eggensperger
  - - Philipp
    - M{\"u}ller
  - - Neeratyoy
    - Mallik
  - - Matthias
    - Feurer
  - - Rene
    - Sass
  - - Aaron
    - Klein
  - - Noor
    - Awad
  - - Marius
    - Lindauer
  - - Frank
    - Hutter
  chapter: null
  descrip: 'HPOBench is another suite of automl (hyperparameter tuning) benchmark
    problems from the automl research group. The problems can be configured to be
    either single or multi-fidelity. They can either be run tabular (meaning the raw
    data is accessed in a table and only configurations in the dataset can be evaluated)
    or with a regression model (XGBoost). The open source software is available for
    download in Python at: github.com/automl/HPOBench'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - autotuning
  - hyperparameter optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Python
  title: '{HPOB}ench: A Collection of Reproducible Multi-Fidelity Benchmark Problems
    for {HPO}'
  type: inproceedings
  url: https://openreview.net/forum?id=1k4rJYEwda-
  venue: Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track
  volume: null
  web: null
  year: 2021
ehrgott2005multicriteria:
  address: Heidelberg, Germany
  articleno: null
  authors:
  - - Matthias
    - Ehrgott
  chapter: null
  descrip: 'A classical textbook on the fundamentals of multiobjective optimization
    theory. Topics include: basic definitions in multiobjective optimization, partial
    orderings and cones and basic theories, linear scalarization and its theory and
    drawbacks, other scalarization methods and their theory and drawbacks, standard
    algorithms for common types of multiobjective optimization problems, and sample
    applications'
  doi: 10.1007/3-540-27659-9
  edition: '2'
  editors: []
  git: null
  isbn: '3540213988'
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer Verlag
  series: Lecture Notes in Economics and Mathematical Systems Series
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Multicriteria Optimization
  type: book
  url: http://link.springer.com/10.1007/3-540-27659-9
  venue: null
  volume: null
  web: null
  year: 2005
eichfelder2009scalarizations:
  address: null
  articleno: null
  authors:
  - - Gabriele
    - Eichfelder
  chapter: null
  descrip: The Pascoletti-Serafini scalarization and its variations, this method involves
    drawing a line through the target to reach various points on the Pareto front.
    It is effective with nonconvex Pareto fronts, but it is not adaptive and not commonly
    used in modern algorithms
  doi: 10.1007/s10589-007-9155-4
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '11'
  note: null
  number: '2'
  pages:
  - '249'
  - '273'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Scalarizations for adaptively solving multi-objective optimization problems
  type: article
  url: http://link.springer.com/10.1007/s10589-007-9155-4
  venue: Computational Optimization and Applications
  volume: '44'
  web: null
  year: 2009
elgart2012note:
  address: null
  articleno: null
  authors:
  - - Alexander
    - Elgart
  - - George A
    - Hagedorn
  chapter: null
  descrip: Article analyzing the annealing time requirements to guarantee convergence
    of adiabatic quantum computing with a shrinking spectral gap
  doi: 10.1063/1.4748968
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-2488
  month: '10'
  note: null
  number: '10'
  pages:
  - 102202
  publisher: AIP
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  title: A note on the switching adiabatic theorem
  type: article
  url: https://aip.scitation.org/doi/abs/10.1063/1.4748968
  venue: Journal of Mathematical Physics
  volume: '53'
  web: null
  year: 2012
elias2020manufacturing:
  address: New Orleans, LA, USA
  articleno: null
  authors:
  - - Jakob R.
    - Elias
  - - Ryan
    - Chard
  - - Joseph A.
    - Libera
  - - Ian T.
    - Foster
  - - Santanu
    - Chaudhuri
  chapter: null
  descrip: The MDML open source software is a wrapper around Apache Kafka with protocols
    for fast data streaming and logging and dashboard generation. This framework was
    developed for usage at the material engineering research facility (MERF) at Argonne
    in order to facilitate the creation of a "smart lab" where MDML is the protocol
    for sending experiment requests to various equipment in the lab and logging results
    -- in an old (out-of-date branch) of ParMOO, this was a valid backend for launching
    simulation / experiment requests
  doi: 10.1109/WF-IoT48130.2020.9221078
  edition: null
  editors: []
  git: 'GitHub: \url{https://github.com/anl-mdml/MDML_Client}'
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - '1'
  - '2'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - software
  - open source
  - OSS
  - Python
  - Kafka
  title: 'The Manufacturing Data and Machine Learning Platform: Enabling Real-time
    Monitoring and Control of Scientific Experiments via {IoT}'
  type: article
  url: https://ieeexplore.ieee.org/document/9221078
  venue: 2020 IEEE 6th World Forum on Internet of Things (WF-IoT)
  volume: null
  web: null
  year: 2020
elsken2019neural:
  address: null
  articleno: null
  authors:
  - - Thomas
    - Elsken
  - - Jan Hendrik
    - Metzen
  - - Frank
    - Hutter
  chapter: null
  descrip: A nice survey paper on neural architecture search covering common search
    (i.e., architecture) space representations, search (i.e., optimization) strategies,
    and how to evaluate the performance of NAS methods
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - '1997'
  - '2017'
  publisher: JMLR.org
  series: null
  tags:
  - optimization
  - autotuning
  - hyperparameter optimization
  - decision trees
  - Bayesian optimization
  - evolutionary algorithms
  - EA
  - Gaussian process
  title: 'Neural architecture search: A survey'
  type: article
  url: https://www.jmlr.org/papers/volume20/18-598/18-598.pdf
  venue: The Journal of Machine Learning Research
  volume: '20'
  web: null
  year: 2019
eriksson2019scalable:
  address: null
  articleno: null
  authors:
  - - David
    - Eriksson
  - - Michael
    - Pearce
  - - Jacob
    - Gardner
  - - Ryan D
    - Turner
  - - Matthias
    - Poloczek
  chapter: null
  descrip: TURBO -- This is an open source numerical software for solving high-dimensional
    optimization problems via Bayesian optimization using BoTorch. Since Bayesian
    optimization performs poorly in high dimensions, they have resorted to applying
    a rudimentary trust region framework on top of their Bayesian optimization algorithm.
    By squeesing the trust region inward (standard practice in derivative-free optimization)
    they are able to force the Bayesian optimization algorithm to converge in a reasonable
    number of true blackbox function evaluations
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1'
  - '12'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - high dimension
  - autograd
  - backpropagation
  - Gaussian process
  title: Scalable global optimization via local bayesian optimization
  type: article
  url: https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '32'
  web: null
  year: 2019
eriksson2021latencyaware:
  address: null
  articleno: null
  authors:
  - - David
    - Eriksson
  - - Pierce I-Jen
    - Chuang
  - - Samuel
    - Daulton
  - - Peng
    - Xia
  - - Akshat
    - Shrivastava
  - - Arun
    - Babu
  - - Shicong
    - Zhao
  - - Ahmed A
    - Aly
  - - Ganesh
    - Venkatesh
  - - Maximilian
    - Balandat
  chapter: null
  descrip: Performing accuracy and latency aware neural architecture search via multiobjective
    optimization using BoTorch. While not part of the publication, the software is
    available open source on the BoTorch website
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - autotuning
  - hyperparameter optimization
  - Gaussian process
  title: Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization
  type: inproceedings
  url: https://openreview.net/forum?id=0ciyfd4SvbI
  venue: 8th ICML Workshop on Automated Machine Learning (AutoML)
  volume: null
  web: null
  year: 2021
farhan2020reinforcement:
  address: Orlando, FL, USA
  articleno: null
  authors:
  - - Mohammed
    - Farhan
  - - Brett
    - G{\"o}hre
  chapter: null
  descrip: Publication and whitepaper on Pathmind, an RL-based solver for simulation
    optimization problems. They also offer multiobjective support but only by using
    a priori scalarization provided by the user (so not real multiobjective support).
    This tool is not open source, it is a service provided by a YC startup of the
    same name. Therefore, it could be considered industry software
  doi: 10.1109/WSC48552.2020.9383916
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '3212'
  - '3223'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - reinforcement learning
  - RL
  - scalarization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - Python
  title: 'Reinforcement Learning in {AnyLogic} Simulation Models: A Guiding Example
    using {Pathmind}'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9383916
  venue: Proc. 2020 Winter Simulation Conference (WSC 2020)
  volume: null
  web: null
  year: 2020
fawzi2022discovering:
  address: null
  articleno: null
  authors:
  - - Alhussein
    - Fawzi
  - - Matej
    - Balog
  - - Aja
    - Huang
  - - Thomas
    - Hubert
  - - Bernardino
    - Romera-Paredes
  - - Mohammadamin
    - Barekatain
  - - Alexander
    - Novikov
  - - Francisco J
    - R. Ruiz
  - - Julian
    - Schrittwieser
  - - Grzegorz
    - Swirszcz
  - - ''
    - others
  chapter: null
  descrip: The culmination of a line of work from Google DeepMind on discovering better
    matrix factorization algorithms using reinforcement learning. They defined a tensor
    game for trying to solve matrix multiplication at a fixed size in fewer moves,
    then trained a RL agent to play the game using monte carlo tree search and training
    a neural network to predict a manageable subset of states and expected state outcomes
    since the number of states was too numerous for true MCTS. They were able to match
    the performance of Strassen's algorithm on 5x5 matrices (this is how recursive
    block-based recursive multiplication is done) and exceed performance on other
    sizes and rings (such as modular arithmetic rings)
  doi: 10.1038/s41586-022-05172-4
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0028-0836
  month: '10'
  note: null
  number: '7930'
  pages:
  - '47'
  - '53'
  publisher: Nature Publishing Group UK London
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - neural networks
  - scientific machine learning
  - SciML
  - computational linear algebra
  title: Discovering faster matrix multiplication algorithms with reinforcement learning
  type: article
  url: https://www.nature.com/articles/s41586-022-05172-4
  venue: Nature
  volume: '610'
  web: null
  year: 2022
feldman2018score:
  address: null
  articleno: '7'
  authors:
  - - Guy
    - Feldman
  - - Susan R.
    - Hunter
  chapter: null
  descrip: A biobjective ranking and selection algorithm from Hunter's NSF Career
    award
  doi: 10.1145/3158666
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1049-3301
  month: '1'
  note: null
  number: '1'
  pages:
  - 28
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - stochastic optimization
  title: '{SCORE} Allocations for Bi-objective Ranking and Selection'
  type: article
  url: https://dl.acm.org/doi/10.1145/3158666
  venue: ACM Transactions on Modeling Computer and Simulation
  volume: '28'
  web: null
  year: 2018
feliot2016bayesian:
  address: null
  articleno: null
  authors:
  - - Paul
    - Feliot
  - - Julien
    - Bect
  - - Emmanuel
    - Vazquez
  chapter: null
  descrip: A Bayesian optimization algorithm for solving constrained optimization
    problems that are both single and multiobjective -- the authors propose expected
    hypervolume improvement (EHVI) which merges expected improvement acquisition from
    Bayesian optimization with the hypervolume indicator for multiobjective optimization
  doi: 10.1007/s10898-016-0427-3
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '1'
  note: null
  number: 1-2
  pages:
  - '97'
  - '133'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - constrained optimization
  - surrogate modeling
  - scalarization
  title: A {B}ayesian approach to constrained single- and multi-objective optimization
  type: article
  url: http://link.springer.com/10.1007/s10898-016-0427-3
  venue: Journal of Global Optimization
  volume: '67'
  web: null
  year: 2016
fiduccia1982lineartime:
  address: Las Vegas, NV, USA
  articleno: null
  authors:
  - - C. M.
    - Fiduccia
  - - R. M.
    - Mattheyses
  chapter: null
  descrip: The Fiduccia-Matheyses graph partitioning algorithm, which calculates the
    minimum cut in a hypergraph via the heuristic of generating a random initial cut
    then moving nodes across the cut (greedily) and remembering the best observed
    cut until all nodes have been moved. The magic of this algorithm is the linear
    complexity due to a heap-like data structure that produces the next node to move
    in constant time in each iteration. However, this trick only works for integer-valued
    cost functions (such as min-cut)
  doi: 10.1109/dac.1982.1585498
  edition: null
  editors: []
  git: null
  isbn: 0897910206
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 7
  publisher: IEEE Press
  series: DAC '82
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - mixed-variable optimization
  - partitioning
  title: A linear-time heuristic for improving network partitions
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/1585498/
  venue: Proceedings of the 19th Design Automation Conference
  volume: null
  web: null
  year: 1982
fletcher1993resolving:
  address: null
  articleno: null
  authors:
  - - Roger
    - Fletcher
  chapter: null
  descrip: Fletcher's paper on how to handle degeneracy in active set methods for
    solving quadratic programming problems. This technique would become the basis
    for the BQPD software package, which is a recently open source (post mortem) numerical
    software for solving (degenerate) quadratic programs via active set methods
  doi: 10.1007/BF02023102
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0254-5330
  month: '9'
  note: null
  number: '2'
  pages:
  - '307'
  - '334'
  publisher: Springer
  series: null
  tags:
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - Fortran
  title: Resolving degeneracy in quadratic programming
  type: article
  url: http://link.springer.com/10.1007/BF02023102
  venue: Annals of Operations Research
  volume: '46'
  web: null
  year: 1993
fletcher2000stable:
  address: null
  articleno: null
  authors:
  - - Roger
    - Fletcher
  chapter: null
  descrip: Fletcher's paper on an efficient Hessian update method which can be used
    in an active set method for solving quadratic programming optimization problems
  doi: 10.1007/s101070050113
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0025-5610
  month: '4'
  note: null
  number: '2'
  pages:
  - '251'
  - '264'
  publisher: Springer
  series: null
  tags:
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  title: Stable reduced Hessian updates for indefinite quadratic programming
  type: article
  url: http://link.springer.com/10.1007/s101070050113
  venue: Mathematical programming
  volume: '87'
  web: null
  year: 2000
fortin2012deap:
  address: null
  articleno: null
  authors:
  - - F\'elix-Antoine
    - Fortin
  - - Fran\ccois-Michel
    - De~Rainville
  - - Marc-Andr\'e
    - Gardner
  - - Marc
    - Parizeau
  - - Christian
    - Gagn\'e
  chapter: null
  descrip: The DEAP framework is a Python framework for easily implementing and deploying
    parallel and distributed evolutionary algorithms. Fairly high quality open source
    software. This is widely used by optimization practitioners, e.g., engineers and
    scientists that read an evolutionary algorithm paper and want to try it out on
    their problem
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - '2171'
  - '2175'
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - evolutionary algorithms
  - EA
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  - MPI
  title: '{DEAP}: Evolutionary Algorithms Made Easy'
  type: article
  url: https://www.jmlr.org/papers/v13/fortin12a.html
  venue: Journal of Machine Learning Research
  volume: '13'
  web: null
  year: 2012
fortune1987sweepline:
  address: null
  articleno: null
  authors:
  - - Steven
    - Fortune
  chapter: null
  descrip: The famous sweepline algorithm for computing Delaunay triangulations and
    Voronoi tesselations in 2D. Works by computing a "beachline" of parabolas whose
    focii are the points on the current boundary of the Delaunay triangulation. Any
    simplices in the triangulation beyond the beachline are "finalized" and those
    vertices need not be considered when adding new points into the triangulation.
    Results in a O(n log n) sweepline algorithm
  doi: 10.1007/bf01840357
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0178-4617
  month: '11'
  note: null
  number: '1'
  pages:
  - '153'
  - '174'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  title: A sweepline algorithm for {V}oronoi diagrams
  type: article
  url: http://link.springer.com/10.1007/BF01840357
  venue: Algorithmica
  volume: '2'
  web: null
  year: 1987
forum2023mpi:
  address: null
  articleno: null
  authors:
  - - Message Passing Interface
    - Forum
  chapter: null
  descrip: Official MPI 4.1 standard document for distributed computing via a message
    passing interface in C and Fortran
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: nov
  note: null
  number: version 4.1
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - software
  - Fortran
  - C
  - MPI
  title: '{MPI}: A Message-Passing Interface Standard'
  type: techreport
  url: https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf
  venue: null
  volume: null
  web: null
  year: 2023
fowkes2022pycutest:
  address: null
  articleno: null
  authors:
  - - Jaroslav
    - Fowkes
  - - Lindon
    - Roberts
  - - "\xC1rp\xE1d"
    - "B\u0171rmen"
  chapter: null
  descrip: 'PyCUTEst is a very nice, easily installed open source Python wrapper for
    the CUTEst test suite for benchmarking blackbox optimization problems. The CUTEst
    software is the standard suite of constrained and unconstrained test problems
    for derivative-free / blackbox / simulation optimization problems. CUTEst is available
    open source and written in Fortran, but now offer C, Python, Matlab, and Julia
    interfaces (plus the default command line interface). Available for download at:
    github.com/jfowkes/pycutest'
  doi: 10.21105/joss.04377
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2475-9066
  month: '10'
  note: null
  number: '78'
  pages:
  - 4377
  publisher: The Open Journal
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Fortran
  - Python
  title: 'PyCUTEst: an open source Python package of optimization test problems'
  type: article
  url: https://joss.theoj.org/papers/10.21105/joss.04377
  venue: Journal of Open Source Software
  volume: '7'
  web: null
  year: 2022
frazier2018tutorial:
  address: null
  articleno: null
  authors:
  - - Peter I.
    - Frazier
  chapter: null
  descrip: A tutorial by Peter Frazier on Bayesian optimization -- not much that isn't
    in the textbooks, except a reference where someone acknowledges the perspective
    that traditional Bayesian optimization (in pursuit of true global convergence
    with no localization strategy) doesn't scale well past 20 dimensions. In my experience,
    even this would be optimistic. I would say that it usually doesn't scale well
    past 5-10 dimensions depending on the computational budget
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - high-dimension
  - surrogate modeling
  - Gaussian process
  title: A Tutorial on {B}ayesian Optimization
  type: article
  url: http://arxiv.org/abs/1807.02811
  venue: arXiv preprint arXiv:1807.02811
  volume: null
  web: null
  year: 2018
fukuda2004possible:
  address: null
  articleno: null
  authors:
  - - Komei
    - Fukuda
  chapter: null
  descrip: 'A famous blog post by K. Fukuda on how to efficiently calculate Delaunay
    simplices (or equivalently, Voronoi cell neighbors) efficiently in high dimensions
    via geometric programming. This method was never published anywhere but appears
    to be an often forgotten technique. This only works for points in general position
    (i.e., non degenerate data sets) in perfect precision, but it is somewhat similar
    to DelaunaySparse in that both could be considered simplex methods for solving
    either the primal or dual form of Fukuda''s problem -- I have personally implemented
    the dual form in addition to DelaunaySparse and it is publicly available on GitHub:
    https://github.com/thchang/DualSimplex However, I could not make this form robust
    to degeneracy'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: August
  note: Retrieved [November 16, 2022]
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithm
  - projection
  - convex hull
  - high dimension
  title: Is it possible to compute only the adjacencies of {V}oronoi cells in the
    {V}oronoi diagram efficiently?
  type: inproceedings
  url: http://www.cs.mcgill.ca/~fukuda/soft/polyfaq/polyfaq.html
  venue: Polyhedral computation {FAQ} (blog)
  volume: null
  web: null
  year: 2004
fukushima1975cognitron:
  address: null
  articleno: null
  authors:
  - - Kunihiko
    - Fukushima
  chapter: null
  descrip: The first paper where the ReLU activation function was used as the activation
    function in neural network models. This would later become the standard for many
    years (and still is for regressor models)
  doi: 10.1007/bf00342633
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0340-1200
  month: null
  note: null
  number: '3'
  pages:
  - '121'
  - '136'
  publisher: Springer
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: 'Cognitron: A self-organizing multilayered neural network'
  type: article
  url: http://link.springer.com/10.1007/BF00342633
  venue: Biological cybernetics
  volume: '20'
  web: null
  year: 1975
gamma1995design:
  address: Reading, MA, USA
  articleno: null
  authors:
  - - Erich
    - Gamma
  - - Richard
    - Helm
  - - Ralph
    - Johnson
  - - John
    - Vlissides
  chapter: null
  descrip: Landmark textbook on standard design patterns in software engineering,
    such as builders, factories, observers, etc. Nicknamed the "Gang of Four" book
    on design patters
  doi: null
  edition: null
  editors: []
  git: null
  isbn: 0-201-63361-2
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Addison-Wesley
  series: null
  tags:
  - software
  title: 'Design patterns: elements of reusable object-oriented software'
  type: book
  url: https://en.wikipedia.org/wiki/Design_Patterns
  venue: null
  volume: null
  web: null
  year: 1995
gao2023scaling:
  address: null
  articleno: null
  authors:
  - - Leo
    - Gao
  - - John
    - Schulman
  - - Jacob
    - Hilton
  chapter: null
  descrip: Paper showing that using the process reward model (PRM) during fine-tuning
    can easily lead models to over-optimize for quick rewards, leading to worse overall
    ability to reach the correct solution. This doesn't mean that process can't work,
    we just need a mechanism to encourage better long-term planning
  doi: null
  edition: null
  editors:
  - - Andreas
    - Krause
  - - Emma
    - Brunskill
  - - Kyunghyun
    - Cho
  - - Barbara
    - Engelhardt
  - - Sivan
    - Sabato
  - - Jonathan
    - Scarlett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '10835'
  - '10866'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - overfitting
  title: Scaling Laws for Reward Model Overoptimization
  type: inproceedings
  url: https://proceedings.mlr.press/v202/gao23h.html
  venue: Proceedings of the 40th International Conference on Machine Learning
  volume: '202'
  web: null
  year: 2023
garg2023sfsfd:
  address: Orlando, FL, USA
  articleno: null
  authors:
  - - Manisha
    - Garg
  - - Tyler H.
    - Chang
  - - Krishnan
    - Raghavan
  chapter: null
  descrip: Manisha's conference paper on SF-SFD -- the theory is far from ready, but
    the problem of concentration of measure in high-dimensions making robust sampling
    difficult and the ability to do something better than just random or latin hypercube
    sampling are clearly demonstrated
  doi: 10.1109/WSC60868.2023.10408245
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '3636'
  - '3646'
  publisher: IEEE
  series: null
  tags:
  - design of experiments
  - DoE
  - Latin hypercube sampling
  - LHS
  - measure theory
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  title: '{SF-SFD}: {S}tochastic optimization of {F}ourier coefficients for space-filling
    designs'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/10408245/
  venue: Proc. 2023 Winter Simulation Conference (WSC 2023)
  volume: null
  web: null
  year: 2023
garnett2023bayesian:
  address: null
  articleno: null
  authors:
  - - Roman
    - Garnett
  chapter: null
  descrip: A community reviewed textbook on Bayesian optimization theory and implementation.
    Very thorough description of Gaussian process and Bayesian optimization fundamentals
    and theory, common techniques and acquisition functions, and implementation details,
    drawbacks, and real-world challenges
  doi: null
  edition: null
  editors: []
  git: null
  isbn: 978-1108425780
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Cambridge University Press
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - Gaussian process
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - regression
  title: Bayesian Optimization
  type: book
  url: https://bayesoptbook.com
  venue: null
  volume: null
  web: null
  year: 2023
gavin2019levenbergmarquardt:
  address: null
  articleno: null
  authors:
  - - HP.
    - Gavin
  chapter: null
  descrip: Commonly cited lecture notes on using the Levenberg-Marquardt algorithm
    to solve least-squares curve-fitting (optimization) problems via a Gauss-Newton
    esque method
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Duke University, Department of Civil and Environmental Engineering
  series: null
  tags:
  - optimization
  - convex optimization
  title: The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting
    problems
  type: techreport
  url: https://people.duke.edu/~hpgavin/lm.pdf
  venue: null
  volume: null
  web: null
  year: 2019
geman1992neural:
  address: Cambridge, MA, USA
  articleno: null
  authors:
  - - Stuart
    - Geman
  - - Elie
    - Bienenstock
  - - Ren\'e
    - Doursat
  chapter: null
  descrip: The original paper on the bias-variance tradeoff curve. Now widely debunked
    (in my opinion) the theory that machine learning and neural networks had to tradeoff
    between training accuracy and overfitting (which would lead to the idea of so-called
    "generalization error") and had to be combatted by limiting model size or regularization
    rules ruled AI theory for many, many years. One key insight in this paper that
    was perhaps ahead of its time, is that the important part of machine learning
    is the representation learning, in which sense training neural networks with backpropagation
    for regression problems is somewhat not special
  doi: 10.1162/neco.1992.4.1.1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-7667
  month: jan
  note: null
  number: '1'
  pages:
  - '1'
  - '58'
  publisher: MIT Press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - representation learning
  - regularization
  - overfitting
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: Neural networks and the bias/variance dilemma
  type: article
  url: https://direct.mit.edu/neco/article/4/1/1-58/5624
  venue: Neural Computation
  volume: '4'
  web: null
  year: 1992
germann2021codesign:
  address: null
  articleno: null
  authors:
  - - Timothy C.
    - Germann
  chapter: null
  descrip: An article on the benefits of co design of algorithms and software and
    hardware in the context of the DOE's exascale computing project (ECP)
  doi: 10.1177/10943420211059380
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1094-3420
  month: '11'
  note: null
  number: '6'
  pages:
  - '503'
  - '507'
  publisher: 'SAGE Publications Sage UK: London, England'
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - software
  - CUDA
  - OpenMP
  - MPI
  - parallel programming
  title: Co-design in the {Exascale Computing Project}
  type: article
  url: https://journals.sagepub.com/doi/10.1177/10943420211059380
  venue: The International Journal of High Performance Computing Applications
  volume: '35'
  web: null
  year: 2021
gillette2022datadriven:
  address: null
  articleno: null
  authors:
  - - Andrew
    - Gillette
  - - Eugene
    - Kur
  chapter: null
  descrip: Introducing a Delaunay density diagnostic -- using the Delaunay interpolant
    to calculate whether they have enough data to resolve the geometry of a problem
    for interpolation in the context of SciML. Essentially, they use DelaunaySparse
    to make predictions and slowly add data. Once they have enough data, the predictions
    begin converging at a linear rate. Then they have enough data to use whatever
    method they want to solve the problem
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv math.NA
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Data-driven geometric scale detection via Delaunay interpolation
  type: techreport
  url: https://arxiv.org/html/2203.05685v2
  venue: null
  volume: null
  web: null
  year: 2022
gillette2024algorithm:
  address: null
  articleno: '24'
  authors:
  - - Andrew
    - Gillette
  - - Eugene
    - Kur
  chapter: null
  descrip: Open source numerical software for the Delaunay density diagnostic -- using
    the Delaunay interpolant to calculate whether they have enough data to resolve
    the geometry of a problem for interpolation in the context of SciML. Essentially,
    they use DelaunaySparse to make predictions and slowly add data. Once they have
    enough data, the predictions begin converging at a linear rate. Then they have
    enough data to use whatever method they want to solve the problem
  doi: 10.1145/3700134
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '12'
  note: null
  number: '4'
  pages:
  - 21
  publisher: Association for Computing Machinery
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - approximation theory
  - uncertainty quantification
  - UQ
  - software
  - Python
  title: 'Algorithm 1049: The Delaunay Density Diagnostic'
  type: article
  url: https://doi.org/10.1145/3700134
  venue: ACM Trans. Math. Softw.
  volume: '50'
  web: null
  year: 2024
goh2017why:
  address: null
  articleno: null
  authors:
  - - Gabriel
    - Goh
  chapter: null
  descrip: Online paper with interactive visualizations explaining what Nesterov's
    momentum is and how it works intuitively by smoothing out optimization sample
    paths and preventing oscillations in the optimizer that occur do to poor problem
    conditioning. Then, they show how the problem conditioning appears as an often
    ignored constant in the convergence rate of gradient descent. All this is to show
    intuitively and mathematically that gradient descent with Nesterov's momentum
    will convergence faster in practice for ill-conditioned problems
  doi: 10.23915/distill.00006
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2476-0757
  month: '4'
  note: null
  number: '4'
  pages: null
  publisher: Distill Working Group
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - optimization
  - stochastic optimization
  - convex optimization
  - scientific machine learning
  - SciML
  title: Why Momentum Really Works
  type: article
  url: http://distill.pub/2017/momentum
  venue: Distill
  volume: '2'
  web: null
  year: 2017
golovin2017google:
  address: Halifax, NS, Canada
  articleno: null
  authors:
  - - Daniel
    - Golovin
  - - Benjamin
    - Solnik
  - - Subhodeep
    - Moitra
  - - Greg
    - Kochanski
  - - John
    - Karro
  - - D.
    - Sculley
  chapter: null
  descrip: Google's OSS Vizier service is a (now open source) blackbox / derivative-free
    optimization numerical software package and service. As far as I can tell, the
    package is primarily used for solving system optimization and A/B testing type
    problems
  doi: 10.1145/3097983.3098043
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '8'
  note: null
  number: null
  pages:
  - '1487'
  - '1495'
  publisher: ACM
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - Bayesian optimization
  - Gaussian process
  - software
  - open source
  - OSS
  - Python
  title: '{Google Vizier}: A Service for Black-Box Optimization'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3097983.3098043
  venue: Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and
    Data Mining (KDD '17)
  volume: null
  web: null
  year: 2017
golub2013matrix:
  address: null
  articleno: null
  authors:
  - - Gene H.
    - Golub
  - - Charles F.
    - Van Loan
  chapter: null
  descrip: Classical textbook that serves as the "bible" of matrix computations and
    computational linear algebra -- contains all the standard factorizations, the
    common algorithms for computing them, and their sensitiviy analyses, pivoting,
    some basic approximation theory, and the basics of iterative methods
  doi: 10.56021/9781421407944
  edition: 4th
  editors: []
  git: null
  isbn: 978-1421407944
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Johns Hopkins University Press
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  title: Matrix computations
  type: book
  url: https://www.press.jhu.edu/books/title/10678/matrix-computations
  venue: null
  volume: null
  web: null
  year: 2013
gorban2017stochastic:
  address: null
  articleno: null
  authors:
  - - Alexander N
    - Gorban
  - - Ivan Yu
    - Tyukin
  chapter: null
  descrip: Theorems on the curse of dimensionality when it comes to drawing data points
    in high-dimensional spaces. The main theorem implies that the convex hull of N
    points in D dimensions has volume ~0 for D sufficiently large -- this occurs because
    of a concentration of measure type result
  doi: 10.1016/j.neunet.2017.07.014
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0893-6080
  month: '10'
  note: null
  number: null
  pages:
  - '255'
  - '259'
  publisher: Elsevier
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - computational geometry
  - convex hull
  - high dimension
  - design of experiments
  - DoE
  - measure theory
  - scientific machine learning
  - SciML
  - approximation theory
  title: Stochastic separation theorems
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0893608017301776
  venue: Neural Networks
  volume: '94'
  web: null
  year: 2017
gould2015cutest:
  address: null
  articleno: null
  authors:
  - - Nicholas I. M.
    - Gould
  - - Dominique
    - Orban
  - - Philippe L.
    - Toint
  chapter: null
  descrip: 'The CUTEst software is the standard suite of constrained and unconstrained
    test problems for derivative-free / blackbox / simulation optimization problems.
    CUTEst is available open source and written in Fortran, but now offer C, Python,
    Matlab, and Julia interfaces (plus the default command line interface). Available
    for download at: github.com/ralna/CUTEst'
  doi: 10.1007/s10589-014-9687-3
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '4'
  note: null
  number: '3'
  pages:
  - '545'
  - '557'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Fortran
  - Python
  - Matlab
  - Julia
  title: '{CUTEst}: a Constrained and Unconstrained Testing Environment with safe
    threads for mathematical optimization'
  type: article
  url: https://doi.org/10.1007/s10589-014-9687-3
  venue: Computational Optimization and Applications
  volume: '60'
  web: null
  year: 2015
gramacy2012cases:
  address: null
  articleno: null
  authors:
  - - Robert B.
    - Gramacy
  - - Herbert K. H.
    - Lee
  chapter: null
  descrip: An interesting analysis by Bobby Gramacy on how adding a "nugget" (i.e.,
    a small perturbation \varepsilon I to the diagonal of a matrix results in vastly
    improved numerical stability and therefore performance when modeling data science
    and other approximation problems via Gaussian processes
  doi: 10.1007/s11222-010-9224-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0960-3174
  month: '5'
  note: null
  number: '3'
  pages:
  - '713'
  - '722'
  publisher: Springer
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - computational linear algebra
  - RBFs
  - Gaussian process
  - high dimension
  - interpolation
  - regression
  - approximation theory
  title: Cases for the nugget in modeling computer experiments
  type: article
  url: http://link.springer.com/10.1007/s11222-010-9224-x
  venue: Statistics and Computing
  volume: '22'
  web: null
  year: 2012
grattafiori2024llama:
  address: null
  articleno: null
  authors:
  - - Aaron
    - Grattafiori
  - - Abhimanyu
    - Dubey
  - - Abhinav
    - Jauhri
  - - Abhinav
    - Pandey
  - - Abhishek
    - Kadian
  - - Ahmad
    - Al-Dahle
  - - Aiesha
    - Letman
  - - Akhil
    - Mathur
  - - Alan
    - Schelten
  - - Alex
    - Vaughan
  - - ''
    - others
  chapter: null
  descrip: 'The LLaMA 3 model publication. In addition to the information from LLaMA
    1 tech report, they add information on training and pipeline parallelism. The
    latest max model size is a 405B parameter (dense) model, which competes with GPT-4
    from OpenAI. The model architectures and instructions to download the weights
    (form HuggingFace) are obtained from github.com/meta-llama/llama TODO: read this
    report carefully, especially the training details, I have only skimmed'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2407.21783
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - open source
  - LLMs
  - transformers
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  title: The {LLaMA} 3 herd of models
  type: techreport
  url: https://arxiv.org/abs/2407.21783
  venue: null
  volume: null
  web: null
  year: 2024
graves2014generating:
  address: null
  articleno: null
  authors:
  - - Alex
    - Graves
  chapter: null
  descrip: First published work using Geoff Hinton's unpublished optimization algorithm
    RMSProp (root mean squared propogation, an adjustment to AdaGrad using an adaptive
    learning rate in each dimension). The author uses RMSProp to train a recurrent
    neural network (RNN) with long short-term memory (LSTM) in order to generate handwritten
    digits
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.NE preprint
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - optimization
  - scientific machine learning
  - SciML
  - convex optimization
  - RNNs
  title: Generating Sequences With Recurrent Neural Networks
  type: techreport
  url: https://arxiv.org/abs/1308.0850
  venue: null
  volume: null
  web: null
  year: 2014
gray2019openmdao:
  address: null
  articleno: null
  authors:
  - - Justin S.
    - Gray
  - - John T.
    - Hwang
  - - Joaquim R.R.A.
    - Martins
  - - Kenneth T.
    - Moore
  - - Bret A.
    - Naylor
  chapter: null
  descrip: The OpenMDAO open source numerical software library for modeling and solving
    multidisciplinary engineering design optimization problems. Combines surrogate
    modeling, gradient based optimization, parallel computing frameworks, and derivative-free
    optimization techniques in one package so in order to solve large mixed-variable
    blackbox optimization problems. Developed by NASA Glenn
  doi: 10.1007/s00158-019-02211-z
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1615-147X
  month: '4'
  note: null
  number: '4'
  pages:
  - '1075'
  - '1104'
  publisher: Springer
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  - software
  - open source
  - OSS
  - Python
  title: '{OpenMDAO}: An open-source framework for multidisciplinary design, analysis,
    and optimization'
  type: article
  url: http://link.springer.com/10.1007/s00158-019-02211-z
  venue: Structural and Multidisciplinary Optimization
  volume: '59'
  web: null
  year: 2019
guhring2020error:
  address: null
  articleno: null
  authors:
  - - Ingo
    - G{\"u}hring
  - - Gitta
    - Kutyniok
  - - Philipp
    - Petersen
  chapter: null
  descrip: An error analysis of ReLU multilayer perceptrons (MLPs). Provides lower
    bounds on approximation error for feed-forward neural networks with ReLU activation
    functions. A good step in SciML, but we typically want upper bounds not lower
    bounds in practice
  doi: 10.1142/S0219530519410021
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '05'
  pages:
  - '803'
  - '859'
  publisher: World Scientific
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - regression
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Error bounds for approximations with deep ReLU neural networks in $W^{s,p}$
    norms
  type: article
  url: null
  venue: Analysis and Applications
  volume: '18'
  web: null
  year: 2020
guo2025deepseekr1:
  address: null
  articleno: null
  authors:
  - - Daya
    - Guo
  - - Dejian
    - Yang
  - - Haowei
    - Zhang
  - - Junxiao
    - Song
  - - Ruoyu
    - Zhang
  - - Runxin
    - Xu
  - - Qihao
    - Zhu
  - - Shirong
    - Ma
  - - Peiyi
    - Wang
  - - Xiao
    - Bi
  - - ''
    - others
  chapter: null
  descrip: 'DeepSeek-R1 tech report: DeepSeek-R1 is DeepSeek''s first state-of-the-art
    reasoning model. DeepSeek-R1-Zero was fine-tuned from DeepSeek-V3, by allowing
    chain-of-thought reasoning and using training objectives without cold start consisting
    of mostly checkable rules and formatting. I.e., it is possible to penalize any
    computer code that doesn''t compile, any images that spill out of a bounding box,
    and any thought chains that are not enclosed within <think></think> blocks. This
    model has good performance but mixes languages and doesn''t always generate human
    readable output. Next, they trained DeepSeek-R1 using some carefully crafted cold-start
    examples to enforce reasoning and logic and output language consistency, using
    mostly coding, math, logic, and science problem sets and language mixing penalties.
    These penalties are critically applied inside as well as without the CoT blocks.
    The authors trained both R1 and R1-Zero via reinforcement learning with group
    relative policy optimization (GRPO). They claim the model naturally learned to
    use long chains of thoughts and internal discussions unprompted, but that''s debatable
    since this kind of discussion is actually common in internet data. The resulting
    model performs well on standard AI model benchmarks. They also show that information
    from their large MoE models can be distilled into smaller dense models with better
    results than just fine-tuning those dense models on the same data'
  doi: 10.48550/arXiv.2501.12948
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2501.12948
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - LLMs
  - open source
  title: '{DeepSeek-R1}: Incentivizing reasoning capability in {LLM}s via reinforcement
    learning'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2025
hadka2015platypus:
  address: null
  articleno: null
  authors:
  - - David
    - Hadka
  chapter: null
  descrip: 'Platypus: an open source numerical software package for performing multiobjective
    optimization in Python and comparing results'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: Version 1.0.4
  pages: null
  publisher: GitHub
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - software
  - open source
  - OSS
  - Python
  title: Platypus -- multiobjective optimization in {P}ython
  type: techreport
  url: https://platypus.readthedocs.io/en/latest
  venue: null
  volume: null
  web: null
  year: 2015
hanson1982algorithm:
  address: New York, NY, USA
  articleno: null
  authors:
  - - Richard J.
    - Hanson
  - - Karen H.
    - Haskell
  chapter: null
  descrip: Hanson's Fortran numerical software for solving equality constrained nonnegative
    least-squares (NNLS) problems via an iterative weighted least squares (WNNLS)
    solver. This is the default constrained least-squares optimization problem solver
    in the Fortran library SLATEC from Sandia
  doi: 10.1145/356004.356010
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '9'
  note: null
  number: '3'
  pages:
  - '323'
  - '333'
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - projection
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - constrained optimization
  - quadratic programming
  - QP
  - convex optimization
  - software
  - Fortran
  title: 'Algorithm 587: Two Algorithms for the Linearly Constrained Least Squares
    Problem'
  type: article
  url: https://dl.acm.org/doi/10.1145/356004.356010
  venue: ACM Trans. Math. Softw.
  volume: '8'
  web: null
  year: 1982
harris2020array:
  address: null
  articleno: null
  authors:
  - - Charles R.
    - Harris
  - - K. Jarrod
    - Millman
  - - St\'efan J. van der
    - Walt
  - - Ralf
    - Gommers
  - - Pauli
    - Virtanen
  - - David
    - Cournapeau
  - - Eric
    - Wieser
  - - Julian
    - Taylor
  - - Sebastian
    - Berg
  - - Nathaniel J.
    - Smith
  - - Robert
    - Kern
  - - Matti
    - Picus
  - - Stephan
    - Hoyer
  - - Marten H. van
    - Kerkwijk
  - - Matthew
    - Brett
  - - Allan
    - Haldane
  - - Jaime Fern\'andez del
    - R\'io
  - - Mark
    - Wiebe
  - - Pearu
    - Peterson
  - - Pierre
    - G\'erard-Marchant
  - - Kevin
    - Sheppard
  - - Tyler
    - Reddy
  - - Warren
    - Weckesser
  - - Hameer
    - Abbasi
  - - Christoph
    - Gohlke
  - - Travis E.
    - Oliphant
  chapter: null
  descrip: 'The official publication of the open source numerical software numpy:
    the standard for basic multivariable computations, vector operations, and simple
    linear algebra in Python'
  doi: 10.1038/s41586-020-2649-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0028-0836
  month: '9'
  note: null
  number: '7825'
  pages:
  - '357'
  - '362'
  publisher: Springer Science and Business Media {LLC}
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - Python
  title: Array programming with {NumPy}
  type: article
  url: https://www.nature.com/articles/s41586-020-2649-2
  venue: Nature
  volume: '585'
  web: null
  year: 2020
hart2017pyomo:
  address: Cham, Switzerland
  articleno: null
  authors:
  - - William E.
    - Hart
  - - Carl D.
    - Laird
  - - Jean-Paul
    - Watson
  - - David L.
    - Woodruff
  - - Gabriel A.
    - Hackebeil
  - - Bethany L.
    - Nicholson
  - - John D.
    - Siirola
  chapter: null
  descrip: 'The official textbook on the Pyomo modeling language: an open source optimization
    modeling language and scientific software developed at Sandia by Bill Hart et
    al. Pyomo is a standard for solving large-scale mathematical programming (linear
    and nonlinear optimization) problems in Python'
  doi: 10.1007/978-3-319-58821-6
  edition: '2'
  editors: []
  git: null
  isbn: '9783319588193'
  issn: 1931-6828
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer Cham
  series: Springer Optimization and Its Applications
  tags:
  - optimization
  - linear programming
  - quadratic programming
  - LP
  - QP
  - constrained optimization
  - convex optimization
  - mixed-variable optimization
  - software
  - open source
  - OSS
  - Python
  title: Pyomo -- optimization modeling in {P}ython
  type: book
  url: http://link.springer.com/10.1007/978-3-319-58821-6
  venue: Springer Optimization and Its Applications
  volume: null
  web: null
  year: 2017
hase2018chimera:
  address: null
  articleno: null
  authors:
  - - Florian
    - H{\"a}se
  - - Lo{\"\i}c M
    - Roch
  - - Al{\'a}n
    - Aspuru-Guzik
  chapter: null
  descrip: 'Chimera: a scientific software package for steering self-driving labs
    via multiobjective optimization -- the software is similar to what we did with
    ParMOO + MDML in the MERF at Argonne. They focus on applications in robot calibration
    and molecular system design. The multiobjective component helps them to explore
    the solution space with experimentation'
  doi: 10.1039/C8SC02239A
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2041-6520
  month: null
  note: null
  number: '39'
  pages:
  - '7642'
  - '7655'
  publisher: Royal Society of Chemistry
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - mixed-variable optimization
  - software
  - open source
  - OSS
  - Python
  title: 'Chimera: enabling hierarchy based multi-objective optimization for self-driving
    laboratories'
  type: article
  url: https://xlink.rsc.org/?DOI=C8SC02239A
  venue: Chemical science
  volume: '9'
  web: null
  year: 2018
hayes2022practical:
  address: null
  articleno: null
  authors:
  - - Conor F
    - Hayes
  - - Roxana
    - R{\u{a}}dulescu
  - - Eugenio
    - Bargiacchi
  - - Johan
    - K{\"a}llstr{\"o}m
  - - Matthew
    - Macfarlane
  - - Mathieu
    - Reymond
  - - Timothy
    - Verstraeten
  - - Luisa M
    - Zintgraf
  - - Richard
    - Dazeley
  - - Fredrik
    - Heintz
  - - ''
    - others
  chapter: null
  descrip: A survey paper on multiobjective reinforcement learning -- RL is basically
    optimization with the addition of a dynamically changing state variable, so this
    is sort of relevant to multiobjecte optimization research
  doi: 10.1007/s10458-022-09552-y
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1387-2532
  month: '4'
  note: null
  number: '1'
  pages:
  - '1'
  - '59'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - reinforcement learning
  - RL
  - multiobjective optimization
  title: A practical guide to multi-objective reinforcement learning and planning
  type: article
  url: https://link.springer.com/10.1007/s10458-022-09552-y
  venue: Autonomous Agents and Multi-Agent Systems
  volume: '36'
  web: null
  year: 2022
he2009algorithm:
  address: null
  articleno: null
  authors:
  - - Jian
    - He
  - - Layne T.
    - Watson
  - - Masha
    - Sosonkina
  chapter: null
  descrip: 'VTDIRECT95 reference: a high-performance parallel Fortran implementation
    of the famous single-objective blackbox (direct search) optimization algorithm
    DIRECT. The numerical software is now open source (maintained by me) on Dr. Watson''s
    GitHub page.'
  doi: 10.1145/1527286.1527291
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '7'
  note: null
  number: '3'
  pages:
  - 17
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  - software
  - open source
  - OSS
  - Fortran
  - parallel programming
  - MPI
  title: 'Algorithm 897: {VTDIRECT95}: Serial and Parallel Codes for the Global Optimization
    Algorithm {DIRECT}'
  type: article
  url: https://dl.acm.org/doi/10.1145/1527286.1527291
  venue: ACM Transactions on Mathematical Software
  volume: '36'
  web: null
  year: 2009
he2009performance:
  address: null
  articleno: null
  authors:
  - - Jian
    - He
  - - Alex
    - Verstak
  - - Layne T.
    - Watson
  - - Masha
    - Sosonkina
  chapter: null
  descrip: 'Studying the parallel performance of VTDIRECT95 at a massive scale: in
    summary VTDIRECT95 scales very well after the initial "warmup" period since there
    are not many boxes in the first couple iterations'
  doi: 10.1177/1094342008098463
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1094-3420
  month: '2'
  note: null
  number: '1'
  pages:
  - '14'
  - '28'
  publisher: SAGE Publications
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  title: Performance modeling and analysis of a massively parallel {DIRECT} -- part
    1
  type: article
  url: https://journals.sagepub.com/doi/10.1177/1094342008098463
  venue: The International Journal of High Performance Computing Applications
  volume: '23'
  web: null
  year: 2009
he2016deep:
  address: Las Vegas, NV, USA
  articleno: null
  authors:
  - - Kaiming
    - He
  - - Xiangyu
    - Zhang
  - - Shaoqing
    - Ren
  - - Jian
    - Sun
  chapter: null
  descrip: 'The original ResNet paper, which proposes the usage of residual layers
    to help solve the vanishing gradient problem when training deep neural networks.
    This technique is still used today in training LLMs. The original paper is applied
    to image segmentation and classification problems (on CIFAR-10 and ImageNet) and
    the resulting models ResNet-50, -101, and -152 were considered state-of-the-art.
    While the author''s original model and weights are available open source, they
    recommend using FAIR''s repo which also contains open source training in pytorch:
    github.com/facebookarchive/fb.resnet.torch'
  doi: 10.1109/CVPR.2016.90
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - '770'
  - '778'
  publisher: IEEE
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - classification
  - neural networks
  - CNNs
  - regularization
  - overfitting
  - optimization
  - software
  - open source
  - OSS
  - Python
  title: Deep Residual Learning for Image Recognition
  type: inproceedings
  url: https://arxiv.org/pdf/1512.03385
  venue: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  volume: null
  web: null
  year: 2016
heroux2020advancing:
  address: Oak Ridge, TN, USA
  articleno: null
  authors:
  - - Michael A.
    - Heroux
  - - Lois
    - McInnes
  - - David E.
    - Bernholdt
  - - Anshu
    - Dubey
  - - Elsa
    - Gonsiorowski
  - - Osni
    - Marques
  - - J. David
    - Moulton
  - - Boyana
    - Norris
  - - Elaine
    - Raybourn
  - - Satish
    - Balay
  - - Roscoe A.
    - Bartlett
  - - Lisa
    - Childers
  - - Todd
    - Gamblin
  - - Patricia
    - Grubel
  - - Rinku
    - Gupta
  - - Rebecca
    - Hartman-Baker
  - - Judith C.
    - Hill
  - - Stephen
    - Hudson
  - - Christoph
    - Junghans
  - - Alicia
    - Klinvex
  - - Reed
    - Milewicz
  - - Mark
    - Miller
  - - Hai
    - Ah Nam
  - - Jared
    - O'Neal
  - - Katherine
    - Riley
  - - Ben
    - Sims
  - - Jean
    - Schuler
  - - Barry F.
    - Smith
  - - Louis
    - Vernon
  - - Gregory R.
    - Watson
  - - James
    - Willenbring
  - - Paul
    - Wolfenbarger
  chapter: null
  descrip: The better scientific software tech report, with recommendations and rules
    of thumb for improving the quality of scientific software within the DOE
  doi: 10.2172/1606662
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '1'
  note: null
  number: ORNL TM-2020 1459 / ECP-U-RPT-2020-0001
  pages: null
  publisher: Oak Ridge National Laboratory
  series: null
  tags:
  - high-performance computing
  - HPC
  - software
  title: 'Advancing Scientific Productivity through Better Scientific Software: Developer
    Productivity and Software Sustainability Report'
  type: techreport
  url: https://www.osti.gov/servlets/purl/1606662
  venue: null
  volume: null
  web: null
  year: 2020
hert2020convex:
  address: null
  articleno: null
  authors:
  - - Susan
    - Hert
  - - Michael
    - Seel
  chapter: null
  descrip: The official CGAL User Guide docs page for computing high-dimensional convex
    hulls and Delaunay triangulations. CGAL is a standard numerical software package
    for using computational geometry data structures and algorithms in perfect precision
    (via symbolic arithmetic). CGAL is a header-only open source C++ library
  doi: null
  edition: '{5.0.2}'
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: '{CGAL Editorial Board}'
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  - convex hull
  - high dimension
  - high-performance computing
  - HPC
  - software
  - open source
  - OSS
  - C++
  title: '{dD} Convex Hulls and {D}elaunay Triangulations'
  type: incollection
  url: https://doc.cgal.org/5.0.2/Manual/packages.html#PkgConvexHullD
  venue: '{CGAL} User and Reference Manual'
  volume: null
  web: null
  year: 2020
hickernell1998generalized:
  address: null
  articleno: null
  authors:
  - - Fred J.
    - Hickernell
  chapter: null
  descrip: A numerical-quadrature based approximation to discrepancy. For high-dimensional
    ill-spaced points, the quadrature error can be huge, and although discrepancies
    should always be between 0 and 1, the approximation can approach (13/12)^d - 1,
    where d is the dimension of the problem. When points are randomly or quasi-randomly
    sampled, such large values of the approximation could be indicative of measure
    collapse. This is the technique used in scipy.stats.qmc.discrepancy(...)
  doi: 10.1090/S0025-5718-98-00894-1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0025-5718
  month: null
  note: null
  number: '221'
  pages:
  - '299'
  - '322'
  publisher: American Mathematical Society (AMS)
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  title: A generalized discrepancy and quadrature error bound
  type: article
  url: https://www.ams.org/mcom/1998-67-221/S0025-5718-98-00894-1
  venue: Mathematics of computation
  volume: '67'
  web: null
  year: 1998
hinton1983optimal:
  address: null
  articleno: null
  authors:
  - - Geoffrey E
    - Hinton
  - - Terrence J
    - Sejnowski
  chapter: null
  descrip: Original publication of the Boltzman machine, an early type of AI model
    that made predictions by modeling the Ising spin-glass model with public input
    units (set by the user with the independent variables during inference) and hidden
    internal units that are correlated to eachother and the public units through learned
    Ising model weights and biases. Each unit has a binary {0, 1} state. The energy
    of the system is the output (prediction). The weights and biases are learned through
    simulated annealing, and in this way, the model is not unlike quantum annealing.
    The model learns a distribution of potential inference values for each input combination.
    Thus, it is a form of distribution learning. These were a commonly used alternative
    to neural networks for a time, and often cited as explainable alternatives. The
    binary state variables would later be replaced by sigmoidal activation functions
    (for smoothness)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '448'
  - '453'
  publisher: Citeseer
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - Boltzmann machine
  - neural networks
  - scientific machine learning
  - SciML
  - regularization
  - overfitting
  - regression
  - classification
  title: Optimal perceptual inference
  type: inproceedings
  url: https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b89e9f0cef5ace08946a7c07bf7284854c418445
  venue: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition
  volume: '448'
  web: null
  year: 1983
hochreiter1997long:
  address: null
  articleno: null
  authors:
  - - Sepp
    - Hochreiter
  - - "J\xFCrgen"
    - Schmidhuber
  chapter: null
  descrip: Original publication on using long-short term memory within a recurrent
    neural network framework to address the issue of vanishing gradients during training.
    The idea is to truncate gradients for certain blocks (short-term memory units)
    and chain these blocks to generate skip connections for long term memory. These
    would be the state-of-the-art in natural language processing and other sequential
    prediction tasks until BERT replaces them with transformer models.
  doi: 10.1162/neco.1997.9.8.1735
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-7667
  month: '11'
  note: null
  number: '8'
  pages:
  - '1735'
  - '1780'
  publisher: MIT Press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - RNNs
  - scientific machine learning
  - SciML
  - transformers
  title: Long Short-Term Memory
  type: article
  url: https://direct.mit.edu/neco/article/9/8/1735-1780/6109
  venue: Neural Computation
  volume: '9'
  web: null
  year: 1997
hof2015google:
  address: null
  articleno: null
  authors:
  - - Robert D.
    - Hof
  chapter: null
  descrip: 'An article on Google GlassBox research: Google''s research division dedicated
    to interpretable machine learning'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: nov
  note: 'Last accessed: June 20, 2022'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  title: Google Tries to Make Machine Learning a Little More Human
  type: article
  url: https://www.technologyreview.com/2015/11/05/165175/google-tries-to-make-machine-learning-a-little-more-human
  venue: MIT Technology Review
  volume: null
  web: null
  year: 2015
hoffman2022optimizing:
  address: null
  articleno: null
  authors:
  - - Samuel C.
    - Hoffman
  - - Vijil
    - Chenthamarakshan
  - - Kahini
    - Wadhawan
  - - Pin-Yu
    - Chen
  - - Payel
    - Das
  chapter: null
  descrip: Techniques for optimizing molecule properties via a latent-space embedding
    that comes from a molecule autoencoder, from IBM Research
  doi: 10.1038/s42256-021-00422-y
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2522-5839
  month: '12'
  note: null
  number: '1'
  pages:
  - '21'
  - '31'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - mixed-variable optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  title: Optimizing molecules using efficient queries from property evaluations
  type: article
  url: https://www.nature.com/articles/s42256-021-00422-y
  venue: Nature Machine Intelligence
  volume: '4'
  web: null
  year: 2022
huang2019cpptaskflow:
  address: Rio de Janeiro, Brazil
  articleno: null
  authors:
  - - Tsung-Wei
    - Huang
  - - Chun-Xun
    - Lin
  - - Guannan
    - Guo
  - - Martin
    - Wong
  chapter: null
  descrip: Cpp-Taskflow original paper on open source software implementing a task-based
    parallel scheduler and parallel programming interface in C++
  doi: 10.1109/IPDPS.2019.00105
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '5'
  note: null
  number: ''
  pages:
  - '974'
  - '983'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - open source
  - OSS
  - parallel programming
  - C++
  title: '{Cpp-Taskflow}: Fast Task-Based Parallel Programming Using Modern {C++}'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8821011/
  venue: 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
  volume: ''
  web: null
  year: 2019
huangfu2018parallelizing:
  address: null
  articleno: null
  authors:
  - - Qi
    - Huangfu
  - - JA Julian
    - Hall
  chapter: null
  descrip: The official publication for HiGHS numerical optimization open source software
    package and solver for linear programming problems. The first successful effort
    to parallelize a simplex method based solver, specifically, they have successfully
    parallelized the dual revised simplex approach. HiGHS is now the default solver
    in most scipy.optimize. HiGHS is written in C++ with Fortran, C, and Python interfaces
    -- support CPU and GPU parallelism, the authors don't get perfect scaling by any
    means but this is the first successful effort to parallelize linear programming
    and still an achievement
  doi: 10.1007/s12532-017-0130-5
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1867-2949
  month: '3'
  note: null
  number: '1'
  pages:
  - '119'
  - '142'
  publisher: Springer
  series: null
  tags:
  - computational geometry
  - high-performance computing
  - HPC
  - parallel computing
  - GPU computing
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - Fortran
  - Python
  - CUDA
  - parallel programming
  - C++
  title: Parallelizing the dual revised simplex method
  type: article
  url: http://link.springer.com/10.1007/s12532-017-0130-5
  venue: Mathematical Programming Computation
  volume: '10'
  web: null
  year: 2018
hudson2022libensemble:
  address: null
  articleno: null
  authors:
  - - Stephen
    - Hudson
  - - Jeffrey
    - Larson
  - - John-Luke
    - Navarro
  - - Stefan M.
    - Wild
  chapter: null
  descrip: The original libEnsemble publication focusing on its techniques for distributing
    and evaluating ensembles of functions in parallel. Although not discussed, libEnsemble
    was already open source at the time
  doi: 10.1109/tpds.2021.3082815
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1045-9219
  month: '4'
  note: null
  number: '4'
  pages:
  - '977'
  - '988'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  title: '{libEnsemble}: A Library to Coordinate the Concurrent Evaluation of Dynamic
    Ensembles of Calculations'
  type: article
  url: https://ieeexplore.ieee.org/document/9439163
  venue: '{IEEE} Transactions on Parallel and Distributed Systems'
  volume: '33'
  web: null
  year: 2022
hudson2023libensemble:
  address: null
  articleno: null
  authors:
  - - Stephen
    - Hudson
  - - Jeffrey
    - Larson
  - - John-Luke
    - Navarro
  - - Stefan M.
    - Wild
  chapter: null
  descrip: 'The official JOSS paper for libEnsemble: an open source software library
    for performing parallel and distributed computations involving ensembles of computationally
    expensive function evaluations in Python'
  doi: 10.21105/joss.06031
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2475-9066
  month: '12'
  note: null
  number: '92'
  pages:
  - 6031
  publisher: The Open Journal
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  - software
  - open source
  - OSS
  - Python
  - parallel programming
  - MPI
  title: '{libEnsemble}: A complete {Python} toolkit for dynamic ensembles of calculations'
  type: techreport
  url: https://joss.theoj.org/papers/10.21105/joss.06031
  venue: Journal of Open Source Software
  volume: '8'
  web: null
  year: 2023
hunter2019introduction:
  address: null
  articleno: null
  authors:
  - - Susan R.
    - Hunter
  - - Eric A.
    - Applegate
  - - Viplove
    - Arora
  - - Bryan
    - Chong
  chapter: null
  descrip: A thorough survey on techniques and algorithms for solving multiobjective
    simulation optimization problems, including algorithms and techniques for handling
    small finite design spaces, discrete integer design spaces, and continuous design
    spaces. Covers theory, algorithms, and popular heuristics for all.
  doi: 10.1145/3299872
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1049-3301
  month: '1'
  note: null
  number: '1'
  pages:
  - '1'
  - '36'
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - mixed-variable optimization
  - surrogate modeling
  title: An introduction to multiobjective simulation optimization
  type: article
  url: https://dl.acm.org/doi/10.1145/3299872
  venue: ACM Transactions on Modeling and Computer Simulation
  volume: '29'
  web: null
  year: 2019
ilyas2019adversarial:
  address: null
  articleno: null
  authors:
  - - Andrew
    - Ilyas
  - - Shibani
    - Santurkar
  - - Dimitris
    - Tsipras
  - - Logan
    - Engstrom
  - - Brandon
    - Tran
  - - Aleksander
    - Madry
  chapter: null
  descrip: An analysis of how adversarial examples for neural network and other AI
    models are typically indicative of the model using highly predictive but brittle
    features to make predictions -- i.e., features that are highly predictive but
    nevertheless not robust enough to use for actually making predictions
  doi: null
  edition: null
  editors:
  - - H.
    - Wallach
  - - H.
    - Larochelle
  - - A.
    - Beygelzimer
  - - F. d\textquotesingle
    - Alch\'{e}-Buc
  - - E.
    - Fox
  - - R.
    - Garnett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '125'
  - '136'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - overfitting
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: Adversarial Examples Are Not Bugs, They Are Features
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '32'
  web: null
  year: 2019
intel2025oneapi:
  address: null
  articleno: null
  authors:
  - - ''
    - Intel
  chapter: null
  descrip: The official Intel TBB developer guide / reference for efficient and performance
    portable task-based parallel programming using a dynamic scheduler with work-stealing
    in modern C++. The open source software library can be downloaded from GitHub.
    They also offer limited NUMA support in the latest version
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 28, 2025'
  number: release 2022.1.0
  pages: null
  publisher: Intel Corporation
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - open source
  - OSS
  - parallel programming
  - C++
  title: '{oneAPI} Threading Building Blocks ({oneTBB})'
  type: misc
  url: https://uxlfoundation.github.io/oneTBB
  venue: null
  volume: null
  web: null
  year: 2025
ios2004information:
  address: Geneva, Switzerland
  articleno: null
  authors: []
  chapter: null
  descrip: ISO Fortran 2003 software standard -- definition of the Fortran 2003 standard
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: November
  note: null
  number: ISO/IEC 1539-1:2004(E)
  pages: null
  publisher: International Organization for Standardization
  series: null
  tags:
  - high-performance computing
  - HPC
  - software
  - Fortran
  title: '{Information technology -- Programming languages -- Fortran -- Part 1: Base
    Language}'
  type: techreport
  url: https://j3-fortran.org/doc/year/04/04-007.pdf
  venue: null
  volume: null
  web: null
  year: 2004
ios2010information:
  address: Geneva, Switzerland
  articleno: null
  authors: []
  chapter: null
  descrip: ISO Fortran 2008 software standard -- definition of the Fortran 2008 standard
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: October
  note: null
  number: ISO/IEC 1539-1:2010(E)
  pages: null
  publisher: International Organization for Standardization
  series: null
  tags:
  - high-performance computing
  - HPC
  - software
  - Fortran
  title: '{Information technology -- Programming languages -- Fortran -- Part 1: Base
    Language}'
  type: techreport
  url: https://j3-fortran.org/doc/year/10/10-007.pdf
  venue: null
  volume: null
  web: null
  year: 2010
ishibuchi2015modified:
  address: Madrid Spain
  articleno: null
  authors:
  - - Hisao
    - Ishibuchi
  - - Hiroyuki
    - Masuda
  - - Yuki
    - Tanigaki
  - - Yusuke
    - Nojima
  chapter: null
  descrip: A thorough analysis of the generational distance (GD), modified generational
    distance (GD+), and inverted generational distance (IGD) performance indicators
    for evaluating solutions to multiobjective optimization problems. I like pairing
    these performance indicators with the hypervolume indicator as they tend to measure
    convergence while hypervolume is more spread focused. The authors show that of
    the GD, IGD, and GD+ indicators, only GD+ is "Pareto compliant", which we often
    refer to as monotonic, meaning it strictly improves when a solution set is a strict
    superset of another. IGD is weakly monotonic in this sense, and standard GD is
    the worst option. The MGD equation is provided -- I have used it in some of my
    HPO work and it works very well
  doi: 10.1145/2739480.2754792
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '110'
  - '125'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Modified distance calculation in generational distance and inverted generational
    distance
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/2739480.2754792
  venue: 'Evolutionary Multi-Criterion Optimization: 8th International Conference,
    EMO 2015, Guimar{\~a}es, Portugal, March 29--April 1, 2015. Proceedings, Part
    II 8'
  volume: null
  web: null
  year: 2015
jain2013evolutionary:
  address: null
  articleno: null
  authors:
  - - Himanshu
    - Jain
  - - Kalyanmoy
    - Deb
  chapter: null
  descrip: 'The orgiinal NSGA-III paper part 2: a multiobjective evolutionary algorithm
    similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many
    objectives by having the user provide a collection of well-spaced reference points
    and optimizing toward those -- this paper focuses on how to handle constraints'
  doi: 10.1109/TEVC.2013.2281534
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '8'
  note: null
  number: '4'
  pages:
  - '602'
  - '622'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - constrained optimization
  - high dimension
  - evolutionary algorithms
  - EA
  title: 'An evolutionary many-objective optimization algorithm using reference-point
    based nondominated sorting approach, part {II}: Handling constraints and extending
    to an adaptive approach'
  type: article
  url: http://ieeexplore.ieee.org/document/6595567
  venue: IEEE Transactions on Evolutionary Computation
  volume: '18'
  web: null
  year: 2013
jain2022tiktoken:
  address: null
  articleno: null
  authors:
  - - Shantanu
    - Jain
  - - ''
    - others
  chapter: null
  descrip: The specific BPE tokenizer used by OpenAI in all their LLMs
  doi: null
  edition: null
  editors: []
  git: https://github.com/openai/tiktoken
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: May 4, 2025'
  number: 0.9.0
  pages: null
  publisher: GitHub
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - neural networks
  - LLMs
  - software
  - open source
  - OSS
  - Python
  title: tiktoken
  type: GitHub repository
  url: null
  venue: null
  volume: null
  web: null
  year: 2022
jain2024livecodebench:
  address: null
  articleno: null
  authors:
  - - Naman
    - Jain
  - - King
    - Han
  - - Alex
    - Gu
  - - Wen-Ding
    - Li
  - - Fanjia
    - Yan
  - - Tianjun
    - Zhang
  - - Sida
    - Wang
  - - Armando
    - Solar-Lezama
  - - Koushik
    - Sen
  - - Ion
    - Stoica
  chapter: null
  descrip: 'LiveCodeBench is a LLM code generation evaluation benchmark. It continuously
    collects new problems from CodeBench and annotates them with their release dates,
    so that LLMs can be evaluated on just problems from specific timeframes. This
    allows researchers to prevent contamination of the test set with data from the
    training set. An open source Python interface is available at: github.com/LiveCodeBench/LiveCodeBench
    Submissions to the leaderboard are made via pull request to github.com/LiveCodeBench/submissions'
  doi: 10.48550/arXiv.2403.07974
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2403.07974
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - benchmarking
  - open source
  title: '{LiveCodeBench}: Holistic and Contamination Free Evaluation of Large Language
    Models for Code'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2024
jiang2018quantum:
  address: null
  articleno: null
  authors:
  - - Shuxian
    - Jiang
  - - Keith A
    - Britt
  - - Alexander J
    - McCaskey
  - - Travis S
    - Humble
  - - Sabre
    - Kais
  chapter: null
  descrip: Quantum annealing algorithm for performing prime factorization
  doi: 10.1038/s41598-018-36058-z
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2045-2322
  month: '12'
  note: null
  number: '1'
  pages:
  - '1'
  - '9'
  publisher: Nature Publishing Group
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Quantum annealing for prime factorization
  type: article
  url: https://www.nature.com/articles/s41598-018-36058-z
  venue: Scientific Reports (Nature Publisher Group)
  volume: '8'
  web: null
  year: 2018
joe2003remark:
  address: null
  articleno: null
  authors:
  - - Stephen
    - Joe
  - - Frances Y.
    - Kuo
  chapter: null
  descrip: A modification to the original Sobol sequence generator code
  doi: 10.1145/641876.641879
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '3'
  note: null
  number: '1'
  pages:
  - 9
  publisher: Association for Computing Machinery
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  - Sobol sequence
  - software
  - Fortran
  title: 'Remark on Algorithm 659: Implementing Sobol''s Quasirandom Sequence Generator'
  type: article
  url: https://dl.acm.org/doi/10.1145/641876.641879
  venue: ACM Transactions on Mathematical Software
  volume: '29'
  web: null
  year: 2003
joe2008constructing:
  address: null
  articleno: null
  authors:
  - - Stephen
    - Joe
  - - Frances Y.
    - Kuo
  chapter: null
  descrip: The numerical software algorithm used in scipy algorithm for generating
    Sobol sequences (low discrepancy sequences)
  doi: 10.1137/070709359
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1064-8275
  month: '1'
  note: null
  number: '5'
  pages:
  - '2635'
  - '2654'
  publisher: SIAM
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  - Sobol sequence
  title: Constructing Sobol sequences with better two-dimensional projections
  type: article
  url: http://epubs.siam.org/doi/10.1137/070709359
  venue: SIAM Journal on Scientific Computing
  volume: '30'
  web: null
  year: 2008
johnson1990minimax:
  address: null
  articleno: null
  authors:
  - - M.E.
    - Johnson
  - - L.M.
    - Moore
  - - D.
    - Ylvisaker
  chapter: null
  descrip: Original publication on maximin and minimax designs for design-of-experiments.
    I.e., minimize the maximum distance from any point in the bounding box to the
    nearest point in the design, and maximize the minimum distance between any pair
    of points in the design.
  doi: 10.1016/0378-3758(90)90122-B
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0378-3758
  month: '10'
  note: null
  number: '2'
  pages:
  - '131'
  - '148'
  publisher: Elsevier BV
  series: null
  tags:
  - computational geometry
  - design of experiments
  - DoE
  title: Minimax and maximin distance designs
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/037837589090122B
  venue: Journal of Statistical Planning and Inference
  volume: '26'
  web: null
  year: 1990
jones1993lipschitzian:
  address: null
  articleno: null
  authors:
  - - Donald R.
    - Jones
  - - Cary D.
    - Perttunen
  - - Bruce E.
    - Stuckman
  chapter: null
  descrip: D.R. Jones' original paper on the landmark DIRECT (DIviding RECTangles)
    algorithm for direct search global blackbox optimization. The idea is that you
    can perform branch-and-bound style Lipschitzian optimization without knoweldge
    of the Lipschitz constant by dividing a rectangular design space into rectangular
    regions (rectangles) and subdividing those rectangles that could be potentially
    optimal given any Lipschitz constant by choosing those boxes on the lower left
    convex hull of the objective value at the center vs box diameter scatter plot
  doi: 10.1007/bf00941892
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-3239
  month: '10'
  note: null
  number: '1'
  pages:
  - '157'
  - '181'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  title: Lipschitzian optimization without the Lipschitz constant
  type: article
  url: http://link.springer.com/10.1007/BF00941892
  venue: Journal of optimization Theory and Applications
  volume: '79'
  web: null
  year: 1993
jones1998efficient:
  address: null
  articleno: null
  authors:
  - - Donald R
    - Jones
  - - Matthias
    - Schonlau
  - - William J
    - Welch
  chapter: null
  descrip: D.R. Jones' original paper on "efficient global optimization" (EGO). This
    is often credited as the original implementation of multivariate Bayesian optimization.
    Jones proposes using Gaussian processes to produce a Gaussian posterior, whose
    expected improvement function can be efficiently optimized to select the next
    candidate. This is also one of the early works in sequential optimization via
    a generic (i.e., non polynomial) surrogate model. However, earlier work on design-of-experiments
    and response surface modeling did exist in the engineering design optimization
    space. Earlier papers had explored the idea of optimizing Gaussian processes to
    select experiments (especially in one and two-dimensions). However, this paper
    is a landmark in that it gave rise to the field of multivariate Bayesian optimization.
    EGO is still often used as the benchmark Bayesian optimization algorithm.
  doi: 10.1023/A:1008306431147
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '455'
  - '492'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - Gaussian process
  title: Efficient global optimization of expensive black-box functions
  type: article
  url: null
  venue: Journal of Global optimization
  volume: '13'
  web: null
  year: 1998
jordan1986serial:
  address: San Diego, La Jolla, CA, USA
  articleno: null
  authors:
  - - Michael I.
    - Jordan
  chapter: null
  descrip: Original tech report where recurent neural networks were originally applied
    at scale to a language processing (next word prediction) application
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Institute for Cognitive Science, University of California
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - RNNs
  - neural networks
  - scientific machine learning
  - SciML
  title: 'Serial order: a parallel distributed processing approach'
  type: techreport
  url: https://www.osti.gov/biblio/6910294
  venue: null
  volume: null
  web: null
  year: 1986
jrad2019selflearning:
  address: San Diego, CA, USA
  articleno: null
  authors:
  - - Mohamed
    - Jrad
  - - Rakesh K.
    - Kapania
  - - Joseph A.
    - Schetz
  - - Layne T.
    - Watson
  chapter: null
  descrip: 'One of the earliest applications of DelunaySparse: predicting oblique
    shocks in supersonic flows for aerospace engineering applications. This is also
    a great example of an early example of scientific machine learning since the authors
    were also using neural networks and other more modern AI methods to make predictions'
  doi: 10.2514/6.2019-1704
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '1'
  note: null
  number: null
  pages: null
  publisher: AIAA
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - Delaunay triangulation
  - interpolation
  - regression
  title: 'Self-Learning, Adaptive Software for Aerospace Engineering Applications:
    Example of Oblique Shocks in Supersonic Flow'
  type: inproceedings
  url: https://arc.aiaa.org/doi/10.2514/6.2019-1704
  venue: AIAA Scitech 2019 Forum
  volume: null
  web: null
  year: 2019
kadowaki1998quantum:
  address: null
  articleno: null
  authors:
  - - Tadashi
    - Kadowaki
  - - Hidetoshi
    - Nishimori
  chapter: null
  descrip: A quantum gate model algorithm for solving Ax=b An adiabatic quantum computing
    algorithm for solving Ax=b A physical implementation of a quantum annealing algorithm
    for solving linear systems This is the original paper where quantum annealing
    is proposed, which shows that the likelihood of achieving the ground state decreases
    quadratically with linearly shrinking energy gap.
  doi: 10.1103/physreve.58.5355
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1063-651X
  month: '11'
  note: null
  number: '5'
  pages:
  - 5355
  publisher: APS
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Quantum annealing in the transverse Ising model
  type: article
  url: https://link.aps.org/doi/10.1103/PhysRevE.58.5355
  venue: Physical Review E
  volume: '58'
  web: null
  year: 1998
kale1993charm:
  address: Washington, D.C., USA
  articleno: null
  authors:
  - - Laxmikant V.
    - Kale
  - - Sanjeev
    - Krishnan
  chapter: null
  descrip: Charm++ original paper introducing open source software for parallel programming
    in C++ via message-passing objects called "chares"
  doi: 10.1145/165854.165874
  edition: null
  editors: []
  git: null
  isbn: 0897915879
  issn: null
  month: '10'
  note: null
  number: null
  pages:
  - 18
  publisher: Association for Computing Machinery
  series: OOPSLA '93
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - open source
  - OSS
  - parallel programming
  - C++
  title: '{CHARM++}: a portable concurrent object oriented system based on {C++}'
  type: techreport
  url: https://dl.acm.org/doi/10.1145/165854.165874
  venue: Proceedings of the Eighth Annual Conference on Object-Oriented Programming
    Systems, Languages, and Applications
  volume: null
  web: null
  year: 1993
kandasamy2020tuning:
  address: null
  articleno: null
  authors:
  - - Kirthevasan
    - Kandasamy
  - - Karun Raju
    - Vysyaraju
  - - Willie
    - Neiswanger
  - - Biswajit
    - Paria
  - - Christopher R.
    - Collins
  - - Jeff
    - Schneider
  - - Barnabus
    - Poczos
  - - Eric P.
    - Xing
  chapter: null
  descrip: 'Introducing Dragonfly: an open source numerical software package for solving
    neural architecture search problems via Bayesian optimization and solving an optimal
    transport problem to evaluate the distance between two networks. Considered a
    bit of a landmark paper for neural network architecture search problems. The open
    source Python software is widely used for a variety of applications outside NAS,
    including molecular discovery'
  doi: null
  edition: null
  editors: []
  git: http://github.com/dragonfly/dragonfly
  isbn: null
  issn: null
  month: null
  note: null
  number: '81'
  pages:
  - '1'
  - '27'
  publisher: null
  series: null
  tags:
  - optimization
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  - Gaussian process
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - uncertainty quantification
  - UQ
  - representation learning
  - software
  - open source
  - OSS
  - Python
  title: 'Tuning Hyperparameters without Grad Students: Scalable and Robust {Bayesian}
    Optimisation with {Dragonfly}'
  type: article
  url: http://jmlr.org/papers/v21/18-223.html
  venue: Journal of Machine Learning Research
  volume: '21'
  web: null
  year: 2020
karimi2019practical:
  address: null
  articleno: null
  authors:
  - - Sahar
    - Karimi
  - - Pooya
    - Ronagh
  chapter: null
  descrip: Paper describing the practical concerns, optimizations, and limitations
    of performing adiabatic quantum computing on real-world systems (i.e., D-Wave)
  doi: 10.1007/s11128-019-2213-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1570-0755
  month: '4'
  note: null
  number: '4'
  pages:
  - 94
  publisher: Springer
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Practical integer-to-binary mapping for quantum annealers
  type: article
  url: https://link.springer.com/article/10.1007/s11128-019-2213-x
  venue: Quantum Information Processing
  volume: '18'
  web: null
  year: 2019
karl2023multiobjective:
  address: null
  articleno: null
  authors:
  - - Florian
    - Karl
  - - Tobias
    - Pielok
  - - Julia
    - Moosbauer
  - - Florian
    - Pfisterer
  - - Stefan
    - Coors
  - - Martin
    - Binder
  - - Lennart
    - Schneider
  - - Janek
    - Thomas
  - - Jakob
    - Richter
  - - Michel
    - Lang
  - - Eduardo C.
    - "Garrido-Merch\xE1n"
  - - Juergen
    - Branke
  - - Bernd
    - Bischl
  chapter: null
  descrip: A survey of multiobjective optimization algorithms for hyperparameter tuning
    in the context of automatic machine learning (autoML)
  doi: 10.1145/3610536
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2688-299X
  month: '12'
  note: null
  number: '4'
  pages:
  - '1'
  - '50'
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  - decision trees
  - Gaussian process
  title: Multi-Objective Hyperparameter Optimization in Machine Learning -- An Overview
  type: article
  url: https://dl.acm.org/doi/10.1145/3610536
  venue: ACM Transactions on Evolutionary Learning and Optimization
  volume: '3'
  web: null
  year: 2023
karpatne2017theoryguided:
  address: null
  articleno: null
  authors:
  - - Anuj
    - Karpatne
  - - Gowtham
    - Atluri
  - - James H.
    - Faghmous
  - - Michael
    - Steinbach
  - - Arindam
    - Banerjee
  - - Auroop
    - Ganguly
  - - Shashi
    - Shekhar
  - - Nagiza
    - Samatova
  - - Vipin
    - Kumar
  chapter: null
  descrip: An introductory-level paper to theory-guided data science. I haven't read
    but I assume it covers the same topics he taught in his class at Virginia Tech
  doi: 10.1109/TKDE.2017.2720168
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1041-4347
  month: '10'
  note: null
  number: '10'
  pages:
  - '2318'
  - '2331'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: 'Theory-Guided Data Science: A New Paradigm for Scientific Discovery from
    Data'
  type: article
  url: http://ieeexplore.ieee.org/document/7959606
  venue: IEEE Transactions on Knowledge and Data Engineering
  volume: '29'
  web: null
  year: 2017
karypis1998fast:
  address: null
  articleno: null
  authors:
  - - George
    - Karypis
  - - Vipin
    - Kumar
  chapter: null
  descrip: Karypis journal paper on numerical algorithms for multilevel graph partitioning.
    The idea in multilevel graph partitioning is that when given a very large graph,
    we first coarsen the graph to a manageable size. Then we compute the cut at the
    coursest level and refine this cut at the finer levels as we flatten (un-coarsen)
    the graph in a typical V-cycle. The meat of this paper is actually a detailed
    comparison of various algorithms for coarsening and their quality tradeoffs, time
    complexities, and compression factors. Additionally a similar comparison of cut
    and cut-refinement algorithms
  doi: 10.1137/S1064827595287997
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1064-8275
  month: '1'
  note: null
  number: '1'
  pages:
  - '359'
  - '392'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - partitioning
  title: A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs
  type: article
  url: http://epubs.siam.org/doi/10.1137/S1064827595287997
  venue: SIAM Journal on Scientific Computing
  volume: '20'
  web: null
  year: 1998
karypis1998hmetis:
  address: Minneapolis, MN, USA
  articleno: null
  authors:
  - - George
    - Karypis
  - - Vipin
    - Kumar
  chapter: null
  descrip: Official documentation and techreport for the widely-used multilevel hypergraph
    partitioning software hMETIS, which is a standard in partitioning based placement
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: version 1.5.3
  pages: null
  publisher: Department of Computer Science \& Engineering, University of Minnesota
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - partitioning
  - software
  title: '{hMETIS}: A hypergraph partitioning package'
  type: techreport
  url: https://course.ece.cmu.edu/~ee760/760docs/hMetisManual.pdf
  venue: null
  volume: null
  web: null
  year: 1998
karypis1999multilevel:
  address: null
  articleno: null
  authors:
  - - G.
    - Karypis
  - - R.
    - Aggarwal
  - - V.
    - Kumar
  - - S.
    - Shekhar
  chapter: null
  descrip: Karypis paper on multilevel hypergraph partitioning for VLSI applications.
    This is also kind of describing the algorithm used in the hMETIS partitioning
    software
  doi: 10.1109/92.748202
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1063-8210
  month: '3'
  note: null
  number: '1'
  pages:
  - '69'
  - '79'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - optimization
  - partitioning
  title: 'Multilevel hypergraph partitioning: applications in {VLSI} domain'
  type: article
  url: http://ieeexplore.ieee.org/document/748202/
  venue: IEEE Transactions on Very Large Scale Integration (VLSI) Systems
  volume: '7'
  web: null
  year: 1999
khan2018manifold:
  address: null
  articleno: null
  authors:
  - - Kamil A.
    - Khan
  - - Jeffrey
    - Larson
  - - Stefan M.
    - Wild
  chapter: null
  descrip: Proposes a new technique called manifold sampling to solve blackbox optimization
    problems where a smooth blackbox function is composed with a piecewise linear
    (non blackbox) function. Normally, this would create a nonsmooth blackbox function,
    but by modeling the blackbox function separately and sampling the different manifolds
    produced by the changes in active components of the piecewise function, we can
    still model the blackbox function with smooth techniques and solve the optimization
    problem efficiently
  doi: 10.1137/17m114741x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '4'
  pages:
  - '3001'
  - '3024'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: Manifold Sampling for Optimization of Nonconvex Functions that are Piecewise
    Linear Compositions of Smooth Components
  type: article
  url: https://epubs.siam.org/doi/10.1137/17M114741X
  venue: '{SIAM} Journal on Optimization'
  volume: '28'
  web: null
  year: 2018
khoshaman2018quantum:
  address: null
  articleno: null
  authors:
  - - Amir
    - Khoshaman
  - - Walter
    - Vinci
  - - Brandon
    - Denis
  - - Evgeny
    - Andriyash
  - - Mohammad H
    - Amin
  chapter: null
  descrip: Algorithm for implementing a quantum variational autoencoder
  doi: 10.1088/2058-9565/aada1f
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2058-9565
  month: '9'
  note: null
  number: '1'
  pages:
  - 14001
  publisher: IOP Publishing
  series: null
  tags:
  - quantum computing
  title: Quantum variational autoencoder
  type: article
  url: https://iopscience.iop.org/article/10.1088/2058-9565/aada1f/meta
  venue: Quantum Science and Technology
  volume: '4'
  web: null
  year: 2018
kim2004spea2:
  address: Birmingham, UK
  articleno: null
  authors:
  - - Mifa
    - Kim
  - - Tomoyuki
    - Hiroyasu
  - - Mitsunori
    - Miki
  - - Shinya
    - Watanabe
  chapter: null
  descrip: The SPEA2+ algorithm for solving multiobjective optimization problems with
    evolutionary algorithms. Apparently this is widely-used numerical software, but
    I can't find the download
  doi: 10.1007/978-3-540-30217-9_75
  edition: null
  editors: []
  git: null
  isbn: 978-3-540-30217-9_75
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '742'
  - '751'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - evolutionary algorithms
  - EA
  title: '{SPEA2+}: Improving the performance of the {S}trength {P}areto {E}volutionary
    {A}lgorithm 2'
  type: inproceedings
  url: null
  venue: Proc. International Conference on Parallel Problem Solving from Nature (PPSN
    VIII)
  volume: null
  web: null
  year: 2004
kimeldorf1971some:
  address: null
  articleno: null
  authors:
  - - George
    - Kimeldorf
  - - Grace
    - Wahba
  chapter: null
  descrip: Landmark paper showing that given a target function f, set of scattered
    data (xi, f(xi)), and family of SPD kernel functions K(x1, x2) spanning the RKHS,
    the unique basis for the RKHS is K(xi, x), and furthermore the minimizer for any
    definite integral of Lf for (where L is any linear operator) can be found by solving
    a least squares problem with the kernel matrix K(xi, xj)*b = f(xi) for any pair
    xi, xj in (xi, f(xi)). This is the theoretical basis for all RBF interpolation
    and regularized RBFs / RBFs with tail.
  doi: 10.1016/0022-247x(71)90184-3
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-247X
  month: '1'
  note: null
  number: '1'
  pages:
  - '82'
  - '95'
  publisher: Elsevier
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - RBFs
  - high dimension
  - interpolation
  - regression
  - approximation theory
  title: Some results on {Tchebycheffian} spline functions
  type: article
  url: https://pages.stat.wisc.edu/~wahba/ftp1/oldie/kw71.pdf
  venue: Journal of mathematical analysis and applications
  volume: '33'
  web: null
  year: 1971
kingma2014autoencoding:
  address: null
  articleno: null
  authors:
  - - Diederik P
    - Kingma
  - - Max
    - Welling
  chapter: null
  descrip: The original paper defining variational autoencoders, a standard practice
    in performing dimension reduction and training models that encode continuous latent
    spaces. The idea being to train and optimize an encoder neural network model whose
    posterior is a continuous latent space to generate samples in the latent space
    distribution based on inputs from the original dataset without "losing information",
    then jointly train a decoder model that samples the latent distribution to produce
    the original observations. The idea being to randomly sample new data points that
    look like the original data. When trained jointly, these models can also be used
    as embedder/extractor or compression/decompression pairs.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - autoencoders
  - representation learning
  - neural networks
  - scientific machine learning
  - SciML
  - dimension reduction
  title: Auto-encoding variational {B}ayes
  type: inproceedings
  url: https://arxiv.org/abs/1312.6114
  venue: 2nd International Conference on Learning Representations (ICLR 2014)
  volume: null
  web: null
  year: 2014
kingma2015adam:
  address: San Diego, CA, USA
  articleno: null
  authors:
  - - Diedrik
    - Kingma
  - - Jimmy
    - Ba
  chapter: null
  descrip: 'The original paper on Adam: an adaptive gradient and moment estimator
    that uses second order moments to approximate curvature (i.e., Hessian information)
    in order to accelerate the convergence of AdaGrad. In particular, this means applying
    Nesterov''s momentum to both the gradient and curvature estimations. From 2015-2024
    this was the state-of-the-art algorithm for optimization of neural network weights
    during training, and was what was typically meant when people talked about stochastic
    gradient descent.'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 11
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - optimization
  - stochastic optimization
  - convex optimization
  - scientific machine learning
  - SciML
  title: 'Adam: A method for stochastic optimization'
  type: inproceedings
  url: https://arxiv.org/abs/1412.6980
  venue: 3rd International Conference on Learning Representations (ICLR 2015)
  volume: null
  web: null
  year: 2015
klee1972how:
  address: null
  articleno: null
  authors:
  - - Victor
    - Klee
  - - George J.
    - Minty
  chapter: null
  descrip: 'The Klee Minty cube: A famous counterexample showing that for every pivoting
    strategy for the simplex method, we can construct a pathological problem where
    that strategy will visit every vertex of the cube before the solution. This proves
    that the simplex method cannot be used to solve linear programming problems in
    strongly polynomial time'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '159'
  - '175'
  publisher: null
  series: null
  tags:
  - computational geometry
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  title: How good is the simplex algorithm?
  type: article
  url: https://en.wikipedia.org/wiki/Klee%E2%80%93Minty_cube
  venue: Inequalities
  volume: III
  web: null
  year: 1972
klee1980complexity:
  address: null
  articleno: null
  authors:
  - - Victor
    - Klee
  chapter: null
  descrip: A counter example showing that the worst-case size of the Delaunay triangulation
    and Voronoi diagram is always exponential in the dimension. The example is somewhat
    similar to the Klee-Minty cube from linear programming.
  doi: 10.1007/bf01224932
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0003-889X
  month: '12'
  note: null
  number: '1'
  pages:
  - '75'
  - '80'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - high dimension
  title: On the complexity of d-dimensional {V}oronoi diagrams
  type: article
  url: http://link.springer.com/10.1007/BF01224932
  venue: Archiv der Mathematik
  volume: '34'
  web: null
  year: 1980
kleen2005numa:
  address: null
  articleno: null
  authors:
  - - Andi
    - Kleen
  chapter: null
  descrip: Original libnuma paper (now dated) introducing the widely used libnuma
    software providing a C interface for portably binding threads to NUMA nodes on
    linux and unix systems
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Suse Lab
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - parallel programming
  - C
  title: A {NUMA} {API} for {Linux}
  type: techreport
  url: https://halobates.de/numaapi3.pdf
  venue: null
  volume: null
  web: null
  year: 2005
knowles2006parego:
  address: null
  articleno: null
  authors:
  - - Joshua
    - Knowles
  chapter: null
  descrip: 'The original ParEGO algorithm paper: in each iteration of the algorithm,
    the authors use NSGA-II to optimize the expected improvement of the Gaussian process
    surrogates of each objective. Then, results are scalarized using augmented chebyshev
    and the best results are evaluated for the next iteration'
  doi: 10.1109/tevc.2005.851274
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '2'
  note: null
  number: '5'
  pages:
  - '1341'
  - '66'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - scalarization
  - evolutionary algorithm
  - EA
  - Gaussian process
  title: '{ParEGO:} A hybrid algorithm with on-line landscape approximation for expensive
    multiobjective optimization problems'
  type: article
  url: http://ieeexplore.ieee.org/document/1583627
  venue: IEEE Transactions on Evolutionary Computation
  volume: '8'
  web: null
  year: 2006
kolonay2011service:
  address: Paris, France
  articleno: null
  authors:
  - - Raymond M.
    - Kolonay
  - - Michael
    - Sobolewski
  chapter: null
  descrip: The SORCER software is a service oriented computing environment used by
    the US Airforce research lab (AFRL) to distribute expensive computations (such
    as design optimizations) across their large distributed network of computing resources
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '26'
  - '30'
  publisher: Citeseer
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - software
  - Java
  - parallel programming
  title: Service oriented computing environment ({SORCER}) for large scale, distributed,
    dynamic fidelity aeroelastic analysis
  type: inproceedings
  url: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.656.7539
  venue: International Forum on Aeroelasticity and Structural Dynamics (IFASD 2011),
    Optimization
  volume: null
  web: null
  year: 2011
kovachki2021neural:
  address: null
  articleno: null
  authors:
  - - Nikola B.
    - Kovachki
  - - Zongyi
    - Li
  - - Burigede
    - Liu
  - - Kamyar
    - Azizzadenesheli
  - - Kaushik
    - Bhattacharya
  - - Andrew M.
    - Stuart
  - - Anima
    - Anandkumar
  chapter: null
  descrip: The original paper on neural operators, a now ubiquitous method in scientific
    machine learning where an operator (i.e., for solving PDEs) between infinite dimensional
    function spaces is learned by composing integral operator basis functions and
    regular ReLU layers
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - neural operators
  - approximation theory
  title: 'Neural Operator: Learning Maps Between Function Spaces'
  type: article
  url: https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf
  venue: CoRR
  volume: abs/2108.08481
  web: null
  year: 2021
kreps2011kafka:
  address: null
  articleno: null
  authors:
  - - Jay
    - Kreps
  - - Neha
    - Narkhede
  - - Jun
    - Rao
  - - ''
    - others
  chapter: null
  descrip: 'Original Kafka paper: a fully-distributed, scalable message-passing and
    log processing interface. A Kafka system consists of producer, consumer, and broker
    processes / nodes. Producers generate logs/messages and consumers subscribte to
    log/message streams from producers. When a producer generates a new message or
    log, it publishes that message to a given topic. Those topics are broken up into
    sequential partitions and distributed across the broker nodes. The broker nodes
    and their partitions are all stateless. Consumers are placed in groups, with each
    group subscribing to a topic. Each group will read messages/logs from a specific
    topic, with a guarantee that each partition is read sequentially, but messages
    from different partitions will be read in any order. They guarantee that each
    group will read each message in their subscribed topics at least once. Groups
    store all state information locally, and therefore have the ability to roll-back
    and replay a topic from any offset. The original application is for distributing
    LinkedIn messages. Kafka is a standard in scalable distributed message/log streaming,
    and created the paradigm of subscribable topics. The official open-source implementation
    is Apache Kafka https://github.com/apache/kafka'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '2011'
  pages:
  - '1'
  - '7'
  publisher: Athens, Greece
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - software
  - open source
  - OSS
  - Kafka
  title: 'Kafka: A distributed messaging system for log processing'
  type: inproceedings
  url: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf
  venue: Proceedings of the NetDB
  volume: '11'
  web: null
  year: 2011
krizhevsky2009learning:
  address: Toronto, ON, Canada
  articleno: null
  authors:
  - - Alex
    - Krizhevsky
  chapter: null
  descrip: The official reference for the CIFAR-10 dataset -- one of the next most
    popular neural network and image classification benchmark problems for machine
    learning and AI research in the 2010s. Just a little harder than MNIST due to
    having lower quality images and more (imbalanced) classes
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: University of Toronto, Dept. of Computer Science
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - benchmarking
  - classification
  - neural networks
  title: Learning multiple layers of features from tiny images
  type: phdthesis
  url: https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf
  venue: null
  volume: null
  web: null
  year: 2009
krizhevsky2012imagenet:
  address: null
  articleno: null
  authors:
  - - Alex
    - Krizhevsky
  - - Ilya
    - Sutskever
  - - Geoffrey E
    - Hinton
  chapter: null
  descrip: The official publication for AlexNet -- one of the first truly massive
    overparameterized convolutional neural networks, which threw away a lot of the
    conventional wisdom around overfitting and achieved state-of-the-art performance
    and fine generalization errors on the ImageNet benchmark problem. This could be
    considered the beginning of "deep" learning in the sense of adding many many layers
  doi: null
  edition: null
  editors:
  - - F.
    - Pereira
  - - C.J.
    - Burges
  - - L.
    - Bottou
  - - K.Q.
    - Weinberger
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - classification
  - neural networks
  - CNNs
  - regularization
  - overfitting
  - high-performance computing
  - HPC
  - benchmarking
  - scientific machine learning
  - SciML
  title: '{ImageNet} Classification with Deep Convolutional Neural Networks'
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '25'
  web: null
  year: 2012
kuipers1974uniform:
  address: null
  articleno: null
  authors:
  - - L.
    - Kuipers
  - - H.
    - Niederreiter
  chapter: null
  descrip: The textbook on discrepancy that Manisha used to learn about low discrepancy
    sequences and their motivation
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - xiv
  - '390'
  publisher: Wiley-Interscience [John Wiley \& Sons], New York-London-Sydney
  series: Pure and Applied Mathematics
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  title: Uniform distribution of sequences
  type: book
  url: https://web.maths.unsw.edu.au/~josefdick/preprints/KuipersNied_book.pdf
  venue: null
  volume: null
  web: null
  year: 1974
kushner1964new:
  address: null
  articleno: null
  authors:
  - - H. J.
    - Kushner
  chapter: null
  descrip: Often credited as the first paper where the idea of Bayesian optimization
    (in the 1D case) was proposed -- personally, I usually credit Jones with EGO
  doi: 10.1115/1.3653121
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0021-9223
  month: '03'
  note: null
  number: '1'
  pages:
  - '97'
  - '106'
  publisher: ASME International
  series: null
  tags:
  - optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - Gaussian process
  title: A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve
    in the Presence of Noise
  type: article
  url: https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/86/1/97/5763745/97\_1.pdf
  venue: Journal of Basic Engineering
  volume: '86'
  web: null
  year: 1964
lai2003stochastic:
  address: null
  articleno: null
  authors:
  - - Tze Leung
    - Lai
  chapter: null
  descrip: Stochastic approximation algorithm (i.e., stochastic gradient descent)
    and how to analyze its radius of convergence for a fixed step-size -- you can
    decay its step size at a square-summable but not summable rate to guarantee convergence
    in the limit
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '2'
  pages:
  - '391'
  - '406'
  publisher: Institute of Mathematical Statistics
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - optimization
  - stochastic optimization
  - convex optimization
  - scientific machine learning
  - SciML
  title: Stochastic approximation
  type: article
  url: https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-2/Stochastic-approximation-invited-paper/10.1214/aos/1051027873.full
  venue: The annals of Statistics
  volume: '31'
  web: null
  year: 2003
lakhmiri2021hypernomad:
  address: null
  articleno: null
  authors:
  - - Dounia
    - Lakhmiri
  - - S{\'e}bastien Le
    - Digabel
  - - Christophe
    - Tribes
  chapter: null
  descrip: HyperNOMAD is the NOMAD team's open source Python software for performing
    hyperparameter optimization. Last I checked, it was not yet mature enough to use
    outside of their test problems, but I suspect improvements have been made since
    then
  doi: 10.1145/3450975
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '9'
  note: null
  number: '3'
  pages:
  - '1'
  - '27'
  publisher: ACM New York, NY, USA
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  title: '{HyperNOMAD}: Hyperparameter Optimization of Deep Neural Networks Using
    Mesh Adaptive Direct Search'
  type: article
  url: https://dl.acm.org/doi/10.1145/3450975
  venue: ACM Transactions on Mathematical Software (TOMS)
  volume: '47'
  web: null
  year: 2021
larson2018asynchronously:
  address: null
  articleno: null
  authors:
  - - Jeffrey
    - Larson
  - - Stefan M
    - Wild
  chapter: null
  descrip: The APOSMM Python software is a framework for implementing multistart derivative-free
    optimization algorithms and running them in asynchronously
  doi: 10.1007/s12532-017-0131-4
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1867-2949
  month: '9'
  note: null
  number: '3'
  pages:
  - '303'
  - '332'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - global optimization
  title: Asynchronously parallel optimization solver for finding multiple minima
  type: article
  url: http://link.springer.com/10.1007/s12532-017-0131-4
  venue: Mathematical Programming Computation
  volume: '10'
  web: null
  year: 2018
larson2019derivativefree:
  address: null
  articleno: null
  authors:
  - - Jeffrey
    - Larson
  - - Matt
    - Menickelly
  - - Stefan M.
    - Wild
  chapter: null
  descrip: A thorough survey of techniques and algorithms in derivative-free optimization
  doi: 10.1017/S0962492919000060
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0962-4929
  month: '5'
  note: null
  number: null
  pages:
  - '287'
  - '404'
  publisher: Cambridge University Press
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: Derivative-free optimization methods
  type: article
  url: https://www.cambridge.org/core/product/identifier/S0962492919000060/type/journal_article
  venue: Acta Numerica
  volume: '28'
  web: null
  year: 2019
larson2024structureaware:
  address: null
  articleno: null
  authors:
  - - Jeffrey
    - Larson
  - - Matt
    - Menickelly
  chapter: null
  descrip: Jeff's GOOMBAH paper on exploiting composite structures in derivative-free
    optimization, meaning that a blackbox function is composed with an algebraic function
    and we want to optimize the result, while exploiting the fact that we know the
    equation of the algebraic function. Similar technique is used in ParMOO to exploit
    this same structure and others that are specific to the multiobjective case
  doi: 10.1007/s12532-023-00245-5
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1867-2949
  month: '3'
  note: null
  number: '1'
  pages:
  - '1'
  - '36'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: Structure-Aware Methods for Expensive Derivative-Free Nonsmooth Composite
    Optimization
  type: article
  url: https://link.springer.com/10.1007/s12532-023-00245-5
  venue: Mathematical Programming Computation
  volume: '16'
  web: null
  year: 2024
lasalle2013multithreaded:
  address: Cambridge, MA, USA
  articleno: null
  authors:
  - - Dominique
    - Lasalle
  - - George
    - Karypis
  chapter: null
  descrip: Algorithm for multi-threaded graph partitioning, e.g., the parallel version
    of the hMETIS algorithm
  doi: 10.1109/IPDPS.2013.50
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '5'
  note: null
  number: ''
  pages:
  - '225'
  - '236'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - FPGA
  title: Multi-threaded Graph Partitioning
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/6569814/
  venue: 2013 IEEE 27th International Symposium on Parallel and Distributed Processing
  volume: ''
  web: null
  year: 2013
lasserre2001global:
  address: null
  articleno: null
  authors:
  - - Jean B
    - Lasserre
  chapter: null
  descrip: A summary of classical solutions and challenges to solving sum-of-squares
    minimization of polynomials
  doi: 10.1137/s1052623400366802
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '3'
  pages:
  - '796'
  - '817'
  publisher: SIAM
  series: null
  tags:
  - optimization
  - convex optimization
  - global optimization
  title: Global optimization with polynomials and the problem of moments
  type: article
  url: http://epubs.siam.org/doi/10.1137/S1052623400366802
  venue: SIAM Journal on optimization
  volume: '11'
  web: null
  year: 2001
laumanns2006efficient:
  address: null
  articleno: null
  authors:
  - - Marco
    - Laumanns
  - - Lothar
    - Thiele
  - - Eckart
    - Zitzler
  chapter: null
  descrip: An adaptive scheme for selecting epsilon-constraint scalarizations when
    solving multiobjective optimization problems.
  doi: 10.1016/j.ejor.2004.08.029
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0377-2217
  month: '3'
  note: null
  number: '3'
  pages:
  - '932'
  - '942'
  publisher: Elsevier
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: An efficient, adaptive parameter variation scheme for metaheuristics based
    on the epsilon-constraint method
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0377221704005715
  venue: European Journal of Operational Research
  volume: '169'
  web: null
  year: 2006
lavely2022powering:
  address: Sunnyvale, CA, USA
  articleno: null
  authors:
  - - Adam
    - Lavely
  chapter: null
  descrip: A whitepaper describing Cerebras's wafer scale neuromorphic computing architectures,
    as used in the AI incubator at Argonne
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Cerebras Systems, Inc.
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  title: Powering Extreme-Scale {HPC} with {C}erebras Wafer-Scale Accelerators
  type: techreport
  url: https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Powering-Extreme-Scale-HPC-with-Cerebras.pdf
  venue: null
  volume: null
  web: null
  year: 2022
lax2002functional:
  address: null
  articleno: null
  authors:
  - - Peter D.
    - Lax
  chapter: null
  descrip: Peter Lax's classic textbook on functional analysis and all the core theorems
    Banach spaces, Hilbert spaces, and approximation theory
  doi: null
  edition: null
  editors: []
  git: null
  isbn: 0-471-55604-1
  issn: null
  month: null
  note: null
  number: null
  pages:
  - xx
  - '580'
  publisher: Wiley-Interscience [John Wiley \& Sons], New York
  series: Pure and Applied Mathematics (New York)
  tags:
  - design of experiments
  - DoE
  - measure theory
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - approximation theory
  title: Functional analysis
  type: book
  url: https://books.google.com/books?hl=en&lr=&id=18VqDwAAQBAJ&oi=fnd&pg=PR17&ots=8FU4i_jAff&sig=Lh4GrUCrS_gt-HKqHSq4X7BaxMs#v=onepage&q&f=false
  venue: null
  volume: null
  web: null
  year: 2002
lecun1989backpropagation:
  address: null
  articleno: null
  authors:
  - - Y.
    - LeCun
  - - B.
    - Boser
  - - J. S.
    - Denker
  - - D.
    - Henderson
  - - R. E.
    - Howard
  - - W.
    - Hubbard
  - - L. D.
    - Jackel
  chapter: null
  descrip: LeCun et al. show that convolutional nets can outperform all other methods
    on handwritten digit classification and other perception problems
  doi: 10.1162/neco.1989.1.4.541
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-7667
  month: '12'
  note: null
  number: '4'
  pages:
  - '541'
  - '551'
  publisher: MIT Press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - CNNs
  - neural networks
  - scientific machine learning
  - SciML
  - classification
  title: Backpropagation Applied to Handwritten Zip Code Recognition
  type: article
  url: https://direct.mit.edu/neco/article/1/4/541-551/5515
  venue: Neural Computation
  volume: '1'
  web: null
  year: 1989
lecun1995convolutional:
  address: null
  articleno: null
  authors:
  - - Yann
    - LeCun
  - - Yoshua
    - Bengio
  - - ''
    - others
  chapter: null
  descrip: LeCun's original paper on convolutional neural network architecture --
    the first example of a problem where neural networks trained with backpropagation
    actually outperformed other models, and one of the early examples of the magic
    of representation learning and how exploiting problem structure is essential to
    making machine learning work. This remained the state-of-the art in image classification
    and image processing for over 20 years until the event of transformers, and whether
    transformers are better for image problems remains debatable
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '10'
  pages:
  - 1995
  publisher: Citeseer
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - autograd
  - algorithmic differentiation
  - backpropagation
  - CNNs
  - neural networks
  - scientific machine learning
  - SciML
  - transformers
  - classification
  title: Convolutional networks for images, speech, and time series
  type: article
  url: https://www.cs.utoronto.ca/~bonner/courses/2016s/csc321/readings/Convolutional%20networks%20for%20images,%20speech,%20and%20time%20series.pdf
  venue: The handbook of brain theory and neural networks
  volume: '3361'
  web: null
  year: 1995
lecun1998gradientbased:
  address: null
  articleno: null
  authors:
  - - Yann
    - LeCun
  - - L{\'e}on
    - Bottou
  - - Yoshua
    - Bengio
  - - Patrick
    - Haffner
  chapter: null
  descrip: LeCun et al. show that convolutional nets can outperform all other methods
    on handwriting classification for efficient document processing
  doi: 10.1109/5.726791
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0018-9219
  month: null
  note: null
  number: '11'
  pages:
  - '2278'
  - '2324'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - CNNs
  - neural networks
  - scientific machine learning
  - SciML
  - classification
  title: Gradient-based learning applied to document recognition
  type: article
  url: http://ieeexplore.ieee.org/document/726791/
  venue: Proceedings of the IEEE
  volume: '86'
  web: null
  year: 1998
ledigabel2011algorithm:
  address: null
  articleno: null
  authors:
  - - S\'ebastien
    - Le~Digabel
  chapter: null
  descrip: NOMAD v3 is a widely-used open source numerical software package (in C++)
    for blackbox and derivative-free optimization via the MADS algorithms. Includes
    the official implementations of algorithms such as Ortho-MADS, PSD-MADS, and BiMADS.
    Support for parallel computing and surrogate modeling, and fairly extensible.
    Can be linked as a C++ library, or usage from command line interface. Used by
    a variety of industries and officially supported by Exxon Mobile. Has recently
    been replaced by the major refactor/rewrite in NOMAD v4. Still an example of widely-used
    open source numerical software, but the NOMAD v4 paper gives a look at how open
    source software practices have changed (improved) in the last 10 years
  doi: 10.1145/1916461.1916468
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '2'
  note: null
  number: '4'
  pages:
  - 44
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - mixed-variable optimization
  - surrogate modeling
  - software
  - open source
  - OSS
  - parallel programming
  - C++
  - C
  title: 'Algorithm 909: {NOMAD}: Nonlinear Optimization with the {MADS} Algorithm'
  type: article
  url: https://dl.acm.org/doi/10.1145/1916461.1916468
  venue: ACM Transactions on Mathematical Software
  volume: '37'
  web: null
  year: 2011
ledigabel2024taxonomy:
  address: null
  articleno: null
  authors:
  - - S\'ebastien
    - Le Digabel
  - - Stefan M.
    - Wild
  chapter: null
  descrip: 'A taxonomy of constraint types encountered when solving blackbox / simulation
    optimization: quantifiable vs nonquantifiable, relaxable vs unrelaxable, a priori
    vs simulation-based, and known vs hidden.'
  doi: 10.1007/s11081-023-09839-3
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1389-4420
  month: '6'
  note: null
  number: '2'
  pages:
  - '1125'
  - '1143'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  title: A Taxonomy of Constraints in Black-Box Simulation-Based Optimization
  type: techreport
  url: https://link.springer.com/10.1007/s11081-023-09839-3
  venue: Optimization and Engineering
  volume: '25'
  web: null
  year: 2024
lee2015pydoe:
  address: null
  articleno: null
  authors:
  - - Abraham D.
    - Lee
  - - ''
    - others
  chapter: null
  descrip: pyDOE a popular software repository for the common design-of-experiments
    in Python. This has been widely replaced by the new release of scipy which includes
    scipy.stats.qmc, which includes most of these techniques (used for monte carlo
    sampling, but still the same techniques)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Nov 2022'
  number: 0.3.8
  pages: null
  publisher: GitHub
  series: null
  tags:
  - design of experiments
  - DoE
  - software
  - open source
  - OSS
  - Python
  title: 'py{DOE}: The experimental design package for python'
  type: misc
  url: https://github.com/tisimst/pyDOE
  venue: GitHub repository
  volume: null
  web: null
  year: 2015
lewis2002globally:
  address: null
  articleno: null
  authors:
  - - Robert Michael
    - Lewis
  - - Virginia
    - Torczon
  chapter: null
  descrip: An algorithm that combines direct search / pattern search with an augmented
    Lagrangian penalty term in order to solve a constrained blackbox optimization
    problem
  doi: 10.1137/S1052623498339727
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '4'
  pages:
  - '1075'
  - '1089'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - global optimization
  - constrained optimization
  title: A Globally Convergent Augmented {L}agrangian Pattern Search Algorithm for
    Optimization with General Constraints and Simple Bounds
  type: article
  url: http://epubs.siam.org/doi/10.1137/S1052623498339727
  venue: SIAM Journal on Optimization
  volume: '12'
  web: null
  year: 2002
liang2024amfplacer:
  address: null
  articleno: null
  authors:
  - - Tingyuan
    - Liang
  - - Gengjie
    - Chen
  - - Jieru
    - Zhao
  - - Sharad
    - Sinha
  - - Wei
    - Zhang
  chapter: null
  descrip: Offifial publication for the AMF-Placer 2.0 paper, which is an open-source
    analytical placer using quadratic programming for global placement and simulated
    annealing for detailed placement. Written primarily in C++ and available at github.com/zslwyuan/AMF-Placer
  doi: 10.1109/TCAD.2024.3373357
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0278-0070
  month: '9'
  note: null
  number: '9'
  pages:
  - '2769'
  - '2782'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - C++
  title: '{AMF-Placer 2.0}: Open-Source Timing-Driven Analytical Mixed-Size Placer
    for Large-Scale Heterogeneous {FPGA}'
  type: article
  url: https://ieeexplore.ieee.org/document/10459236/
  venue: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
  volume: '43'
  web: null
  year: 2024
lindauer2022smac3:
  address: null
  articleno: null
  authors:
  - - Marius
    - Lindauer
  - - Katharina
    - Eggensperger
  - - Matthias
    - Feurer
  - - "Andr\xE9"
    - Biedenkapp
  - - Difan
    - Deng
  - - Carolin
    - Benjamins
  - - Tim
    - Ruhkopf
  - - "Ren\xE9"
    - Sass
  - - Frank
    - Hutter
  chapter: null
  descrip: 'The official publication of SMAC3 -- the latest version of SMAC from the
    automl group. SMAC was the first hyperparameter optimization and neural architecture
    search software to use random forest surrogates for modeling the hyperparameter
    configuration space. They also use a hierarchy of hyperparameters to handle "hidden
    parameters". They support multi and single-fidelity NAS applications. The open
    source software is written in Python and available from: github.com/automl/SMAC3'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '54'
  pages:
  - '1'
  - '9'
  publisher: null
  series: null
  tags:
  - optimization
  - decision trees
  - Bayesian optimization
  - global optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - uncertainty quantification
  - UQ
  - software
  - open source
  - OSS
  - Python
  title: '{SMAC3}: A Versatile Bayesian Optimization Package for Hyperparameter Optimization'
  type: article
  url: http://jmlr.org/papers/v23/21-0888.html
  venue: Journal of Machine Learning Research
  volume: '23'
  web: null
  year: 2022
liu2016surrogate:
  address: Vancouver, BC, Canada
  articleno: null
  authors:
  - - Bo
    - Liu
  - - Nan
    - Sun
  - - Qingfu
    - Zhang
  - - Vic
    - Grout
  - - Georges
    - Gielen
  chapter: null
  descrip: An early paper proposing the usage of surrogate models within multiobjective
    evolutionary algorithms in order to improve their performance on computationally
    expensive blackbox / simulation optimization problems, where the function evaluation
    budget may be limited
  doi: 10.1109/cec.2016.7743986
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '1650'
  - '1657'
  publisher: '{IEEE}'
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - surrogate modeling
  - evolutionary algorithms
  - EA
  title: A surrogate model assisted evolutionary algorithm for computationally expensive
    design optimization problems with discrete variables
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/7743986
  venue: Proc. 2016 {IEEE} Congress on Evolutionary Computation ({CEC})
  volume: null
  web: null
  year: 2016
liu2017adaptive:
  address: null
  articleno: null
  authors:
  - - Haitao
    - Liu
  - - Jianfei
    - Cai
  - - Yew-Soon
    - Ong
  chapter: null
  descrip: Adaptive Kriging model-based sampling basically means using an interpolating
    Gaussian process's uncertainty information to select where to sample the next
    point during an adaptive sampling algorithm (for generating design-of-experiments
    or design space exploration).
  doi: 10.1016/j.compchemeng.2017.05.025
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-1354
  month: '11'
  note: null
  number: null
  pages:
  - '171'
  - '182'
  publisher: Elsevier BV
  series: null
  tags:
  - design of experiments
  - DoE
  - adaptive sampling
  - model-based sampling
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - Gaussian process
  title: An adaptive sampling approach for Kriging metamodeling by maximizing expected
    prediction error
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S009813541730234X
  venue: Computers \& Chemical Engineering, Special Section - ESCAPE-26
  volume: '106'
  web: null
  year: 2017
liu2019darts:
  address: null
  articleno: null
  authors:
  - - Hanxiao
    - Liu
  - - Karen
    - Simonyan
  - - Yiming
    - Yang
  chapter: null
  descrip: The DARTS algorithm trains a probability distribution of possible weights,
    layer types, and connection topologies to sample. This creates a continuous relaxation
    of the neural architecture search problem, allowing it to be optimized using derivative-based
    techniques in fewer operations. This allows it to train faster, however, this
    method doesn't work that well in practice because even if it trains nice distributions,
    it generally doesn't actually sample well-performing networks with high probability.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - optimization
  - autotuning
  - hyperparameter optimization
  title: '{DARTS}: Differentiable Architecture Search'
  type: inproceedings
  url: https://openreview.net/forum?id=S1eYHoC5FX
  venue: Proc. 7th International Conference on Learning Representations (ICLR '19)
  volume: null
  web: null
  year: 2019
liu2019nonparametric:
  address: Beijing, China
  articleno: null
  authors:
  - - Yehong
    - Liu
  - - Guosheng
    - Yin
  chapter: null
  descrip: A paper on the Delaunay triangulation learner -- an alternative to ReLU
    multilayer perceptrons where they fit a Delaunay triangulation to a small set
    of nodes and then train the response values of those nodes to make the Delaunay
    interpolant match the data
  doi: 10.1109/icbk.2019.00030
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '11'
  note: null
  number: null
  pages:
  - '167'
  - '174'
  publisher: IEEE
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - interpolation
  - regression
  - classification
  title: Nonparametric functional approximation with {D}elaunay triangulation learner
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8944414
  venue: Proceedings of the 2019 IEEE International Conference on Big Knowledge (ICBK)
  volume: null
  web: null
  year: 2019
liu2024deepseekv3:
  address: null
  articleno: null
  authors:
  - - Aixin
    - Liu
  - - Bei
    - Feng
  - - Bing
    - Xue
  - - Bingxuan
    - Wang
  - - Bochao
    - Wu
  - - Chengda
    - Lu
  - - Chenggang
    - Zhao
  - - Chengqi
    - Deng
  - - Chenyu
    - Zhang
  - - Chong
    - Ruan
  - - ''
    - others
  chapter: null
  descrip: 'DeepSeek-V3 technical report: DeepSeek-V3 is the first open source model
    to obtain performance on par with closed source models. The architecture is transformer
    based with multi-headed latent attention applied to a mixture of experts model.
    The first layer is an embedding layer that is shared by all models and trained
    on the data. Next, the architecture passes through alternating multi-headed latent
    attention and feed-forward (i.e., MLP) layers. The latent attention blocks are
    shared between all experts. Each expert then has its own multi-layer perceptron
    consisting of feed-forward layers with sigmoidal activation functions. Each individual
    expert model is run sequential and used to generate a sequence of multiple output
    tokens (as opposed to just the next word). All experts are run in parallel. The
    predictions from each expert are ultimately combined in an output layer and hit
    with a softmax to generate the logits for the next word prediction(s). During
    training, the objective is the CrossEntropy loss for each prediction depth. The
    authors use model sharding and pipeline paralleism during training. They break
    apart and overlap forward and backward pass computations to create 2 overlapping
    pipelines, which they use to hide latency. The model is trained in FP8 and mixed
    precision. The weight checkpoints are publicly available at: https://github.com/deepseek-ai/DeepSeek-V3
    TODO: read this report carefully'
  doi: 10.48550/arXiv.2412.19437
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2412.19437
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - open source
  - high-performance computing
  - HPC
  - parallel computing
  title: '{DeepSeek-V3} technical report'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2024
liuzzi2016derivativefree:
  address: null
  articleno: null
  authors:
  - - Giampaolo
    - Liuzzi
  - - Stefano
    - Lucidi
  - - Francesco
    - Rinaldi
  chapter: null
  descrip: The DFMO algorithm is a multiobjective line search, which the authors recommend
    combining with MODIR to improve its convergence to the Pareto front after identifying
    the global Pareto front. The open source numerical software is implemented in
    Fortran and is currently bundled inside the MODIR software package on the authors'
    GitHub account
  doi: 10.1137/15M1037810
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '4'
  pages:
  - '2744'
  - '2774'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: A derivative-free approach to constrained multiobjective nonsmooth optimization
  type: article
  url: http://epubs.siam.org/doi/10.1137/15M1037810
  venue: SIAM Journal on Optimization
  volume: '26'
  web: null
  year: 2016
liuzzi2024dfolib:
  address: null
  articleno: null
  authors:
  - - Giampaolo
    - Liuzzi
  - - ''
    - others
  chapter: null
  descrip: The official GitHub account for the DFO-lib -- open source numerical software
    in Fortran for solving blackbox optimization and multiobjective optimization problems.
    The library used to be obtained via download from Liuzzi's personal website, where
    it was referred to as the DFO lib. In 2024 it appears to have been migrated to
    a GitHub account, with each individual piece of software in its own separate repository.
    Therefore, any reference to the DFO-lib must now be directed to the account as
    a whole, not an individual repository.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Jul 2024'
  number: 0.3.8
  pages: null
  publisher: GitHub
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - software
  - open source
  - OSS
  - Fortran
  title: '{DFO-lib}'
  type: misc
  url: https://github.com/DerivativeFreeLibrary
  venue: GitHub repository
  volume: null
  web: null
  year: 2024
louw2021using:
  address: virtual event
  articleno: null
  authors:
  - - Thorben
    - Louw
  - - Simon
    - McIntosh-Smith
  chapter: null
  descrip: Publication describing Graphcore's IPU architecture (intelligence processing
    unit), which is a neuromorphic parallel processor optimized for high-throughput
    vector operations
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1'
  - '9'
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  title: Using the {Graphcore IPU} for traditional {HPC} applications
  type: inproceedings
  url: https://easychair.org/publications/preprint/ztfj
  venue: Proc. 3rd Workshop on Accelerated Machine Learning (AccML)
  volume: null
  web: null
  year: 2021
lovison2021extension:
  address: null
  articleno: null
  authors:
  - - Alberto
    - Lovison
  - - Kaisa
    - Miettinen
  chapter: null
  descrip: Multiobjective extension of the DIRECT algorithm for derivative-free blackbox
    optimization. This algorithm may suffer from some scalability issues, but is a
    good first step
  doi: 10.1007/s10898-020-00942-8
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-5001
  month: '2'
  note: null
  number: '2'
  pages:
  - '387'
  - '412'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  title: On the Extension of the {DIRECT} Algorithm to Multiple Objectives
  type: article
  url: https://link.springer.com/10.1007/s10898-020-00942-8
  venue: Journal of Global Optimization
  volume: '79'
  web: null
  year: 2021
lu2019nsganet:
  address: Prague, Czech Republic
  articleno: null
  authors:
  - - Zhichao
    - Lu
  - - Ian
    - Whalen
  - - Vishnu
    - Boddeti
  - - Yashesh
    - Dhebar
  - - Kalyanmoy
    - Deb
  - - Erik
    - Goodman
  - - Wolfgang
    - Banzhaf
  chapter: null
  descrip: 'NSGA-Net is the NSGA-II/pymoo team''s multiobjective genetic algorithm
    based NAS solver. The open source python software is available from: github.com/ianwhale/nsga-net'
  doi: 10.1145/3321707.3321729
  edition: null
  editors: []
  git: null
  isbn: '9781450361118'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '419'
  - '427'
  publisher: Association for Computing Machinery
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - mixed-variable optimization
  - autotuning
  - hyperparameter optimization
  - evolutionary algorithm
  - EA
  - software
  - open source
  - OSS
  - Python
  title: '{NSGA-Net}: neural architecture search using multi-objective genetic algorithm'
  type: inproceedings
  url: https://doi.org/10.1145/3321707.3321729
  venue: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO
    '19)
  volume: null
  web: null
  year: 2019
lundberg2017unified:
  address: null
  articleno: null
  authors:
  - - Scott M
    - Lundberg
  - - Su-In
    - Lee
  chapter: null
  descrip: Official citation for the open source numerical Python software for evaluating
    SHAP scores for interpreting feature importance in a model agnostic way for various
    machine learning (primarily scientific machine learning) applications. Very popular
    in many fields, including finance, computational medicine, etc. github.com/shap/shap
    Based on the original publication by Shapley + some additional methods and improvements
    proposed by the authors and previous works
  doi: null
  edition: null
  editors:
  - - I.
    - Guyon
  - - U. V.
    - Luxburg
  - - S.
    - Bengio
  - - H.
    - Wallach
  - - R.
    - Fergus
  - - S.
    - Vishwanathan
  - - R.
    - Garnett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '4765'
  - '4774'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  - regression
  - classification
  - software
  - open source
  - OSS
  - Python
  title: A Unified Approach to Interpreting Model Predictions
  type: inproceedings
  url: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf
  venue: Advances in Neural Information Processing Systems 30
  volume: null
  web: null
  year: 2017
luo2020preexascale:
  address: null
  articleno: null
  authors:
  - - L.
    - Luo
  - - T.
    - P. Straatsma
  - - L. E. Aguilar
    - Suarez
  - - R.
    - Broer
  - - D.
    - Bykov
  - - E.
    - F. D'Azevedo
  - - S.
    - S. Faraji
  - - K.
    - C. Gottiparthi
  - - C.
    - De Graaf
  - - J.
    - A. Harris
  - - R. W.
    - A. Havenith
  - - H. J. Aa.
    - Jensen
  - - W.
    - Joubert
  - - R.
    - K. Kathir
  - - J.
    - Larkin
  - - Y.
    - W. Li
  - - D.
    - I. Lyakh
  - - O. E.
    - B. Messer
  - - M.
    - R. Norman
  - - J.
    - C. Oefelein
  - - R.
    - Sankaran
  - - A.
    - F. Tillack
  - - A.
    - L. Barnes
  - - L.
    - Visscher
  - - J.
    - C. Wells
  - - M.
    - Wibowo
  chapter: null
  descrip: Summary of discussions from Oak Ridge National Laboratory Summit discussing
    GPU-based architectures and other developments from the exascale computing project
  doi: 10.1147/JRD.2020.2965881
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0018-8646
  month: '5'
  note: null
  number: 3/4
  pages:
  - '11:1'
  - '11:21'
  publisher: IBM
  series: null
  tags:
  - high-performance computing
  - HPC
  - GPU computing
  - parallel computing
  - distributed computing
  title: 'Pre-exascale accelerated application development: The {ORNL Summit} experience'
  type: article
  url: https://ieeexplore.ieee.org/document/8960361
  venue: IBM Journal of Research and Development
  volume: '64'
  web: null
  year: 2020
luu2011vpr:
  address: New York, NY, USA
  articleno: '32'
  authors:
  - - Jason
    - Luu
  - - Ian
    - Kuon
  - - Peter
    - Jamieson
  - - Ted
    - Campbell
  - - Andy
    - Ye
  - - Wei Mark
    - Fang
  - - Kenneth
    - Kent
  - - Jonathan
    - Rose
  chapter: null
  descrip: Latest version of VPR, documenting new features and support -- very little
    mention of changes (if any) to the actual placement algorithms though
  doi: 10.1145/2068716.2068718
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1936-7406
  month: '12'
  note: null
  number: '4'
  pages:
  - 23
  publisher: Association for Computing Machinery
  series: null
  tags:
  - FPGA
  - high-performance computing
  - HPC
  - software
  - open source
  - OSS
  - C++
  - Verilog
  title: '{VPR} 5.0: {FPGA CAD} and architecture exploration tools with single-driver
    routing, heterogeneity and process scaling'
  type: article
  url: https://doi.org/10.1145/2068716.2068718
  venue: ACM Trans. Reconfigurable Technol. Syst.
  volume: '4'
  web: null
  year: 2011
lux2018nonparametric:
  address: St. Petersburg, FL, USA
  articleno: null
  authors:
  - - Thomas C. H.
    - Lux
  - - Layne T.
    - Watson
  - - Tyler H.
    - Chang
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Xiadong
    - Yu
  - - Li
    - Xu
  - - Godmar
    - Back
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: Fitting nonparametric distribution models by interpolating the cumulative
    distribution functions -- the application is that the CDFs measure HPC throughput
    distributions, so it is also a little bit of a HPC performance modeling paper
  doi: 10.1109/secon.2018.8478814
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '4'
  note: null
  number: null
  pages:
  - '1'
  - '7'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - performance modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - interpolation
  - uncertainty quantification
  - UQ
  title: Nonparametric distribution models for predicting and managing computational
    performance variability
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8478814
  venue: Proceedings of IEEE SoutheastCon 2018
  volume: null
  web: null
  year: 2018
lux2018novel:
  address: Richmond, KY, USA
  articleno: '13'
  authors:
  - - Thomas C. H.
    - Lux
  - - Layne T.
    - Watson
  - - Tyler H.
    - Chang
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Xiadong
    - Yu
  - - Li
    - Xu
  - - Godmar
    - Back
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Danfeng
    - Yao
  - - Yili
    - Hong
  chapter: null
  descrip: Thomas' thorough survey paper on useful meshes and their properties for
    multivariate interpolation
  doi: 10.1145/3190645.3190687
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '3'
  note: null
  number: null
  pages:
  - 7
  publisher: Association of Computing Machinery
  series: null
  tags:
  - computational geometry
  - interpolation
  - Delaunay triangulation
  - design of experiments
  - DoE
  title: Novel meshes for multivariate interpolation and approximation
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3190645.3190687
  venue: Proc. 2018 ACM Southeast Conference (ACMSE '18)
  volume: null
  web: null
  year: 2018
lux2018predictive:
  address: Baltimore, MD, USA
  articleno: '8'
  authors:
  - - Thomas C. H.
    - Lux
  - - Layne T.
    - Watson
  - - Tyler H.
    - Chang
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Li
    - Xu
  - - Godmar
    - Back
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: Using various interpolation, neural networks, and other scientific machine
    learning methods to predict and model HPC performance based on system configuration
    parameters -- explores Delaunay interpolation, support vector regressors, Shepard's
    method, and multilayer perceptrons
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: SCS
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - high-performance computing
  - HPC
  - performance modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - regression
  title: Predictive modeling of {I/O} characteristics in high performance computing
    systems
  type: inproceedings
  url: https://par.nsf.gov/servlets/purl/10111447
  venue: Proceedings of the 2018 Spring Simulation Conference (SpringSim 2018), the
    26th High Performance Computing Symposium (HPC '18)
  volume: null
  web: null
  year: 2018
lux2020analytic:
  address: Raleigh, NC, USA
  articleno: null
  authors:
  - - Thomas C. H.
    - Lux
  - - Tyler H.
    - Chang
  chapter: null
  descrip: Our paper proposing test functions for convex optimization problems with
    very specific properties, in order to gauge the effectiveness and robustness of
    various techniques subject to various forms of degeneracy
  doi: 10.1109/SoutheastCon44009.2020.9368254
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '3'
  note: null
  number: null
  pages:
  - 8
  publisher: Institute of Electrical and Electronics Engineers
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - benchmarking
  - open source
  - optimization
  - convex optimization
  - scientific machine learning
  - SciML
  - high dimension
  title: Analytic test functions for generalizable evaluation of convex optimization
    techniques
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9368254/
  venue: Proc. IEEE SoutheastCon 2020
  volume: null
  web: null
  year: 2020
lux2021interpolation:
  address: null
  articleno: null
  authors:
  - - Thomas C. H.
    - Lux
  - - Layne T.
    - Watson
  - - Tyler H.
    - Chang
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Li
    - Xu
  - - Godmar
    - Back
  - - Ali R.
    - Butt
  - - Kirk W.
    - Cameron
  - - Yili
    - Hong
  chapter: null
  descrip: Error bounds for piecewise linear interpolation within a simplex (i.e.,
    Delaunay interpolation error bounds) for scattered data. Also, critically, empirical
    evidence that you can just interpolate noisy data and it does just as well as
    true regression in high dimensions. First hand evidence that feeds my opinion
    that "overfitting is a lie"
  doi: 10.1007/s11075-020-01040-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1017-1398
  month: '9'
  note: null
  number: '1'
  pages:
  - '281'
  - '313'
  publisher: Springer
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - high dimension
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - overfitting
  - regression
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Interpolation of sparse high-dimensional data
  type: article
  url: https://link.springer.com/10.1007/s11075-020-01040-2
  venue: Numerical Algorithms
  volume: '88'
  web: null
  year: 2021
lux2023algorithm:
  address: null
  articleno: '6'
  authors:
  - - Thomas C. H.
    - Lux
  - - Layne T.
    - Watson
  - - Tyler H.
    - Chang
  - - William I.
    - Thacker
  chapter: null
  descrip: Thomas' open source montone quintic spline interpolation software -- a
    high order spline interpolant implemented in modern Fortran. Really nice high
    performance numerical software. Available open source on Dr. Watson's GitHub.
    His main application was distribution modeling, but I think for noisy data, lower
    order methods may actually be more robust
  doi: 10.1145/3570157
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '3'
  note: null
  number: '1'
  pages:
  - 17
  publisher: Association of Computing Machinery
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - interpolation
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 1031: {MQSI}---{M}onotone quintic spline interpolation'
  type: article
  url: https://dl.acm.org/doi/10.1145/3570157
  venue: ACM Transactions on Mathematical Software
  volume: '49'
  web: null
  year: 2023
mackay1992practical:
  address: Cambridge, MA, USA
  articleno: null
  authors:
  - - David JC
    - MacKay
  chapter: null
  descrip: Original paper on Bayesian neural networks (or just Bayesian networks),
    which learn joint-probability distributions of neural network weights and biases
    instead of just single sets. The original proposal was as a means of regularization,
    to avoid overfitting. However, sampling networks from this distribution can also
    be used as a means of uncertainty quantification.
  doi: 10.1162/neco.1992.4.3.448
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-7667
  month: '5'
  note: null
  number: '3'
  pages:
  - '448'
  - '472'
  publisher: MIT Press
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - Gaussian process
  - regularization
  - overfitting
  - regression
  - classification
  - uncertainty quantification
  - UQ
  title: A practical Bayesian framework for backpropagation networks
  type: article
  url: https://direct.mit.edu/neco/article/4/3/448-472/5654
  venue: Neural computation
  volume: '4'
  web: null
  year: 1992
maidee2005timingdriven:
  address: null
  articleno: null
  authors:
  - - P.
    - Maidee
  - - C.
    - Ababei
  - - K.
    - Bazargan
  chapter: null
  descrip: Partition-based placement general algorithms and some possible implementation
    details
  doi: 10.1109/TCAD.2004.842812
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0278-0070
  month: '3'
  note: null
  number: '3'
  pages:
  - '395'
  - '406'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  title: Timing-driven partitioning-based placement for island style {FPGAs}
  type: article
  url: http://ieeexplore.ieee.org/document/1397800/
  venue: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
  volume: '24'
  web: null
  year: 2005
mannor2014approachability:
  address: Barcelona, Spain
  articleno: null
  authors:
  - - Shie
    - Mannor
  - - Vianney
    - Perchet
  - - Gilles
    - Stoltz
  chapter: null
  descrip: Discussion of online learning in the context of multiobjective optimization
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: 13--15 June
  note: null
  number: null
  pages:
  - '339'
  - '355'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - optimization
  - multiobjective optimization
  title: 'Approachability in unknown games: {O}nline learning meets multi-objective
    optimization'
  type: inproceedings
  url: https://proceedings.mlr.press/v35/mannor14.html
  venue: Proc. 27th Conference on Learning Theory (PMLR)
  volume: '35'
  web: null
  year: 2014
manton2015primer:
  address: null
  articleno: null
  authors:
  - - Jonathan H
    - Manton
  - - Pierre-Olivier
    - Amblard
  - - ''
    - others
  chapter: null
  descrip: A thorough foundation on analysis of RBF interpolant error bounds, through
    the lens of the RKHS
  doi: 10.1561/2000000050
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: 1--2
  pages:
  - '1'
  - '126'
  publisher: Now Publishers, Inc.
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - interpolation
  - approximation theory
  title: A primer on reproducing kernel {H}ilbert spaces
  type: article
  url: null
  venue: Foundations and Trends{\textregistered} in Signal Processing
  volume: '8'
  web: null
  year: 2015
marler2004survey:
  address: null
  articleno: null
  authors:
  - - Timothy R.
    - Marler
  - - Jasbir S.
    - Arora
  chapter: null
  descrip: Survey of common multiobjective optimization algorithms and scalarization
    techniques in the context of engineering design optimization
  doi: 10.1007/s00158-003-0368-6
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1615-147X
  month: '4'
  note: null
  number: '6'
  pages:
  - '369'
  - '395'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - scalarization
  title: Survey of multi-objective optimization methods for engineering
  type: article
  url: http://link.springer.com/10.1007/s00158-003-0368-6
  venue: Structural and Multidisciplinary Optimization
  volume: '26'
  web: null
  year: 2004
martens2013asynchronous:
  address: Amsterdam, The Netherlands
  articleno: null
  authors:
  - - Marcus
    - M{\"a}rtens
  - - Dario
    - Izzo
  chapter: null
  descrip: This is a modification to the NSGA-II multiobjective optimization algorithm
    where the authors introduce a pipeline of smaller "islands" of populations, which
    evolve independently. Cross migration between these islands keeps the populations
    from diverging too far apart and allowing for progress to be shared between all
    islands through eventual consistency. This modification allows NSGA-II to run
    fully asynchronously. This is the version used by Optuna
  doi: 10.1145/2463372.2463516
  edition: null
  editors: []
  git: null
  isbn: '9781450319638'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1173'
  - '1180'
  publisher: Association for Computing Machinery
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - optimization
  - multiobjective optimization
  - evolutionary algorithm
  - EA
  title: 'The asynchronous island model and {NSGA-II}: study of a new migration operator
    and its performance'
  type: inproceedings
  url: https://doi.org/10.1145/2463372.2463516
  venue: Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation
    (GECCO '13)
  volume: null
  web: null
  year: 2013
martins2009pymdo:
  address: null
  articleno: null
  authors:
  - - Joaquim R. R. A.
    - Martins
  - - Christopher
    - Marriage
  - - Nathan
    - Tedford
  chapter: null
  descrip: 'pyMDO: an open source numerical Python framework for modeling and solving
    multidisciplinary engineering design optimization problems'
  doi: 10.1145/1555386.1555389
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '8'
  note: null
  number: '4'
  pages:
  - 20
  publisher: ACM
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - software
  - open source
  - OSS
  - Python
  title: '{pyMDO}: An Object-Oriented Framework for Multidisciplinary Design Optimization'
  type: article
  url: https://dl.acm.org/doi/10.1145/1555386.1555389
  venue: ACM Transactions on Mathematical Software
  volume: '36'
  web: null
  year: 2009
mayes2023pysuperfish:
  address: null
  articleno: null
  authors:
  - - Christopher
    - Mayes
  chapter: null
  descrip: Python wrapper around the open source numerical simulation software superfish
    We used this when performing particle accelerator RF gun cavity optimization at
    Argonne in order to provide a bridge between POISSON/SUPERFISH (Fortran codes)
    and the ParMOO optimizer (a Python code)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Aug 2023'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - software
  - open source
  - OSS
  - Fortran
  - Python
  title: PySuperfish
  type: misc
  url: https://github.com/ChristopherMayes/PySuperfish
  venue: null
  volume: null
  web: null
  year: 2023
megiddo1991finding:
  address: null
  articleno: null
  authors:
  - - Nimrod
    - Megiddo
  chapter: null
  descrip: Algorithms and theorems on the difficulty of finding basic solutions for
    linear programming problems
  doi: 10.1287/ijoc.3.1.63
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-1499
  month: '2'
  note: null
  number: '1'
  pages:
  - '63'
  - '65'
  publisher: Institute for Operations Research and the Management Sciences (INFORMS)
  series: null
  tags:
  - computational geometry
  - algorithms
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  title: On finding primal- and dual-optimal bases
  type: article
  url: https://pubsonline.informs.org/doi/10.1287/ijoc.3.1.63
  venue: ORSA Journal on Computing
  volume: '3'
  web: null
  year: 1991
menzel1987users:
  address: Los Alamos, NM, USA
  articleno: null
  authors:
  - - M. T.
    - Menzel
  - - H. K.
    - Stokes
  chapter: null
  descrip: The POISSON/SUPERFISH open source numerical software (in Fortran) for calculating
    static, magnetic, and electric fields and rf electromagnetic fields. Used to design
    particle accelerators, developed and maintained by Los Alamos
  doi: 10.2172/10140823
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '1'
  note: null
  number: LA-UR-87-115
  pages: null
  publisher: Los Alamos National Laboratory
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - software
  - open source
  - OSS
  - Fortran
  title: Users guide for the {POISSON/SUPERFISH} group of codes
  type: techreport
  url: https://www.osti.gov/servlets/purl/10140823
  venue: null
  volume: null
  web: null
  year: 1987
mikolov2013efficient:
  address: Scottsdale, AZ, USA
  articleno: null
  authors:
  - - Tomas
    - Mikolov
  - - Kai
    - Chen
  - - Greg
    - Corrado
  - - Jeffrey
    - Dean
  chapter: null
  descrip: 'Original word2vec publication: embedding words into a continuous vector
    space in a "dense" way, meaning that the embedding is somewhat minimal and no
    extra dimensions are added and word relationships are preserved (as in semantic
    regularization). The authors show that a relatively small and simple model with
    semantic regularization is sufficient to train such an embedding in less than
    a day, and the resulting word2vec embedding layer is fast and cheap to evaluate.
    The word2vec embedding would go on to be the basis for most early language models,
    until being replaced by BPE for transformer models and modern LLMs. The software
    is available for download from Google: https://code.google.com/archive/p/word2vec'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - open source
  - neural networks
  - regularization
  - scientific machine learning
  - SciML
  - software
  title: Efficient estimation of word representations in vector space
  type: inproceedings
  url: https://openreview.net/forum?id=idpCdOWtqXd60&noteld=C8Vn84fq
  venue: 1st International Conference on Learning Representations (ICLR 2013)
  volume: null
  web: null
  year: 2013
mikolov2013linguistic:
  address: null
  articleno: null
  authors:
  - - Tom{\'a}{\v{s}}
    - Mikolov
  - - Wen-tau
    - Yih
  - - Geoffrey
    - Zweig
  chapter: null
  descrip: Introduces the idea of semantic regularization for generating word embeddings.
    This is a massive improvement over bag of word embeddings, where each word gets
    a dimension and sentences / documents are embedded as vectors listing the number
    of occurrences for each word. Instead, for this work, words are compressed into
    a continuous latent space using a RNN-based compression model. In the compression,
    the embedding is regularized by relationships between words. For example, embedding(king):embedding(queen)
    ~ embedding(man):embedding(woman) and embedding(clothes):embedding(shirt) ~ embedding(dishes):embedding(bowl).
    These relationships are enforced using the cosin similarity score of their embeddings.
    The authors also collected a massive dataset of english words and the corresponding
    relationships, and successfully trained the first word to vector embedding that
    (mostly) preserves these relationships, meaning that all vector operations remain
    meaningful
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '746'
  - '751'
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - neural networks
  - regularization
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  title: Linguistic regularities in continuous space word representations
  type: inproceedings
  url: https://aclanthology.org/N13-1090.pdf
  venue: 'Proceedings of the 2013 conference of the north american chapter of the
    association for computational linguistics: Human language technologies'
  volume: null
  web: null
  year: 2013
miller2008linearsize:
  address: Montr{\'e}al, Qu{\'e}bec
  articleno: null
  authors:
  - - G.
    - Miller
  - - T.
    - Phillips
  - - D.
    - Sheehy
  chapter: null
  descrip: How to construct meshes whose size only grows linearly (expected space)
    with the dimension
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '175'
  - '178'
  publisher: null
  series: null
  tags:
  - computational geometry
  - algorithms
  - high dimension
  title: Linear-size meshes
  type: inproceedings
  url: null
  venue: Proceedings of the 20th Canadian Conference on Computational Geometry (CCCG
    2008)
  volume: null
  web: null
  year: 2008
mills2021toward:
  address: null
  articleno: null
  authors:
  - - Richard Tran
    - Mills
  - - Mark F.
    - Adams
  - - Satish
    - Balay
  - - Jed
    - Brown
  - - Alp
    - Dener
  - - Matthew
    - Knepley
  - - Scott E.
    - Kruger
  - - Hannah
    - Morgan
  - - Todd
    - Munson
  - - Karl
    - Rupp
  - - Barry F.
    - Smith
  - - Stefano
    - Zampini
  - - Hong
    - Zhang
  - - Junchao
    - Zhang
  chapter: null
  descrip: Report on challenges and experiences gained in porting PETSc to GPUs for
    the exascale computing project
  doi: 10.1016/j.parco.2021.102831
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0167-8191
  month: '12'
  note: null
  number: null
  pages:
  - 102831
  publisher: Elsevier BV
  series: null
  tags:
  - high-performance computing
  - HPC
  - GPU computing
  - simulation
  - computational linear algebra
  - software
  - CUDA
  - C++
  title: Toward performance-portable {PETSc} for {GPU}-based exascale systems
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S016781912100079X
  venue: Parallel Computing
  volume: '108'
  web: null
  year: 2021
misitano2021desdeo:
  address: null
  articleno: null
  authors:
  - - Giovanni
    - Misitano
  - - Bhupinder S.
    - Saini
  - - Bekir
    - Afsar
  - - Babooshka
    - Shavazipour
  - - Kaisa
    - Miettinen
  chapter: null
  descrip: DESDEO an open source Python framework for implementing interactive multiobjective
    optimization solvers
  doi: 10.1109/ACCESS.2021.3123825
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2169-3536
  month: null
  note: null
  number: null
  pages:
  - '148277'
  - '148295'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - software
  - open source
  - OSS
  - Python
  title: '{DESDEO}: The Modular and Open Source Framework for Interactive Multiobjective
    Optimization'
  type: article
  url: https://ieeexplore.ieee.org/document/9591595
  venue: IEEE Access
  volume: '9'
  web: null
  year: 2021
mobius1827der:
  address: Leipzig, Germany
  articleno: null
  authors:
  - - August Ferdinand
    - M\"obius
  chapter: null
  descrip: Original reference for barycentric weight based interpolation -- I have
    not read this, it's in German, but this is the proper way to cite barycentric
    interpolation
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1'
  - '388'
  publisher: Verlag von Johann Ambrosius Barth
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - interpolation
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  title: Der barycentrische Calcul
  type: book
  url: https://books.google.com/books?id=eFPluv_UqFEC&printsec=frontcover#v=onepage&q&f=false
  venue: null
  volume: null
  web: null
  year: 1827
more2009benchmarking:
  address: null
  articleno: null
  authors:
  - - Jorge J.
    - Mor\'{e}
  - - Stefan M.
    - Wild
  chapter: null
  descrip: Introduction of performance profiles for benchmarking derivative-free and
    blackbox optimization algorithms and solvers. In particular, when comparing a
    set of optimization algorithms on a set of problems, we first calculate for each
    problem and solver the ratio of the value of some performance metric for that
    solver on that problem / the best observed value of that metric for all solvers.
    Then, we rank the solvers according to the fraction of problems where their performance
    is at least alpha. (Sort of an h-index like ranking).
  doi: 10.1137/080724083
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '1'
  pages:
  - '172'
  - '191'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - benchmarking
  title: Benchmarking Derivative-Free Optimization Algorithms
  type: article
  url: http://epubs.siam.org/doi/10.1137/080724083
  venue: SIAM Journal on Optimization
  volume: '20'
  web: null
  year: 2009
moriwaki2018mordred:
  address: null
  articleno: '4'
  authors:
  - - Hirotomo
    - Moriwaki
  - - Yu-Shi
    - Tia
  - - Norihito
    - Kawashita
  - - Tatsuya
    - Takagi
  chapter: null
  descrip: 'MORDRED: A 3D molecular descriptor calculator, which is widely used for
    embedding molecules into a continuous latent space (parameterized by their descriptors)
    which can be used to solve chemical property optimization problems. The MORDRED
    software is available open source in Python.'
  doi: 10.1186/s13321-018-0258-y
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1758-2946
  month: '12'
  note: null
  number: '1'
  pages:
  - 14
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - simulation optimization
  - mixed-variable optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - representation learning
  - software
  - open source
  - OSS
  - Python
  title: 'Mordred: a molecular descriptor calculator'
  type: article
  url: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y
  venue: Journal of Cheminformatics
  volume: '10'
  web: null
  year: 2018
mucke1999fast:
  address: null
  articleno: null
  authors:
  - - Ernst P.
    - M{\"u}cke
  - - Isaac
    - Saias
  - - Binhai
    - Zhu
  chapter: null
  descrip: An early paper on simplex walks for locating which element of a Delaunay
    triangulation contains a point (after the triangulation has already been precomputed
    and stored)
  doi: 10.1016/S0925-7721(98)00035-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - '63'
  - '83'
  publisher: null
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: Fast randomized point location without preprocessing in two- and three-dimensional
    {D}elaunay triangulations
  type: article
  url: https://www.osti.gov/biblio/231593
  venue: Computational Geometry
  volume: '12'
  web: null
  year: 1999
muller2017socemo:
  address: null
  articleno: null
  authors:
  - - Juliane
    - M{\"u}ller
  chapter: null
  descrip: 'SOCEMO: A response surface modeling (RSM) based algorithm for solving
    multiobjective optimization problems. Uses a Latin hypercube design-of-experiments,
    RBF surrogate modeling, multiple scalarizations, and solves the scalarized subproblem
    via evolutionary algorithms to produce a batch of evaluations in each iteration
    of the algorithm'
  doi: 10.1287/ijoc.2017.0749
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1091-9856
  month: '11'
  note: null
  number: '4'
  pages:
  - '581'
  - '596'
  publisher: Institute for Operations Research and the Management Sciences (INFORMS)
  series: null
  tags:
  - design of experiments
  - DoE
  - Latin hypercube sampling
  - LHS
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - scalarization
  - evolutionary algorithms
  - EA
  - RBFs
  title: '{SOCEMO}: {S}urrogate optimization of computationally expensive multiobjective
    problems'
  type: article
  url: https://pubsonline.informs.org/doi/10.1287/ijoc.2017.0749
  venue: INFORMS Journal on Computing
  volume: '29'
  web: null
  year: 2017
munson2015tao:
  address: Lemont, IL, USA
  articleno: null
  authors:
  - - Todd
    - Munson
  - - Jason
    - Sarich
  - - Stefan
    - Wild
  - - Steven
    - Benson
  - - Lois Curfman
    - McInnes
  chapter: null
  descrip: This is the last TAO (toolkit for advanced optimization) reference before
    this open source numerical simulation optimization software when merged with PETSc,
    into a single PETSc + TAO release
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: ANL/MCS-TM-322 version 3.5
  pages: null
  publisher: Argonne National Laboratory
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - convex optimization
  - software
  - open source
  - OSS
  - C++
  title: '{TAO} 3.5 Users Manual'
  type: techreport
  url: https://www.mcs.anl.gov/petsc/petsc-3.5.4/docs/tao_manual.pdf
  venue: null
  volume: null
  web: null
  year: 2015
murray2020vtr:
  address: New York, NY, USA
  articleno: '9'
  authors:
  - - Kevin E.
    - Murray
  - - Oleg
    - Petelin
  - - Sheng
    - Zhong
  - - Jia Min
    - Wang
  - - Mohamed
    - Eldafrawy
  - - Jean-Philippe
    - Legault
  - - Eugene
    - Sha
  - - Aaron G.
    - Graham
  - - Jean
    - Wu
  - - Matthew J. P.
    - Walker
  - - Hanqing
    - Zeng
  - - Panagiotis
    - Patros
  - - Jason
    - Luu
  - - Kenneth B.
    - Kent
  - - Vaughn
    - Betz
  chapter: null
  descrip: 'The official publication for the latest version of the VTR (verilog-to-routing)
    toolkit. The latest publication focuses on updates to the user interface and new
    features and support, but not algorithmic changes. VTR is currently the standard
    in open source placement and routing software, written primarilly in C++ with
    a Python interface, available for download at: github.com/verilog-to-routing/vtr-verilog-to-routing'
  doi: 10.1145/3388617
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1936-7406
  month: '6'
  note: null
  number: '2'
  pages:
  - 55
  publisher: Association for Computing Machinery
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - C++
  - Verilog
  title: '{VTR} 8: High-performance {CAD} and Customizable {FPGA} Architecture Modelling'
  type: article
  url: https://doi.org/10.1145/3388617
  venue: ACM Trans. Reconfigurable Technol. Syst.
  volume: '13'
  web: null
  year: 2020
myers2016response:
  address: Hoboken, NJ, USA
  articleno: null
  authors:
  - - Raymond H.
    - Myers
  - - Douglas C.
    - Montgomery
  - - Christine M.
    - Anderson-Cook
  chapter: null
  descrip: 'The classical textbook on response surface methodology and modeling practices.
    Contains useful information on the basic framework and applications of RSM. Also
    a useful reference for many of the options for specific techniques: Chapter 7
    is a good reference for basic techniques in multiobjective RSM and Chapters 8-9
    surveys the basic methods in design-of-experiments'
  doi: null
  edition: '4'
  editors: []
  git: null
  isbn: '9781118916032'
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: John Wiley \& Sons, Inc.
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - surrogate modeling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - regression
  title: 'Response Surface Methodology: Process and Design Optimization Using Designed
    Experiments'
  type: book
  url: https://books.google.com/books?hl=en&lr=&id=T-BbCwAAQBAJ&oi=fnd&pg=PR13&dq=Response+Surface+Methodology:+Process+and+Product+Optimization+Using+Designed+Experiments,+4th+Edition&ots=O3jdPna83T&sig=IimJlE46JBVkHOu7eik3RN9Z5GA#v=onepage&q=Response%20Surface%20Methodology%3A%20Process%20and%20Product%20Optimization%20Using%20Designed%20Experiments%2C%204th%20Edition&f=false
  venue: null
  volume: null
  web: null
  year: 2016
nair2010rectified:
  address: Haifa, Israel
  articleno: null
  authors:
  - - Vinod
    - Nair
  - - Geoffrey E.
    - Hinton
  chapter: null
  descrip: Paper where ReLU functions were first used to replace sigmoidal activations
    in a restricted Boltzmann machine with much success. After this, ReLU became the
    standard activation function in all neural network regressive models
  doi: null
  edition: null
  editors: []
  git: null
  isbn: '9781605589077'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '807'
  - '814'
  publisher: Omnipress
  series: ICML'10
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - Boltzmann machine
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: Rectified linear units improve restricted {B}oltzmann machines
  type: inproceedings
  url: https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf
  venue: Proceedings of the 27th International Conference on International Conference
    on Machine Learning
  volume: null
  web: null
  year: 2010
nesterov1983method:
  address: null
  articleno: null
  authors:
  - - Yurii
    - Nesterov
  chapter: null
  descrip: 'Original publication on Nesterov''s momentum. I haven''t read it (it is
    hard to find a copy and likely in Russian) but this is the preferred citation.
    The equation for Nesterov momentum in gradient descent is instead of using the
    update: x'' = x - a*g(x), use x'' = x - a*g(y) - b*v where y = x - b*v and v =
    b*v + a*g(x) -- in this equation, b*v is the momentum term which smooths out poor
    conditioning in the problem by encouraging the algorithm to continue in the direction
    it was headed instead of oscillating. Nesterov proves that this term also leads
    to better convergence rates. For best results, b is usually chosen to be a large
    value such as 0.9 or 0.99'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '3'
  pages:
  - '543'
  - '547'
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - optimization
  - stochastic optimization
  - convex optimization
  - scientific machine learning
  - SciML
  title: A method for solving the convex programming problem with convergence rate
    $O(1/k^2)$
  type: article
  url: https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng
  venue: Dokl. Akad. Nauk SSSR
  volume: '269'
  web: null
  year: 1983
neveu2023comparison:
  address: null
  articleno: '108566'
  authors:
  - - Nicole
    - Neveu
  - - Tyler H.
    - Chang
  - - Paris
    - Franz
  - - Stephen
    - Hudson
  - - Jeffrey
    - Larson
  chapter: null
  descrip: An application of VTMOP for the multiobjective optimization (tuning) of
    the LCLS-II photoinjector (linear accelerator at SLAC). Had to use some hacks
    to get VTMOP to work, such as penalizing bad regions of the Pareto front, but
    ultimately performed better than NSGA-II
  doi: 10.1016/j.cpc.2022.108566
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0010-4655
  month: '2'
  note: null
  number: null
  pages:
  - 10
  publisher: Elsevier BV
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - evolutionary algorithm
  - EA
  title: Comparison of multiobjective optimization methods for the {LCLS-II} photoinjector
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0010465522002855
  venue: Computer Physics Communication
  volume: '283'
  web: null
  year: 2023
ng2004feature:
  address: Banff, Alberta, Canada
  articleno: null
  authors:
  - - Andrew Y.
    - Ng
  chapter: null
  descrip: Not an original paper, as L1 and L2 regularization have been around since
    the beginning of machine learning and applied math, and are generally not attributed
    to anyone in particular, but a nice review paper on the usage and effects of L1
    and L2 regularization in machine learning and neural network training with back
    propagation. Regularization was considered an essential part of neural network
    training (and still is in scientific machine learning) for many many years.
  doi: 10.1145/1015330.1015435
  edition: null
  editors: []
  git: null
  isbn: '1581138385'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 78
  publisher: Association for Computing Machinery
  series: ICML '04
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - regularization
  - overfitting
  - scientific machine learning
  - SciML
  - dimension reduction
  - regression
  - classification
  title: Feature selection, L1 vs. L2 regularization, and rotational invariance
  type: inproceedings
  url: https://doi.org/10.1145/1015330.1015435
  venue: Proceedings of the Twenty-First International Conference on Machine Learning
  volume: null
  web: null
  year: 2004
niederreiter1988lowdiscrepancy:
  address: null
  articleno: null
  authors:
  - - Harald
    - Niederreiter
  chapter: null
  descrip: The original publication where Sobol's sequences are generalized to the
    class now known as low-discrepancy sequenes
  doi: 10.1016/0022-314X(88)90025-X
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-314X
  month: '9'
  note: null
  number: '1'
  pages:
  - '51'
  - '70'
  publisher: Elsevier BV
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  - Sobol sequence
  title: Low-discrepancy and low-dispersion sequences
  type: article
  url: https://doi.org/10.1016/0022-314X(88)90025-X
  venue: Journal of Number Theory
  volume: '30'
  web: null
  year: 1988
nocedal2006numerical:
  address: New York, NY, USA
  articleno: null
  authors:
  - - Jorge
    - Nocedal
  - - Stephen J.
    - Wright
  chapter: null
  descrip: The classic textbook by Nocedal on fundamental techniques in nonlinear
    programming, such as local modeling and trust-region methods
  doi: 10.1007/978-0-387-40065-5
  edition: '2'
  editors: []
  git: null
  isbn: '9780387303031'
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer Verlag
  series: Springer Series in Operations Research and Financial Engineering
  tags:
  - optimization
  - convex optimization
  - constrained optimization
  - stochastic optimization
  title: Numerical Optimization
  type: book
  url: http://link.springer.com/10.1007/978-0-387-40065-5
  venue: Springer Series in Operations Research and Financial Engineering
  volume: null
  web: null
  year: 2006
nvidiacorporation2025cublas:
  address: null
  articleno: null
  authors:
  - - ''
    - NVIDIA~Corporation
  chapter: null
  descrip: NVIDIA's official API docs for the latest version (as of Apr 2025) of the
    cuBLAS API. With both C and Fortran bindings.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 26, 2025'
  number: Release 12.8
  pages: null
  publisher: NVIDIA Corporation
  series: null
  tags:
  - high-performance computing
  - HPC
  - GPU computing
  - CUDA
  - computational linear algebra
  - software
  - C
  - Fortran
  title: '{cuBLAS API}'
  type: techreport
  url: https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf
  venue: null
  volume: null
  web: null
  year: 2025
nvidiahpccompilers2025cuda:
  address: null
  articleno: null
  authors:
  - - ''
    - NVIDIA~HPC~Compilers
  chapter: null
  descrip: NVIDIA's official developer's guide and API for writing high-performance
    CUDA in Fortran95+ on compilers that support the CUDA API, such as Nvidia's nvcc
    family of compilers and Portland group's pgfrotran family.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 26, 2023'
  number: Version 23.3
  pages: null
  publisher: NVIDIA Corporation
  series: null
  tags:
  - high-performance computing
  - HPC
  - GPU computing
  - CUDA
  - software
  - Fortran
  title: '{CUDA Fortran} programming guide'
  type: techreport
  url: https://docs.nvidia.com/hpc-sdk/archive/23.3/pdf/hpc233cudaforug.pdf
  venue: null
  volume: null
  web: null
  year: 2025
odonoghue2016conic:
  address: null
  articleno: null
  authors:
  - - Brendan
    - O'Donoghue
  - - Eric
    - Chu
  - - Neal
    - Parikh
  - - Stephen
    - Boyd
  chapter: null
  descrip: SCS is an open source numerical software for solving second-order cone
    problems from Steph Boyd's lab
  doi: 10.1007/s10957-016-0892-3
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-3239
  month: June
  note: null
  number: '3'
  pages:
  - '1042'
  - '1068'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - C++
  title: Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding
  type: article
  url: http://link.springer.com/10.1007/s10957-016-0892-3
  venue: Journal of Optimization Theory and Applications
  volume: '169'
  web: null
  year: 2016
omohundro1990geometric:
  address: null
  articleno: null
  authors:
  - - Stephen M.
    - Omohundro
  chapter: null
  descrip: An early review paper on methods and algorithms in "geometric learning",
    i.e., a form of topological data analysis that focuses on the usage of geometric
    algorithms and modeling for machine learning and data science applications
  doi: 10.1016/0167-2789(90)90085-4
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0167-2789
  month: '6'
  note: null
  number: '1'
  pages:
  - '307'
  - '321'
  publisher: Elsevier BV
  series: null
  tags:
  - computational geometry
  - Delaunay triangulations
  - algorithms
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - dimension reduction
  - regression
  - classification
  title: Geometric learning algorithms
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/0167278990900854
  venue: 'Physica D: Nonlinear Phenomena'
  volume: '42'
  web: null
  year: 1990
openmp2015openmp:
  address: null
  articleno: null
  authors:
  - - Architecture Review Board
    - OpenMP
  chapter: null
  descrip: 'OpenMP 4.5 standard: Official definition of OpenMP 4.5 software standard'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: November
  note: null
  number: version 4.5
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - OpenMP
  - parallel programming
  - C
  - Fortran
  title: '{OpenMP} Application Programming Interface'
  type: techreport
  url: https://www.openmp.org/wp-content/uploads/openmp-4.5.pdf
  venue: null
  volume: null
  web: null
  year: 2015
pakin2018performing:
  address: null
  articleno: null
  authors:
  - - Scott
    - Pakin
  chapter: null
  descrip: Los Almos article on converting Prolog code into D-WAVE circuits
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: 5-6
  pages:
  - '928'
  - '949'
  publisher: Cambridge University Press
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  - D-Wave
  title: Performing fully parallel constraint logic programming on a quantum annealer
  type: article
  url: https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/performing-fully-parallel-constraint-logic-programming-on-a-quantum-annealer/AB4CCF2D913D0325F770B3DA02AA262D
  venue: Theory and Practice of Logic Programming
  volume: '18'
  web: null
  year: 2018
papazafeiropoulos2014matlab:
  address: null
  articleno: null
  authors:
  - - George
    - Papazafeiropoulos
  chapter: null
  descrip: Official docs and reference for the Matlab computational geometry toolbox
    (numerical software), which includes all of Matlab's built-in tools for computing
    geometric algorithms and structures, including Delaunay triangulations
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: November
  note: 'Last accessed: April 27, 2020'
  number: null
  pages: null
  publisher: MathWorks
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  - convex hull
  - high dimension
  - software
  - Matlab
  title: '{MATLAB} Computational Geometry Toolbox version 1.2'
  type: techreport
  url: https://www.mathworks.com/matlabcentral/fileexchange/48509-computational-geometry-toolbox
  venue: null
  volume: null
  web: null
  year: 2014
park1991universal:
  address: null
  articleno: null
  authors:
  - - Jooyoung
    - Park
  - - Irwin W
    - Sandberg
  chapter: null
  descrip: Original definition of RBF networks, an early form of neural networks that
    focused on the usage of combining RBF basis functions. These networks are not
    used much anymore, although occasionally still come up in the context of scientific
    machine learning theory and proofs
  doi: 10.1162/neco.1991.3.2.246
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0899-7667
  month: '6'
  note: null
  number: '2'
  pages:
  - '246'
  - '257'
  publisher: MIT Press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - RBFs
  - scientific machine learning
  - SciML
  - regression
  - approximation theory
  title: Universal approximation using radial-basis-function networks
  type: article
  url: https://direct.mit.edu/neco/article/3/2/246-257/5580
  venue: Neural computation
  volume: '3'
  web: null
  year: 1991
parsa2019pabo:
  address: Westin Westminster, CO, USA
  articleno: null
  authors:
  - - Maryam
    - Parsa
  - - Aayush
    - Ankit
  - - Amirkoushyar
    - Ziabari
  - - Kaushik
    - Roy
  chapter: null
  descrip: PABO - a multiobjective Bayesian optimization software package that is
    specialized for NAS -- basically an inner network is used as a surrogate and an
    outer network is used to predict which designs to evaluate next -- I have serious
    reservations about this kind of approach, and it doesn't seem to work that well
  doi: 10.1109/ICCAD45719.2019.8942046
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '11'
  note: null
  number: null
  pages:
  - '1'
  - '8'
  publisher: IEEE/ACM
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  title: '{PABO}: Pseudo Agent-Based Multi-Objective {B}ayesian Hyperparameter Optimization
    for Efficient Neural Accelerator Design'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/8942046
  venue: Proc. 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)
  volume: null
  web: null
  year: 2019
parsa2020bayesian:
  address: null
  articleno: null
  authors:
  - - Maryam
    - Parsa
  - - John P.
    - Mitchell
  - - Catherine D.
    - Schuman
  - - Robert M.
    - Patton
  - - Thomas E.
    - Potok
  - - Kaushik
    - Roy
  chapter: null
  descrip: H-PABO multiobjective Bayesian optimization framework, specialized for
    NAS, and improvement on PABO
  doi: 10.3389/fnins.2020.00667
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1662-453X
  month: '7'
  note: null
  number: null
  pages:
  - 667
  publisher: Frontiers Media SA
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - autotuning
  - hyperparameter optimization
  - surrogate modeling
  title: Bayesian Multi-objective Hyperparameter Optimization for Accurate, Fast,
    and Efficient Neural Network Accelerator Design
  type: article
  url: https://www.frontiersin.org/article/10.3389/fnins.2020.00667/full
  venue: Frontiers in Neuroscience
  volume: '14'
  web: null
  year: 2020
paszke2019pytorch:
  address: null
  articleno: null
  authors:
  - - Adam
    - Paszke
  - - Sam
    - Gross
  - - Francisco
    - Massa
  - - Adam
    - Lerer
  - - James
    - Bradbury
  - - Gregory
    - Chanan
  - - Trevor
    - Killeen
  - - Zeming
    - Lin
  - - Natalia
    - Gimelshein
  - - Luca
    - Antiga
  - - Alban
    - Desmaison
  - - Andreas
    - Kopf
  - - Edward
    - Yang
  - - Zachary
    - DeVito
  - - Martin
    - Raison
  - - Alykhan
    - Tejani
  - - Sasank
    - Chilamkurthy
  - - Benoit
    - Steiner
  - - Lu
    - Fang
  - - Junjie
    - Bai
  - - Soumith
    - Chintala
  chapter: null
  descrip: The official publication for pytorch a gold standard in open source software,
    providing automatic differentiation and numerical linear algebra in Python, targeted
    at implementing deep learning algorithms. Pytorch is pretty much a standard in
    not just open source software, but also machine learning software, and also numerical
    software
  doi: null
  edition: null
  editors:
  - - H.
    - Wallach
  - - H.
    - Larochelle
  - - A.
    - Beygelzimer
  - - F. d\textquotesingle
    - Alch\'{e}-Buc
  - - E.
    - Fox
  - - R.
    - Garnett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1'
  - '12'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - autograd
  - algorithmic differentiation
  - backpropagation
  - open source
  - neural networks
  - high-performance computing
  - HPC
  - computational linear algebra
  - scientific machine learning
  - SciML
  - software
  - OSS
  - Python
  - C++
  - CUDA
  title: '{PyTorch}: An Imperative Style, High-Performance Deep Learning Library'
  type: inproceedings
  url: https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '32'
  web: null
  year: 2019
patel2025marriage:
  address: null
  articleno: null
  authors:
  - - Anita K.
    - Patel
  chapter: null
  descrip: Anita's commentary paper on recent advancements in SciML for computable
    phenotypes in the field of computational medicine (i.e., computing medical risks
    from patient data without human intervention). She congratulates recent advancments
    in using uncertainty aware, well-validated SciML methods such as ensemble models
    and gradient-boosted decision trees (XGBoost) to implement and calibrate (optimize)
    computable phenotypes in real clinical studies
  doi: 10.1001/jamanetworkopen.2024.57422
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2574-3805
  month: '02'
  note: null
  number: '2'
  pages:
  - e2457422
  - e2457422
  publisher: American Medical Association (AMA)
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - decision trees
  - regression
  - classification
  - uncertainty quantification
  - UQ
  title: "The Marriage of Computable Phenotypes With Machine Learning\u2014A Pathway\
    \ to Evidence-Based Care for Critically Ill Children"
  type: article
  url: https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2829845/patel\_2025\_ic\_240355\_1738013025.0942.pdf
  venue: JAMA Network Open
  volume: '8'
  web: null
  year: 2025
pearson1901liii:
  address: null
  articleno: null
  authors:
  - - Karl
    - Pearson
  chapter: null
  descrip: Original paper proposing PCA for dimension reduction. PCA is equivalent
    to taking the SVD of a the data matrix A = USV^T (where A \in R^(n X d) then truncating
    the first r < d columns of U, S, and V such that all the entries in S >= eps.
    This reduces the dimension of the data in A from d->r while maintaining that the
    reconstruction error (information loss) is O(eps^2). There is also a statistical
    explanation that I find less concise, in terms of minimizing the variance in truncated
    dimensions.
  doi: 10.1080/14786440109462720
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1941-5982
  month: '11'
  note: null
  number: '11'
  pages:
  - '559'
  - '572'
  publisher: Taylor \& Francis
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - representation learning
  - high dimension
  - dimension reduction
  title: '{LIII.} On lines and planes of closest fit to systems of points in space'
  type: article
  url: https://www.tandfonline.com/doi/full/10.1080/14786440109462720
  venue: The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science
  volume: '2'
  web: null
  year: 1901
pedregosa2011scikitlearn:
  address: null
  articleno: null
  authors:
  - - Fabian
    - Pedregosa
  - - Ga{\"e}l
    - Varoquaux
  - - Alexandre
    - Gramfort
  - - Vincent
    - Michel
  - - Bertrand
    - Thirion
  - - Olivier
    - Grisel
  - - Mathieu
    - Blondel
  - - Peter
    - Prettenhofer
  - - Ron
    - Weiss
  - - Vincent
    - Dubourg
  - - ''
    - others
  chapter: null
  descrip: The official publication for scikit-learn a gold standard in open source
    software, providing a clean interface to several standard implementations of numerical
    approximation, optimization, machine learning, and deep learning algorithms. Scikit-learn
    is pretty much a standard in not just open source software, but also machine learning
    software, and also numerical software
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '2825'
  - '2830'
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - open source
  - decision trees
  - neural networks
  - scientific machine learning
  - SciML
  - Gaussian process
  - regression
  - classification
  - software
  - OSS
  - Python
  title: 'Scikit-learn: Machine learning in {P}ython'
  type: article
  url: https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf
  venue: Journal of Machine Learning Research
  volume: '12'
  web: null
  year: 2011
peng2008quantum:
  address: null
  articleno: null
  authors:
  - - Xinhua
    - Peng
  - - Zeyang
    - Liao
  - - Nanyang
    - Xu
  - - Gan
    - Qin
  - - Xianyi
    - Zhou
  - - Dieter
    - Suter
  - - Jiangfeng
    - Du
  chapter: null
  descrip: Quantum annealing for integer factorization
  doi: 10.1103/physrevlett.101.220405
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0031-9007
  month: '11'
  note: null
  number: '22'
  pages:
  - 220405
  publisher: APS
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Quantum adiabatic algorithm for factorization and its experimental implementation
  type: article
  url: https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.101.220405
  venue: Physical review letters
  volume: '101'
  web: null
  year: 2008
petitet2018hpl:
  address: null
  articleno: null
  authors:
  - - Antoine
    - Petitet
  - - R. Clint
    - Whaley
  - - Jack
    - Dongarra
  - - Andy
    - Cleary
  chapter: null
  descrip: The official handbook and user's manual for the high-performance LINPACK
    benchmark (HPL) describing how to set-up and run the HPC performance tuning benchmark
    on distributed-memory systems, what problem it solves, what values the config
    file sets, and what are the rules for defining a solver for the problem and having
    it still count toware the Top 500 list
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Innovation Computing Laboratory, University of Tennessee
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - computational linear algebra
  - benchmarking
  - software
  - Fortran
  - parallel programming
  title: '{HPL -- A Portable Implementation of the High-Performance Linpack Benchmark
    for Distributed-Memory Computers}'
  type: book
  url: https://www.netlib.org/benchmark/hpl/
  venue: null
  volume: null
  web: null
  year: 2018
petrini2003case:
  address: Phoenix, AZ, USA
  articleno: null
  authors:
  - - Fabrizio
    - Petrini
  - - Darren J.
    - Kerbyson
  - - Scott
    - Pakin
  chapter: null
  descrip: A case report from configuring a Los Alamos HPC, where real-world performance
    ended up being much lower than expected due to performance variability
  doi: 10.1145/1048935.1050204
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '11'
  note: null
  number: null
  pages:
  - '55'
  - '55'
  publisher: ACM
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - performance modeling
  title: 'The case of the missing supercomputer performance: Achieving optimal performance
    on the 8,192 processors of {ASCI Q}'
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/1048935.1050204
  venue: Proceedings of the 2003 ACM/IEEE Conference on Supercomputing (SC '03)
  volume: null
  web: null
  year: 2003
pfisterer2022yahpo:
  address: null
  articleno: null
  authors:
  - - Florian
    - Pfisterer
  - - Lennart
    - Schneider
  - - Julia
    - Moosbauer
  - - Martin
    - Binder
  - - Bernd
    - Bischl
  chapter: null
  descrip: 'YAHPO gym is yet another hyperparameter optimization gym consisting of
    benchmark and test problems for testing neural architecture search and hyperparameter
    optimization algorithms. Most of the test problems are based on xgboost, knn,
    or svm models of real data. As of 2022, this one was not as mature as HPOBench
    and JAHS-Bench, so we didn''t use it. They claim to now offer multiobjective optimization
    test problems as well. Open source Python software implementation available at:
    github.com/slds-lmu/yahpo_gym'
  doi: null
  edition: null
  editors:
  - - Isabelle
    - Guyon
  - - Marius
    - Lindauer
  - - Mihaela
    - van der Schaar
  - - Frank
    - Hutter
  - - Roman
    - Garnett
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 3/1
  - '39'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - optimization
  - multiobjective optimization
  - autotuning
  - hyperparameter optimization
  - benchmarking
  - software
  - open source
  - OSS
  - Python
  title: '{YAHPO} Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for
    Hyperparameter Optimization'
  type: inproceedings
  url: https://proceedings.mlr.press/v188/pfisterer22a.html
  venue: Proceedings of the First International Conference on Automated Machine Learning
  volume: '188'
  web: null
  year: 2022
pickering2022discovering:
  address: null
  articleno: null
  authors:
  - - Ethan
    - Pickering
  - - Stephen
    - Guth
  - - George Em
    - Karniadakis
  - - Themistoklis P
    - Sapsis
  chapter: null
  descrip: An active learning method for extreme event modeling. Specifically, the
    authors provide a novel acquisition function which targets extreme events
  doi: 10.1038/s43588-022-00376-0
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2662-8457
  month: '12'
  note: null
  number: '12'
  pages:
  - '823'
  - '833'
  publisher: Nature Publishing Group US New York
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  title: Discovering and forecasting extreme events via active learning in neural
    operators
  type: article
  url: https://www.nature.com/articles/s43588-022-00376-0
  venue: Nature Computational Science
  volume: '2'
  web: null
  year: 2022
polianskii2020voronoi:
  address: Virtual Event CA USA
  articleno: null
  authors:
  - - Vladislav
    - Polianskii
  - - Florian T
    - Pokorny
  chapter: null
  descrip: A novel traversal algorithm for high-dimensional Vornoi graphs (or equivalently,
    a simplex walk algorithm for high-dimensional Delaunay triangulations)
  doi: 10.1145/3394486.3403266
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '8'
  note: null
  number: null
  pages:
  - '2154'
  - '2164'
  publisher: ACM
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - Voronoi diagram
  - algorithms
  - high dimension
  title: Voronoi Graph Traversal in High Dimensions with Applications to Topological
    Data Analysis and Piecewise Linear Interpolation
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3394486.3403266
  venue: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
    Discovery \& Data Mining
  volume: null
  web: null
  year: 2020
powell1994direct:
  address: null
  articleno: null
  authors:
  - - Michael J. D.
    - Powell
  chapter: null
  descrip: The COBYLA paper on Powell's constrainted optimization by linear approximation
    algorithm. COBYLA basically performs gradient descent on a constrainted blackbox
    optimization problem by fitting a linear model to the underlying function and
    following its gradient within a shrinking trust region. COBYLA is typically able
    to do this taking typically only one or two function evaluation per iteration
    since typically only one point is exiting the trust-region per iteration. (Occasionally,
    additional model improvement points must be sampled to maintain the interpolation
    set geometry). This makes COBYLA barely more expensive then true gradient descent.
    COBYQA has supplanted COBYLA since then (the Q standing for quadratic), but I
    personally prefer COBYLA still as a find the locally linear models to be more
    robust against noisy and nonsmooth data, still efficiently finding local minima
    even though COBYLA was not design for such problems. The original open source
    software was in impossibly complex old-style Fortran. A modern Fortran version
    has been provided in Pima by Zaikun Zhang, and a modern Python implementation
    is provided in PDFO by Ragonneau and Zhang.
  doi: 10.1007/978-94-015-8330-5_4
  edition: null
  editors: []
  git: null
  isbn: '9789048143580'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '51'
  - '67'
  publisher: Springer
  series: null
  tags:
  - optimization
  - blackbox optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  title: A direct search optimization method that models the objective and constraint
    functions by linear interpolation
  type: inproceedings
  url: http://link.springer.com/10.1007/978-94-015-8330-5_4
  venue: Gomez, S. and Hennart, J. P. (eds) Advances in Optimization and Numerical
    Analysis, vol 275
  volume: null
  web: null
  year: 1994
powell1994uniform:
  address: null
  articleno: null
  authors:
  - - Michael JD
    - Powell
  chapter: null
  descrip: A thorough analysis and review article by Powell on how to bound the error
    of thin-plate spline (TPS) RBF interpolants for scattered data
  doi: 10.1007/s002110050051
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0029-599X
  month: '6'
  note: null
  number: '1'
  pages:
  - '107'
  - '128'
  publisher: Springer
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - interpolation
  - approximation theory
  title: The uniform convergence of thin plate spline interpolation in two dimensions
  type: article
  url: http://link.springer.com/10.1007/s002110050051
  venue: Numerische Mathematik
  volume: '68'
  web: null
  year: 1994
pronzato2017minimax:
  address: null
  articleno: null
  authors:
  - - Luc
    - Pronzato
  chapter: null
  descrip: Review of minimax and maximin techniques and computational methods. Minimax
    is what we want, but it is hard to compute in high dimensions (would require optimizing
    a Delaunay triangulation, which is exponential complexity to compute). Maximin
    is easier to compute (and often used in many algorithms because of this). However,
    minimax gives better dispersion
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - '7'
  - '36'
  publisher: null
  series: null
  tags:
  - design of experiments
  - DoE
  title: 'Minimax and maximin space-filling designs: some properties and methods for
    construction'
  type: article
  url: http://www.numdam.org/item/JSFS_2017__158_1_7_0
  venue: Journal de la Soci{\'e}t{\'e} Fran{\c{c}}aise de Statistique
  volume: '158'
  web: null
  year: 2017
radford2018improving:
  address: San Francisco, CA, USA
  articleno: null
  authors:
  - - Alec
    - Radford
  - - Karthik
    - Narasimhan
  - - Tim
    - Salimans
  - - Ilya
    - Sutskever
  - - ''
    - others
  chapter: null
  descrip: The original GPT paper, describing the techniques used to train the GPT-1
    language model. The highlighted contribution over previous decoder-only transformer
    architectures, is that they focus on training a foundational model on massive
    amounts of unlabeled web text by simply masking out and generating the next word
    in each block of web text and using the true next word as the label. Then, after
    training a massive generative model in this way, they fine-tune the model on carefully
    curated labeled data in order to optimize it for language tasks. This would become
    the paradigm which all LLMs are built on, of training a foundational model then
    fine-tuning.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: OpenAI
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - transformers
  - LLMs
  title: Improving language understanding by generative pre-training
  type: techreport
  url: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
  venue: null
  volume: null
  web: null
  year: 2018
radford2019language:
  address: San Francisco, CA, USA
  articleno: null
  authors:
  - - Alec
    - Radford
  - - Jeffrey
    - Wu
  - - Rewon
    - Child
  - - David
    - Luan
  - - Dario
    - Amodei
  - - Ilya
    - Sutskever
  - - ''
    - others
  chapter: null
  descrip: The credited "GPT-2" paper, describing OpenAI's experience in creating
    the foundational GPT2 language model based on web text. They describe how they
    have built on top of GPT-1, including the usage of BPE embedding (tokenizer) scheme.
    Their claim is that by simply memorizing web text and language, GPT-2 is able
    to complete a reasonably wide variety of tasks and produce sound answers to many
    questions. This is observation represents the true birth of LLM assistants.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: OpenAI
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - transformers
  - LLMs
  title: Language models are unsupervised multitask learners
  type: techreport
  url: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf
  venue: null
  volume: null
  web: null
  year: 2019
raghunath2017global:
  address: Virginia Beach, VA, USA
  articleno: null
  authors:
  - - Chaitra
    - Raghunath
  - - Tyler H.
    - Chang
  - - Layne T.
    - Watson
  - - Mohamad
    - Jrad
  - - Rakesh K.
    - Kapania
  - - Raymond M.
    - Kolonay
  chapter: null
  descrip: Experiences, challenges, and techniques for integrating the blackbox optimization
    solvers VTDIRECT95 and QNSTOP into the parallel service architecture SORCER
  doi: 10.22360/springsim.2017.hpc.023
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 7
  publisher: SCS
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  - stochastic optimization
  - software
  - parallel programming
  - Fortran
  - C
  - Java
  title: Global deterministic and stochastic optimization in a service oriented architecture
  type: inproceedings
  url: http://dl.acm.org/citation.cfm?id=3108103
  venue: Proc. 2017 Spring Simulation Conference (SpringSim 2017), the 25th High Performance
    Computing Symposium (HPC '17)
  volume: null
  web: null
  year: 2017
ragonneau2021pdfo:
  address: null
  articleno: null
  authors:
  - - Tom M.
    - Ragonneau
  - - Zaikun
    - Zhang
  chapter: null
  descrip: 'PDFO: An open source modern Python implementation of Powell''s derivative-free
    numerical optimization software suite, which is considered to be the standard
    (baseline) in derivative-free optimization solvers'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 2025'
  number: '1.2'
  pages: null
  publisher: GitHub
  series: null
  tags:
  - optimization
  - blackbox optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - software
  - open source
  - OSS
  - Python
  title: "{PDFO}: Cross-Platform Interfaces for {P}owell\u2019s Derivative-Free Optimization\
    \ Solvers"
  type: misc
  url: https://github.com/pdfo/pdfo
  venue: GitHub repository
  volume: null
  web: null
  year: 2021
raissi2019physicsinformed:
  address: null
  articleno: null
  authors:
  - - M.
    - Raissi
  - - P.
    - Perdikaris
  - - G.E.
    - Karniadakis
  chapter: null
  descrip: Official publication where PINNs (physics-informed neural networks) were
    first defined by Karniadakis' lab at Brown. Sometimes credited with starting the
    field of scientific machine learning (SciML) -- or at least boosting its popularity
    to the main stream
  doi: 10.1016/j.jcp.2018.10.045
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0021-9991
  month: '2'
  note: null
  number: null
  pages:
  - '686'
  - '707'
  publisher: Elsevier BV
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - neural networks
  - PINNs
  - regression
  - approximation theory
  title: 'Physics-informed neural networks: A deep learning framework for solving
    forward and inverse problems involving nonlinear partial differential equations'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125
  venue: Journal of Computational Physics
  volume: '378'
  web: null
  year: 2019
rajan1994optimality:
  address: null
  articleno: null
  authors:
  - - V. T.
    - Rajan
  chapter: null
  descrip: An analysis on whether and how Delaunay triangulations can be optimal in
    higher dimensional spaces, given that many early optimality conditions had to
    do with two dimensions (i.e., self-centeredness of elements and minimizing the
    sum-of-squared edge lengths in the graph only apply in 2D)
  doi: 10.1007/bf02574375
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0179-5376
  month: '7'
  note: null
  number: '2'
  pages:
  - '189'
  - '202'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - high dimension
  title: Optimality of the {D}elaunay triangulation in {$\R^d$}
  type: article
  url: https://link.springer.com/10.1007/BF02574375
  venue: Discrete \& Computational Geometry
  volume: '12'
  web: null
  year: 1994
rajarathnam2022dreamplacefpga:
  address: Taipei, Taiwan
  articleno: null
  authors:
  - - Rachel Selina
    - Rajarathnam
  - - Mohamed Baker
    - Alawieh
  - - Zixuan
    - Jiang
  - - Mahesh
    - Iyer
  - - David Z.
    - Pan
  chapter: null
  descrip: DREAMPlaceFPGA the official publication for the DREAMPlace open source
    using analytical placement with some deep learning for FPGA placement acceleration
    -- written in C++ with Python interfaces and available at github.com/rachelselinar/DREAMPlaceFPGA
  doi: 10.1109/ASP-DAC52403.2022.9712562
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '1'
  note: null
  number: ''
  pages:
  - '300'
  - '306'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - Python
  - C++
  title: '{DREAMPlaceFPGA}: An Open-Source Analytical Placer for Large Scale Heterogeneous
    {FPGAs} using Deep-Learning Toolkit'
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9712562/
  venue: 2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)
  volume: ''
  web: null
  year: 2022
ranjan2011computationally:
  address: null
  articleno: null
  authors:
  - - Pritam
    - Ranjan
  - - Ronald
    - Haynes
  - - Richard
    - Karsten
  chapter: null
  descrip: Attempting to make Gaussian process interpolation (i.e., Kriging) numerically
    stable as the minimum pairwise distance between points shrinks. In particular,
    they propose a lower bound on the nugget that must be added to the diagonal to
    guarantee a sufficient distance to singularity
  doi: 10.1198/tech.2011.09141
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0040-1706
  month: '11'
  note: null
  number: '4'
  pages:
  - '366'
  - '378'
  publisher: Taylor \& Francis
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - regression
  - approximation theory
  title: A computationally stable approach to Gaussian process interpolation of deterministic
    computer simulation data
  type: article
  url: http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.09141
  venue: Technometrics
  volume: '53'
  web: null
  year: 2011
rasmussen2006gaussian:
  address: null
  articleno: null
  authors:
  - - Carl Edward
    - Rasmussen
  - - Christopher KI
    - Williams
  - - ''
    - others
  chapter: null
  descrip: The book that scipy and scikit-learn cite when it comes to Gaussian process
    regression fundamentals and algorithms. It's a good thorough book, and I find
    some of the error bound derivations to be useful, but I generally prefer Garnet's
    book
  doi: null
  edition: null
  editors: []
  git: null
  isbn: 978-0-262-18253-9
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: The MIT Press
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - regression
  - uncertainty quantification
  - UQ
  title: Gaussian processes for machine learning
  type: book
  url: https://gaussianprocess.org/gpml
  venue: null
  volume: '2'
  web: null
  year: 2006
ray1989sherringtonkirkpatrick:
  address: null
  articleno: null
  authors:
  - - Pulak
    - Ray
  - - Bikas K
    - Chakrabarti
  - - Arunava
    - Chakrabarti
  chapter: null
  descrip: This article lays the theoretical foundation for Quantum Annealing via
    quantum tunneling.
  doi: 10.1103/physrevb.39.11828
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0163-1829
  month: '6'
  note: null
  number: '16'
  pages:
  - 11828
  publisher: APS
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: 'Sherrington-Kirkpatrick model in a transverse field: Absence of replica
    symmetry breaking due to quantum fluctuations'
  type: article
  url: https://link.aps.org/doi/10.1103/PhysRevB.39.11828
  venue: Physical Review B
  volume: '39'
  web: null
  year: 1989
recht2019imagenet:
  address: null
  articleno: null
  authors:
  - - Benjamin
    - Recht
  - - Rebecca
    - Roelofs
  - - Ludwig
    - Schmidt
  - - Vaishaal
    - Shankar
  chapter: null
  descrip: One of the original papers proposing that overfitting, the bias variance
    tradeoff curve, and generalization error are all misunderstood and misused ideas.
    The authors present a true holdout set of lost and never-before-seen ImageNet
    and CIFAR-10 data, and show that massive interpolatory models (models trained
    to zero error) generalize just as well as the well-regularized models on this
    new data (better because their training error was lower and total error = training
    error + generalization error). Ben would go on to claim that (1) you want to interpolate
    a lot of data that we claim we don't want to overfit (such as image data). (2)
    Models that interpolate don't generalize poorly in practice and large overparameterized
    models always perform better in practice. And (3) Hyperparameter tuning is basically
    a form of training on the test set in order to find the best model that interpolates
    the training data. This paper was an eye-opener for me personally, and lead me
    to firmly believe in interpolation for high-dimensional data
  doi: null
  edition: null
  editors:
  - - Kamalika
    - Chaudhuri
  - - Ruslan
    - Salakhutdinov
  git: null
  isbn: null
  issn: null
  month: 09--15 Jun
  note: null
  number: null
  pages:
  - '5389'
  - '5400'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - regularization
  - overfitting
  - scientific machine learning
  - SciML
  - CNNs
  - high dimension
  - interpolation
  - classification
  title: Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?
  type: inproceedings
  url: https://proceedings.mlr.press/v97/recht19a.html
  venue: Proceedings of the 36th International Conference on Machine Learning
  volume: '97'
  web: null
  year: 2019
regis2015calculus:
  address: null
  articleno: null
  authors:
  - - Rommel G.
    - Regis
  chapter: null
  descrip: 'The calculus of simplex gradients: a detailed analysis of simplex gradients
    and their properties for approximating true gradients when solving derivative-free
    and blackbox optimization problems'
  doi: 10.1007/s11590-014-0815-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1862-4472
  month: '6'
  note: null
  number: '5'
  pages:
  - '845'
  - '865'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - derivative-free optimization
  - DFO
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  - interpolation
  - approximation theory
  - uncertainty quantification
  - UQ
  title: The calculus of simplex gradients
  type: article
  url: http://link.springer.com/10.1007/s11590-014-0815-x
  venue: Optimization Letters
  volume: '9'
  web: null
  year: 2015
regis2016multiobjective:
  address: null
  articleno: null
  authors:
  - - Rommel G.
    - Regis
  chapter: null
  descrip: Using RBF surrogates in the context of multiobjective optimization. I believe
    he ultimately solved the surrogate problems with NSGA-II or some other heuristic
  doi: 10.1016/j.jocs.2016.05.013
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1877-7503
  month: '9'
  note: null
  number: null
  pages:
  - '140'
  - '155'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - evolutionary algorithm
  - EA
  - RBFs
  title: Multi-objective constrained black-box optimization using radial basis function
    surrogates
  type: article
  url: http://linkinghub.elsevier.com/retrieve/pii/S1877750316300904
  venue: Journal of Computational Science
  volume: '16'
  web: null
  year: 2016
rose2012vtr:
  address: Monterey, California, USA
  articleno: null
  authors:
  - - Jonathan
    - Rose
  - - Jason
    - Luu
  - - Chi Wai
    - Yu
  - - Opal
    - Densmore
  - - Jeffrey
    - Goeders
  - - Andrew
    - Somerville
  - - Kenneth B.
    - Kent
  - - Peter
    - Jamieson
  - - Jason
    - Anderson
  chapter: null
  descrip: 'The original publication for VTR (Verilog-to-routing) -- open-source synthesis-packing-place-and-route
    platform. Still uses VPR for the place and route steps. VTR is currently the standard
    in open source placement and routing software, written primarilly in C++ with
    a Python interface, available for download at: github.com/verilog-to-routing/vtr-verilog-to-routing'
  doi: 10.1145/2145694.2145708
  edition: null
  editors: []
  git: null
  isbn: '9781450311557'
  issn: null
  month: '2'
  note: null
  number: null
  pages:
  - 10
  publisher: Association for Computing Machinery
  series: FPGA '12
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - Python
  - C++
  title: 'The {VTR} project: architecture and {CAD} for {FPGAs} from verilog to routing'
  type: inproceedings
  url: https://doi.org/10.1145/2145694.2145708
  venue: Proceedings of the ACM/SIGDA International Symposium on Field Programmable
    Gate Arrays
  volume: null
  web: null
  year: 2012
rosenberg1975reduction:
  address: null
  articleno: null
  authors:
  - - Ivo G
    - Rosenberg
  chapter: null
  descrip: Original paper describing "Reduction by Substitution" in the QUBO model
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '71'
  - '74'
  publisher: null
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Reduction of bivalent maximization to the quadratic case
  type: article
  url: null
  venue: "Cahiers du Centre d\u2019etudes de Recherche Operationnelle"
  volume: '17'
  web: null
  year: 1975
rosenblatt1958perceptron:
  address: null
  articleno: null
  authors:
  - - Frank
    - Rosenblatt
  chapter: null
  descrip: 'Rosenblatt''s original paper proposing the multi-layer perceptron with
    an input layer, output layer, and a single hidden layer: this is the foundation
    for all AI and neural network research. It is proposed as a simulation model for
    biologists and psychologists to study brain function. It would not be useful as
    a tool for regression, classification, or prediction for many years until the
    advent of representation learning. He originally used a step (discontinuous) activation
    function since it was well-known from functional analysis and approximation theory
    that the span of these functions is dense in L2 space. The sigmoidal activation
    would later be introduced as a continuous smoothing of the step function (so that
    back propogation would have gradients to train on), but there is no clear reference
    for who first proposed this'
  doi: 10.1037/h0042519
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1939-1471
  month: null
  note: null
  number: '6'
  pages:
  - 386
  publisher: American Psychological Association
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - autograd
  - algorithmic differentiation
  - backpropagation
  - neural networks
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: 'The perceptron: a probabilistic model for information storage and organization
    in the brain.'
  type: article
  url: https://doi.apa.org/doi/10.1037/h0042519
  venue: Psychological review
  volume: '65'
  web: null
  year: 1958
roy2023quasimonte:
  address: null
  articleno: null
  authors:
  - - Pamphile T.
    - Roy
  - - Art B.
    - Owen
  - - Maximilian
    - Balandat
  - - Matt
    - Haberland
  chapter: null
  descrip: Official publication of the scipy.stats.qmc module, which is the newly
    released module for performing quasi-monte carlo sampling and design-of-experiments
    in scipy. Scipy is an open source numerical software package which is the standard
    for advanced numerical methods and scientific software packages in Python. Most
    of scipy are wrappers for much older Fortran or C++ code, that has been highly
    optimized.
  doi: 10.21105/joss.05309
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2475-9066
  month: '4'
  note: null
  number: '84'
  pages:
  - 5309
  publisher: The Open Journal
  series: null
  tags:
  - design of experiments
  - DoE
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  - software
  - open source
  - OSS
  - Fortran
  - Python
  title: Quasi-Monte Carlo Methods in Python
  type: article
  url: https://joss.theoj.org/papers/10.21105/joss.05309
  venue: Journal of Open Source Software
  volume: '8'
  web: null
  year: 2023
rumelhart1985learning:
  address: San Diego, CA, USA
  articleno: null
  authors:
  - - David E.
    - Rumelhart
  - - Geoffrey E.
    - Hinton
  - - Ronald J.
    - Williams
  chapter: null
  descrip: Original tech report introducing recurrent neural networks (RNNs), which
    use a recursively defined state variable to track the context of sequential data
    observations so far, plus the value of the current output to predict the next
    output. The authors derive how the gradients can be backpropogated through this
    entire chain for efficient training. This idea had been around since the 60s,
    but this is the first paper explicitly proposing such recurrent layers in a representation
    learning context. The authors propose these recurrent layers as a means of representation
    learning for sequence data (such as next-word prediction in language models).
    RNNs would go on to become the state-of-the-art in language models and natural
    language processing for almost 30 years until they were replaced by transformers
    with the advent of BERT in 2017. Although the idea of encoding state information
    is a valid solution, RNNs notoriously suffered from a vanishing gradient issue
    where tokens further back in the sequence had little effect on the current prediction.
  doi: 10.21236/ada164453
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '9'
  note: null
  number: ICS 8504
  pages: null
  publisher: Institute for Cognitive Science, University of California
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - backpropagation
  - RNNs
  - neural networks
  - scientific machine learning
  - SciML
  - transformers
  title: Learning internal representations by error propagation
  type: techreport
  url: https://www.cs.toronto.edu/~hinton/absps/pdp8.pdf
  venue: null
  volume: null
  web: null
  year: 1985
rumelhart1986learning:
  address: null
  articleno: null
  authors:
  - - David E.
    - Rumelhart
  - - Geoffrey E.
    - Hinton
  - - Ronald J.
    - Williams
  chapter: null
  descrip: Original paper on back-propagation for training neural networks. Equivalent
    to reverse-mode algorithmic differentiation or more simply applying the chain
    rule recursively.
  doi: 10.1038/323533a0
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0028-0836
  month: '10'
  note: null
  number: '6088'
  pages:
  - '533'
  - '536'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - autograd
  - algorithmic differentiation
  - backpropagation
  - neural networks
  - scientific machine learning
  - SciML
  - regression
  - classification
  title: Learning representations by back-propagating errors
  type: article
  url: https://www.nature.com/articles/323533a0
  venue: Nature
  volume: '323'
  web: null
  year: 1986
ryu2009pareto:
  address: Austin, TX, USA
  articleno: null
  authors:
  - - Jong-hyun
    - Ryu
  - - Sujin
    - Kim
  - - Hong
    - Wan
  chapter: null
  descrip: 'The original publication of PAWS: An adaptive weighted sum and trust-regions
    method for solving biobjective (multiobjective) optimization problems. The key
    here is that the trust-regions help the weighted sum scalarization reach into
    nonconvex regions of the Pareto front, even though that would not normally be
    possible'
  doi: 10.1109/WSC.2009.5429562
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '623'
  - '633'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - simulation optimization
  - scalarization
  title: Pareto front approximation with adaptive weighted sum method in multiobjective
    simulation optimization
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/5429562
  venue: Proc. 2009 Winter Simulation Conference (WSC '09)
  volume: null
  web: null
  year: 2009
ryu2014derivativefree:
  address: null
  articleno: null
  authors:
  - - Jong-Hyun
    - Ryu
  - - Sujin
    - Kim
  chapter: null
  descrip: 'The latest version of PAWS: An adaptive weighted sum and trust-regions
    method for solving biobjective (multiobjective) optimization problems. The key
    here is that the trust-regions help the weighted sum scalarization reach into
    nonconvex regions of the Pareto front, even though that would not normally be
    possible'
  doi: 10.1137/120864738
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '1'
  pages:
  - '334'
  - '362'
  publisher: SIAM
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: A derivative-free trust-region method for biobjective optimization
  type: article
  url: http://epubs.siam.org/doi/10.1137/120864738
  venue: SIAM Journal on Optimization
  volume: '24'
  web: null
  year: 2014
santoro2002theory:
  address: null
  articleno: null
  authors:
  - - Giuseppe E
    - Santoro
  - - Roman
    - Marto{\v{n}}{\'a}k
  - - Erio
    - Tosatti
  - - Roberto
    - Car
  chapter: null
  descrip: Analysis that using quantum annealing is theoretically faster than using
    simulated annealing to solve an Ising/QUBO model
  doi: 10.1126/science.1068774
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0036-8075
  month: '3'
  note: null
  number: '5564'
  pages:
  - '2427'
  - '2430'
  publisher: American Association for the Advancement of Science
  series: null
  tags:
  - quantum computing
  - adiabatic quantum computing
  - quantum annealing
  title: Theory of quantum annealing of an Ising spin glass
  type: article
  url: https://science.sciencemag.org/content/295/5564/2427?casa_token=oOyVfFb2YFkAAAAA:uuUmeZPbfJVVjm9XlBllasY6OA1n_kOEJA4E5DSOD6neFa2PuVI6pEElaMvRkbomX4sJiq9gxn_YPpY
  venue: Science
  volume: '295'
  web: null
  year: 2002
sapsis2022optimal:
  address: null
  articleno: null
  authors:
  - - Themistoklis P.
    - Sapsis
  - - Antoine
    - Blanchard
  chapter: null
  descrip: Techniques and optimal sampling criteria for performing adaptive sampling
    to enable downstream Gaussian process regression modeling
  doi: 10.1098/rsta.2021.0197
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1364-503X
  month: '8'
  note: null
  number: '2229'
  pages:
  - 20210197
  publisher: The Royal Society
  series: null
  tags:
  - design of experiments
  - DoE
  - adaptive sampling
  - model-based sampling
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - optimization
  - RBFs
  - Gaussian process
  - regression
  - uncertainty quantification
  - UQ
  title: Optimal criteria and their asymptotic form for data selection in data-driven
    reduced-order modelling with {Gaussian} process regression
  type: article
  url: https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0197
  venue: Philosophical Transactions of the Royal Society A
  volume: '380'
  web: null
  year: 2022
saves2024smt:
  address: null
  articleno: null
  authors:
  - - Paul
    - Saves
  - - "R\xE9mi"
    - Lafage
  - - Nathalie
    - Bartoli
  - - Youssef
    - Diouane
  - - Jasper
    - Bussemaker
  - - Thierry
    - Lefebvre
  - - John T.
    - Hwang
  - - Joseph
    - Morlier
  - - Joaquim R.R.A.
    - Martins
  chapter: null
  descrip: The SMT 2.0 paper, major improvements to the open source numerical software
    package (in Python) pySMT for solving multidisciplinary engineering design optimization
    (MDO) problems, while utilizing derivatives and providing numerical stability
    analysis for each surrogate model class. In SMT 2.0, support is added for hierarchical
    and mixed variables, and major improvements have been made to the structure, completeness,
    and features of the SMT library.
  doi: 10.1016/j.advengsoft.2023.103571
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0965-9978
  month: '2'
  note: null
  number: null
  pages:
  - 103571
  publisher: Elsevier BV
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - mixed-variable optimization
  - surrogate modeling
  - constrained optimization
  - RBFs
  - Gaussian process
  - software
  - open source
  - OSS
  - Python
  title: 'SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed
    variables Gaussian processes'
  type: article
  url: https://www.sciencedirect.com/science/article/pii/S096599782300162X
  venue: Advances in Engineering Software
  volume: '188'
  web: null
  year: 2024
schaback1995error:
  address: null
  articleno: null
  authors:
  - - Robert
    - Schaback
  chapter: null
  descrip: Some thorough analysis on upper and lower error bounds for RBF interpolants,
    which ultimately yield the famous "uncertainty princple for RBFs" -- i.e., a large
    shape parameter is needed for accuracy but a small shape parameter is needed for
    solvability of the resulting RBF system. Both are not possible, so you cannot
    have accuracy and solvability at once. Generally, the author recommends making
    the parameter as large as possible without causing singularity, a tradeoff that
    is often optimized in modern Gaussian process implementations via L-BFGS-B. There
    is also a really nice table of kernel errors for various kernels, which can be
    directly plugged into the generic RBF interpolation error formula to get error
    bounds for most common RBF basis functions
  doi: 10.1007/BF02432002
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1019-7168
  month: '4'
  note: null
  number: '3'
  pages:
  - '251'
  - '264'
  publisher: Springer
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - approximation theory
  - uncertainty quantification
  - UQ
  title: Error estimates and condition numbers for radial basis function interpolation
  type: article
  url: http://link.springer.com/10.1007/BF02432002
  venue: Advances in Computational Mathematics
  volume: '3'
  web: null
  year: 1995
schulman2017proximal:
  address: null
  articleno: null
  authors:
  - - John
    - Schulman
  - - Filip
    - Wolski
  - - Prafulla
    - Dhariwal
  - - Alec
    - Radford
  - - Oleg
    - Klimov
  chapter: null
  descrip: Proximal policy optimization (PPO) is the state-of-the-art policy gradient
    optimization method for reinforcement learning. Proximal gradient methods computes
    the update by directly optimizing the parameters for a given batch of observations
    in order to maximize the intermediate value function. PPO limits the step size
    in each iteration and enforces a penalization to discourage models from drifting
    too far from the original "base" model.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.LG
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - neural networks
  - optimization
  - scientific machine learning
  - SciML
  title: Proximal Policy Optimization Algorithms
  type: techreport
  url: https://arxiv.org/abs/1707.06347
  venue: null
  volume: null
  web: null
  year: 2017
schweidtmann2018machine:
  address: null
  articleno: null
  authors:
  - - Artur M.
    - Schweidtmann
  - - Adam D.
    - Clayton
  - - Nicholas
    - Holmes
  - - Eric
    - Bradford
  - - Richard A.
    - Bourne
  - - Alexei A.
    - Lapkin
  chapter: null
  descrip: Multiobjective molecule property optimization by optimizing processes in
    a continuous flow reactor (CFR) using the multiobjective evolutionary algorithm
    TS-EMO (thompson sampling evolutionary multiobjective optimization?)
  doi: 10.1016/j.cej.2018.07.031
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1385-8947
  month: '11'
  note: null
  number: null
  pages:
  - '277'
  - '282'
  publisher: Elsevier
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - mixed-variable optimization
  - evolutionary algorithms
  - EA
  title: 'Machine learning meets continuous flow chemistry: Automated optimization
    towards the {Pareto} front of multiple objectives'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S1385894718312634
  venue: Chemical Engineering Journal
  volume: '352'
  web: null
  year: 2018
sennrich2016neural:
  address: Berlin, Germany
  articleno: null
  authors:
  - - Rico
    - Sennrich
  - - Barry
    - Haddow
  - - Alexandra
    - Birch
  chapter: null
  descrip: The original paper for applying byte pair encoding (BPE) to words. This
    is the method used to embed words into numerical tokens for modern LLMs. The idea
    is to recursively combine the most commonly paired letters into a single token.
    Then an encoding value (number) is assigned to every token in the resulting vocabulary,
    to generate a massive encoder/decoder lookup table.
  doi: 10.18653/v1/P16-1162
  edition: null
  editors:
  - - Katrin
    - Erk
  - - Noah A.
    - Smith
  git: null
  isbn: null
  issn: null
  month: aug
  note: null
  number: null
  pages:
  - '1715'
  - '1725'
  publisher: Association for Computational Linguistics
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - representation learning
  - neural networks
  - LLMs
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  title: Neural Machine Translation of Rare Words with Subword Units
  type: inproceedings
  url: https://aclanthology.org/P16-1162/
  venue: 'Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)'
  volume: null
  web: null
  year: 2016
shang2020survey:
  address: null
  articleno: null
  authors:
  - - Ke
    - Shang
  - - Hisao
    - Ishibuchi
  - - Linjun
    - He
  - - Lie Meng
    - Pang
  chapter: null
  descrip: A survey of hypervolume indicator usage in multiobjective evolutionary
    optimization, mainly in terms of algorithms that use hypervolume improvement and
    also as a performance indicator
  doi: 10.1109/TEVC.2020.3013290
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '2'
  note: null
  number: '1'
  pages:
  - '1'
  - '20'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  - evolutionary algorithms
  - EA
  title: A survey on the hypervolume indicator in evolutionary multiobjective optimization
  type: article
  url: https://ieeexplore.ieee.org/document/9153850
  venue: IEEE Transactions on Evolutionary Computation
  volume: '25'
  web: null
  year: 2020
shao2024deepseekmath:
  address: null
  articleno: null
  authors:
  - - Zhihong
    - Shao
  - - Peiyi
    - Wang
  - - Qihao
    - Zhu
  - - Runxin
    - Xu
  - - Junxiao
    - Song
  - - Xiao
    - Bi
  - - Haowei
    - Zhang
  - - Mingchuan
    - Zhang
  - - YK
    - Li
  - - Y
    - Wu
  - - ''
    - others
  chapter: null
  descrip: 'DeepSeekMath: Describes how DeepSeek''s mathematical specialist model
    was trained TODO: read this report carefully'
  doi: 10.48550/arXiv.2402.03300
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2402.03300
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - open source
  title: '{DeepSeekMath}: Pushing the limits of mathematical reasoning in open language
    models'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2024
shapley1953value:
  address: null
  articleno: null
  authors:
  - - Lloyd S
    - Shapley
  - - ''
    - others
  chapter: null
  descrip: Original paper on Shapley values (used to calculate SHAP scores). The idea
    is to interpret the importance of features to a model agonstically by making predictions
    with (and without) every combination of features available. Then, we compute the
    weighted average of each feature's importance by calculating how much each feature
    improves the model performance. This is an expensive but polynomial time procedure
    given fixed number of features, and can be used to plot the contributions. The
    software is in lundberg et al 2017 (github.com/shap/shap)
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '28'
  pages:
  - '307'
  - '317'
  publisher: Princeton University Press Princeton
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - high dimension
  - dimension reduction
  - regression
  - classification
  title: A value for n-person games
  type: article
  url: http://www.library.fa.ru/files/Roth2.pdf#page=39
  venue: Contributions to the Theory of Games
  volume: '2'
  web: null
  year: 1953
shashaani2016astrodf:
  address: Washington, DC, USA
  articleno: null
  authors:
  - - Sara
    - Shashaani
  - - Susan R.
    - Hunter
  - - Raghu
    - Pasupathy
  chapter: null
  descrip: ASTRO-DF is a derivative-free stochastic optimization solver, targeted
    at noisy simulation optimization problems. Particularly those derived from Monte
    Carlo simulations. The algorithm is model-based within a trust region. The key
    contribution is adaptively determining how many Monte carlo samples to take in
    each iteration, in order to guarantee convergence. There are also ASTRO and ASTRO-C
    variants available. The open source Python software is available at github.com/sshashaa/astro-df
  doi: 10.1109/WSC.2016.7822121
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '12'
  note: null
  number: null
  pages:
  - '554'
  - '565'
  publisher: IEEE
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - stochastic optimization
  - software
  - open source
  - OSS
  - Python
  title: '{ASTRO-DF}: Adaptive sampling trust-region optimization algorithms, heuristics,
    and numerical experience'
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/7822121/
  venue: 2016 Winter Simulation Conference (WSC)
  volume: null
  web: null
  year: 2016
shewchuk2002what:
  address: null
  articleno: null
  authors:
  - - Jonathan
    - Shewchuk
  chapter: null
  descrip: 'Extremely thorough analysis of using linear models inside a simplex (e.g.,
    tetrahedral meshes) for interpolation: covering basic algorithm, conditioning,
    approximation error, quality metrics, etc.'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '115'
  - '126'
  publisher: null
  series: null
  tags:
  - computational geometry
  - interpolation
  - Delaunay triangulation
  - algorithms
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - approximation theory
  - uncertainty quantification
  - UQ
  title: What is a good linear finite element? {I}nterpolation, conditioning, anisotropy,
    and quality measures
  type: inproceedings
  url: https://people.eecs.berkeley.edu/~jrs/papers/elemj.pdf
  venue: Proceedings of the 11th International Meshing Roundtable
  volume: null
  web: null
  year: 2002
shi2023numerical:
  address: null
  articleno: null
  authors:
  - - Hao-Jun Michael
    - Shi
  - - Melody Qiming
    - Xuan
  - - Figen
    - Oztoprak
  - - Jorge Nocedal
    - and
  chapter: null
  descrip: A recent paper on the numerical performance of finite-difference-based
    methods for DFO. Personally, I don't think this is a viable approach given the
    performance of model-based methods. However, one of their experiments corroborates
    my experience that you can just run COBYLA on noisy and nonsmooth blackbox optimization
    problems, and even though it was not designed for those problems, it still does
    extremely well and regularly finds local minima (at least up to the noise level)
    -- from Nocedal's lab
  doi: 10.1080/10556788.2022.2121832
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1055-6788
  month: '3'
  note: null
  number: '2'
  pages:
  - '289'
  - '311'
  publisher: Taylor \& Francis
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  title: On the numerical performance of finite-difference-based methods for derivative-free
    optimization
  type: article
  url: https://doi.org/10.1080/10556788.2022.2121832
  venue: Optimization Methods and Software
  volume: '38'
  web: null
  year: 2023
shields2021bayesian:
  address: null
  articleno: null
  authors:
  - - Benjamin J.
    - Shields
  - - Jason
    - Stevens
  - - Jun
    - Li
  - - Marvin
    - Parasram
  - - Farhan
    - Damani
  - - Jesus I. M.
    - Alvarado
  - - Jacob M.
    - Janey
  - - Rryan P.
    - Adams
  - - Abigail G.
    - Doyle
  chapter: null
  descrip: 'The EDBO software: open source Python software for performing multiobjective
    bayesian optimization for chemical synthesis and molecular discovery. Links a
    multiobjective optimization solver with the MORDRED software for getting molecular
    descriptors and optimizes for the desired properties'
  doi: 10.1038/s41586-021-03213-y
  edition: null
  editors: []
  git: http://github.com/b-shields/edbo
  isbn: null
  issn: 0028-0836
  month: '2'
  note: null
  number: '7844'
  pages:
  - '89'
  - '96'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - surrogate modeling
  - mixed-variable optimization
  - Gaussian process
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - representation learning
  - software
  - open source
  - OSS
  - Python
  title: Bayesian reaction optimization as a tool for chemical synthesis
  type: article
  url: https://www.nature.com/articles/s41586-021-03213-y
  venue: Nature
  volume: '590'
  web: null
  year: 2021
shor1999polynomialtime:
  address: null
  articleno: null
  authors:
  - - Peter W
    - Shor
  chapter: null
  descrip: Shor's algorithm for prime factorization
  doi: 10.1137/s0036144598347011
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0036-1445
  month: '1'
  note: null
  number: '2'
  pages:
  - '303'
  - '332'
  publisher: SIAM
  series: null
  tags:
  - quantum computing
  title: Polynomial-time algorithms for prime factorization and discrete logarithms
    on a quantum computer
  type: article
  url: https://epubs.siam.org/doi/abs/10.1137/S0036144598347011?casa_token=a1HjL4NT4JcAAAAA:76FzW89tOwMuu471TSx0PCrJFUTa0tbvOyNfM4isoCzj3NwY6pvUAlJJpAlvq_emxY4fuksR_WRh
  venue: SIAM review
  volume: '41'
  web: null
  year: 1999
shroff1992adaptive:
  address: null
  articleno: null
  authors:
  - - Gautam M.
    - Shroff
  - - Christian H.
    - Bischof
  chapter: null
  descrip: A numerical algorithm for performing a rank-revealing rank-1 QR update
    (given an existing orthonormal (QR) matrix factorization and updating just one
    column of the basis Q)
  doi: 10.1137/0613077
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0895-4798
  month: '10'
  note: null
  number: '4'
  pages:
  - '1264'
  - '1278'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - dimension reduction
  title: Adaptive condition estimation for rank-one updates of {QR} factorizations
  type: article
  url: http://epubs.siam.org/doi/10.1137/0613077
  venue: SIAM Journal on Matrix Analysis and Applications
  volume: '13'
  web: null
  year: 1992
shwartzziv2022tabular:
  address: null
  articleno: null
  authors:
  - - Ravid
    - Shwartz-Ziv
  - - Amitai
    - Armon
  chapter: null
  descrip: A survey on how deep learning models (such as transformer models and neural
    networks/multilayer perceptrons) still do not perform well (are not the state
    of the art) when it comes to tabular data, which remains the most important application
    for most businesses. Gradient boosted trees remain the most reliable predictors
    for these applications
  doi: 10.1016/j.inffus.2021.11.011
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1566-2535
  month: '5'
  note: null
  number: null
  pages:
  - '84'
  - '90'
  publisher: Elsevier
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - decision trees
  - neural networks
  - scientific machine learning
  - SciML
  - decision-trees
  - transformers
  - regression
  - classification
  title: 'Tabular data: Deep learning is not all you need'
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S1566253521002360
  venue: Information Fusion
  volume: '81'
  web: null
  year: 2022
silver2016mastering:
  address: null
  articleno: null
  authors:
  - - David
    - Silver
  - - Aja
    - Huang
  - - Chris J
    - Maddison
  - - Arthur
    - Guez
  - - Laurent
    - Sifre
  - - George
    - Van Den Driessche
  - - Julian
    - Schrittwieser
  - - Ioannis
    - Antonoglou
  - - Veda
    - Panneershelvam
  - - Marc
    - Lanctot
  - - ''
    - others
  chapter: null
  descrip: Google DeepMind's AlphaGo was the first reinforcement learning agent to
    beat pro players (exceed the maximum human skill) in Go, which is a much more
    complex game than chess and required modeling an enumerable amount of game states
    and move possibilities. This required combining monte carlo tree search (MCTS)
    with neural networks to filter down to a reasonable number of potential states.
    This is credited as the first RL model to use MCTS, which is now a standard in
    RL
  doi: 10.1038/nature16961
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0028-0836
  month: '1'
  note: null
  number: '7587'
  pages:
  - '484'
  - '489'
  publisher: Nature Publishing Group
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - neural networks
  - scientific machine learning
  - SciML
  title: Mastering the game of Go with deep neural networks and tree search
  type: article
  url: https://www.nature.com/articles/nature16961
  venue: nature
  volume: '529'
  web: null
  year: 2016
slepicka2020poisson:
  address: null
  articleno: null
  authors:
  - - Hugo
    - Slepicka
  chapter: null
  descrip: The docker image for the SUPERFISH/POISSON container + wrapper -- we used
    this at Argonne to obtain a portable copy of POISSON/SUPERFISH when performing
    particle accelerator RF gun cavity optimization and distributing work on remote
    systems
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Aug 2023'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - simulation
  - software
  - Fortran
  - Kubernetes
  title: Poisson Superfish via Docker
  type: misc
  url: https://github.com/hhslepicka/docker-poisson-superfish-nobin
  venue: null
  volume: null
  web: null
  year: 2020
smale1998mathematical:
  address: null
  articleno: null
  authors:
  - - Steve
    - Smale
  chapter: null
  descrip: Steve Smale's list of unsolved open problems in mathematics, and mostly
    algorithms, one of which is reliably finding basic solutions to linear programming
    problems
  doi: 10.1007/bf03025291
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0343-6993
  month: '3'
  note: null
  number: '2'
  pages:
  - '7'
  - '15'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - algorithms
  - optimization
  - linear programming
  - LP
  - convex optimization
  - constrained optimization
  title: Mathematical problems for the next century
  type: article
  url: http://link.springer.com/10.1007/BF03025291
  venue: The Mathematical Intelligencer
  volume: '20'
  web: null
  year: 1998
sobieszczanskisobieski2015multidisciplinary:
  address: Chichester, UK
  articleno: null
  authors:
  - - Jaroslaw
    - Sobieszczanski-Sobieski
  - - Alan
    - Morris
  - - Michel
    - Van Tooren
  chapter: null
  descrip: One of the most popular textbooks in the field of multidisciplinary engineering
    design optimization. The introduction provides plenty of motivation for solving
    computational expensive multiobjective simulation / blackbox optimization problems
  doi: 10.1002/9781118897072
  edition: null
  editors: []
  git: null
  isbn: 978-1-118-49212-3
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: John Wiley \& Sons, Ltd.
  series: null
  tags:
  - design of experiments
  - DoE
  - optimization
  - multiobjective optimization
  - simulation optimization
  title: Multidisciplinary Design Optimization Supported by Knowledge Based Engineering
  type: book
  url: null
  venue: null
  volume: null
  web: null
  year: 2015
sobol1967distribution:
  address: null
  articleno: null
  authors:
  - - I. M.
    - Sobol
  chapter: null
  descrip: The original publication of the Sobol sequence algorithm for generating
    well-distributed points for in the context of good nodes for numerical integration.
    This is now referred to as a low-discrepancy sequence and is also used for design-of-experiments
    and quasi-random number generation
  doi: 10.1016/0041-5553(67)90144-9
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0044-4669
  month: '1'
  note: null
  number: '4'
  pages:
  - '784'
  - '802'
  publisher: Elsevier BV
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  - Sobol sequence
  title: Distribution of points in a cube and approximate evaluation of integrals
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/0041555367901449
  venue: \v{Z}urnal Vy\v{c}islitel\cprime no\u{\i} Matematiki i Matemati\v{c}esko\u{\i}
    Fiziki
  volume: '7'
  web: null
  year: 1967
sohldickstein2015deep:
  address: Lille, France
  articleno: null
  authors:
  - - Jascha
    - Sohl-Dickstein
  - - Eric
    - Weiss
  - - Niru
    - Maheswaranathan
  - - Surya
    - Ganguli
  chapter: null
  descrip: Original publication on diffusion models. The paper introduces a framework
    inspired by non-equilibrium thermodynamics where a forward diffusion process gradually
    destroys structure in the data distribution, transforming it into a simple distribution
    like Gaussian noise. A reverse diffusion process is then learned to reconstruct
    the original data distribution, serving as a generative mode. Data can be generated
    by training the distribution through diffusion, then applying the reverse diffusion
    operator to sample (or "generate") new data. This is the basis for most image
    and video generation models
  doi: null
  edition: null
  editors:
  - - Francis
    - Bach
  - - David
    - Blei
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '2256'
  - '2265'
  publisher: PMLR
  series: Proceedings of Machine Learning Research
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - diffusion models
  - scientific machine learning
  - SciML
  - neural networks
  title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics
  type: inproceedings
  url: https://proceedings.mlr.press/v37/sohl-dickstein15.html
  venue: Proceedings of the 32nd International Conference on Machine Learning
  volume: '37'
  web: null
  year: 2015
srivastava2014dropout:
  address: null
  articleno: null
  authors:
  - - Nitish
    - Srivastava
  - - Geoffrey
    - Hinton
  - - Alex
    - Krizhevsky
  - - Ilya
    - Sutskever
  - - Ruslan
    - Salakhutdinov
  chapter: null
  descrip: The original dropout paper, this was standard practice in training deep
    neural networks, especially massive convolutional nets and RNNs for image and
    language processing for a long time, prior to the invention of transformers --
    the idea being that you zero out the effects (and updates to) a small percentage
    of the nodes in each layer in each iteration, in order to (1) make training cheaper,
    (2) prevent overfitting since over reliance on any individual node(s) makes the
    predictions brittle, and (3) redistribute weights to earlier layers and avoid
    vanishing gradients
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '1'
  pages:
  - '1929'
  - '1958'
  publisher: JMLR. org
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - scientific machine learning
  - SciML
  - regularization
  - overfitting
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
  type: article
  url: https://jmlr.org/papers/v15/srivastava14a.html
  venue: The Journal of Machine Learning research
  volume: '15'
  web: null
  year: 2014
stall2019make:
  address: null
  articleno: null
  authors:
  - - Shelley
    - Stall
  - - Lynn
    - Yarmey
  - - Joel
    - Cutcher-Gershenfeld
  - - Brooks
    - Hanson
  - - Kerstin
    - Lehnert
  - - Brian
    - Nosek
  - - Mark
    - Parsons
  - - Erin
    - Robinson
  - - Lesley
    - Wyborn
  chapter: null
  descrip: 'The FAIR principles of data management: Scientific data should be findable,
    accessible, interpretable, and reproducible'
  doi: 10.1038/d41586-019-01720-7
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0028-0836
  month: '6'
  note: null
  number: '7759'
  pages:
  - '27'
  - '29'
  publisher: Nature Publishing Group
  series: null
  tags:
  - high-performance computing
  - HPC
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  title: Make scientific data {FAIR}
  type: article
  url: https://www.nature.com/articles/d41586-019-01720-7
  venue: Nature
  volume: '570'
  web: null
  year: 2019
steane1998quantum:
  address: null
  articleno: null
  authors:
  - - Andrew
    - Steane
  chapter: null
  descrip: A concise summary of the quantum gate model
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '2'
  pages:
  - 117
  publisher: IOP Publishing
  series: null
  tags:
  - quantum computing
  title: Quantum computing
  type: article
  url: https://iopscience.iop.org/article/10.1088/0034-4885/61/2/002
  venue: Reports on Progress in Physics
  volume: '61'
  web: null
  year: 1998
stellato2020osqp:
  address: null
  articleno: null
  authors:
  - - Bartolomeo
    - Stellato
  - - Goran
    - Banjac
  - - Paul
    - Goulart
  - - Alberto
    - Bemporad
  - - Stephen
    - Boyd
  chapter: null
  descrip: OSQP is an open source numerical software for solving quadratic programming
    problems from Stephen Boyd's lab
  doi: 10.1007/s12532-020-00179-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1867-2949
  month: '12'
  note: null
  number: '4'
  pages:
  - '637'
  - '672'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - projection
  - optimization
  - quadratic programming
  - QP
  - convex optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - C++
  title: '{OSQP}: an operator splitting solver for quadratic programs'
  type: article
  url: http://link.springer.com/10.1007/s12532-020-00179-2
  venue: Mathematical Programming Computation
  volume: '12'
  web: null
  year: 2020
steuer1983interactive:
  address: null
  articleno: null
  authors:
  - - Ralph E
    - Steuer
  - - Eng-Ung
    - Choo
  chapter: null
  descrip: The original publication of the weighted Chebyshev scalarization scheme
    for multiobjective optimization
  doi: 10.1007/BF02591870
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0025-5610
  month: '10'
  note: null
  number: '3'
  pages:
  - '326'
  - '344'
  publisher: Springer
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: An interactive weighted Tchebycheff procedure for multiple objective programming
  type: article
  url: http://link.springer.com/10.1007/BF02591870
  venue: Mathematical programming
  volume: '26'
  web: null
  year: 1983
strohmaier2019top:
  address: null
  articleno: null
  authors:
  - - Eric
    - Strohmaier
  - - Jack
    - Dongarra
  - - Horst
    - Simon
  - - Martin
    - Meuer
  chapter: null
  descrip: Citation for the Nov 2019, HPC Top 500 -- measuring the fastest HPCs in
    the world based on their max throughput on the high-performance LINPACK HPC performance
    benchmark problem
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: November
  note: 'Last accessed: April 18, 2020'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - distributed computing
  - computational linear algebra
  - benchmarking
  title: The Top 500 List
  type: misc
  url: https://www.top500.org
  venue: null
  volume: null
  web: null
  year: 2019
su1997comparison:
  address: null
  articleno: null
  authors:
  - - Peter
    - Su
  - - Robert L. S.
    - Drysdale
  chapter: null
  descrip: A nice summary article (with some performance comparisons) for a wide variety
    of sequential Delaunay triangulation algorithms in 2D
  doi: 10.1016/s0925-7721(96)00025-9
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0925-7721
  month: '4'
  note: null
  number: '5'
  pages:
  - '361'
  - '385'
  publisher: Elsevier BV
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: A comparison of sequential {D}elaunay triangulation algorithms
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0925772196000259
  venue: Computational Geometry
  volume: '7'
  web: null
  year: 1997
sv2005evolutionary:
  address: London, UK
  articleno: null
  authors: []
  chapter: null
  descrip: The textbook on evolutionary multiobjective optimization (MOO), including
    common algorithms, evaluation methodologies, fundamental techniques, and test
    problems
  doi: 10.1007/1-84628-137-7
  edition: null
  editors:
  - - Ajith
    - Abraham
  - - Lakhmi
    - Jain
  - - Robert
    - Goldberg
  git: null
  isbn: '1852337877'
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Springer Verlag
  series: Advanced Information and Knowledge Processing Series
  tags:
  - optimization
  - multiobjective optimization
  - evolutionary algorithms
  - EA
  title: 'Evolutionary Multiobjective Optimization: Theoretical Advances and Applications'
  type: book
  url: http://link.springer.com/10.1007/1-84628-137-7
  venue: Advanced Information and Knowledge Processing
  volume: null
  web: null
  year: 2005
tao2011epsilon:
  address: null
  articleno: null
  authors:
  - - Terence
    - Tao
  chapter: null
  descrip: 'The textbook from which I learned measure theory: covering measure theory
    and topology bases, common measure and metric spaces, Banach and Hilbert spaces,
    compactness, strong and weak topologies for a linear operator, L^p spaces, Fourier
    transforms, the Hausdorff dimension, and most basic theorems (Hahn-Banach, Caratheodory,
    Baire Category theorem, etc.)'
  doi: 10.1090/mbk/077
  edition: null
  editors: []
  git: null
  isbn: '9780821852804'
  issn: null
  month: '3'
  note: null
  number: null
  pages: null
  publisher: American Mathematical Society
  series: null
  tags:
  - design of experiments
  - DoE
  - measure theory
  title: 'An Epsilon of Room: Pages from year three of a mathematical blog'
  type: book
  url: http://www.ams.org/mbk/077
  venue: null
  volume: '2'
  web: null
  year: 2011
tavallaee2009detailed:
  address: Ottawa, ON, Canada
  articleno: null
  authors:
  - - Mahbod
    - Tavallaee
  - - Ebrahim
    - Bagheri
  - - Wei
    - Lu
  - - Ali A.
    - Ghorbani
  chapter: null
  descrip: 'A benchmark data science problem from the field of computer security:
    robustly classifying suspicious network traffic based on metadata about the requests'
  doi: 10.1109/CISDA.2009.5356528
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '1'
  - '6'
  publisher: IEEE
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - benchmarking
  - classification
  title: A detailed analysis of the {KDD} {CUP} 99 data set
  type: inproceedings
  url: http://ieeexplore.ieee.org/document/5356528
  venue: 2009 IEEE Symposium on Computational Intelligence for Security and Defense
    Applications
  volume: null
  web: null
  year: 2009
tavares2022parallel:
  address: null
  articleno: null
  authors:
  - - S.
    - Tavares
  - - C. P.
    - Br\'as
  - - A. L.
    - Cust\'odio
  - - V.
    - Duarte
  - - P.
    - Medeiros
  chapter: null
  descrip: BoostDMS is numerical software library providing access to Custodio's direct
    search and pattern search software, including MultiGLODS and DMS, in Matlab with
    full parallel computing support
  doi: 10.1007/s11075-022-01364-1
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1017-1398
  month: '3'
  note: null
  number: '3'
  pages:
  - '1757'
  - '1788'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - pattern search
  - software
  - Matlab
  - parallel programming
  title: Parallel Strategies for Direct Multisearch
  type: article
  url: https://link.springer.com/10.1007/s11075-022-01364-1
  venue: Numerical Algorithms
  volume: '92'
  web: null
  year: 2022
team2023gemini:
  address: null
  articleno: null
  authors:
  - - Gemini
    - Team
  - - Rohan
    - Anil
  - - Sebastian
    - Borgeaud
  - - Jean-Baptiste
    - Alayrac
  - - Jiahui
    - Yu
  - - Radu
    - Soricut
  - - Johan
    - Schalkwyk
  - - Andrew M
    - Dai
  - - Anja
    - Hauth
  - - Katie
    - Millican
  - - ''
    - others
  chapter: null
  descrip: 'The original Google Gemini tech report. The model is closed, so this is
    not that useful, but they discuss the architecture. There is an interesting contribution
    to architecture, where they utilize heterogeneous encoders from text, speech,
    video, and images into the same latent space, then feed these encoded tokens (in
    sequence) to a transformer-based decoder-only model, then pass its output to either
    an image or text decoder to generate the next token. TODO: read this report carefully,
    especially the architecture details.'
  doi: 10.48550/arXiv.2312.11805
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2312.11805
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - transformers
  - representation learning
  title: 'Gemini: a family of highly capable multimodal models'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2023
thacker2010algorithm:
  address: null
  articleno: null
  authors:
  - - William I.
    - Thacker
  - - Jingwei
    - Zhang
  - - Layne T.
    - Watson
  - - Jeffrey B.
    - Birch
  - - Manjula A.
    - Iyer
  - - Michael W.
    - Berry
  chapter: null
  descrip: The SHEPPACK numerical software package contains a variety of open source
    Fortran codes for computing Shepard's method interpolants in two, three, and four
    dimensions. There is also a linear Shepard's method interpolant that can be used
    for interpolation in arbitrary dimension.
  doi: 10.1145/1824801.1824812
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '9'
  note: null
  number: '3'
  pages:
  - 34
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - computational linear algebra
  - interpolation
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 905: {SHEPPACK}: {M}odified {S}hepard algorithm for interpolation
    of scattered multivariate data'
  type: article
  url: https://dl.acm.org/doi/10.1145/1824801.1824812
  venue: ACM Transactions on Mathematical Software
  volume: '37'
  web: null
  year: 2010
thomann2019trustregion:
  address: null
  articleno: null
  authors:
  - - Jana
    - Thomann
  - - Gabriele
    - Eichfelder
  chapter: null
  descrip: A trust-region + RBF surrogate-based multiobjective optimization algorithm
    for solving heterogeneous multiobjective optimization problems (where one or more
    objectives is a computationally expensive blackbox, and one or more is not). The
    key is to just use the derivative of all the non blackbox objectives and use the
    model derivative for the blackbox terms. The algorithm itself follows something
    like Orbit
  doi: 10.1137/18m1173277
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '2'
  pages:
  - '1017'
  - '1047'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: A Trust-Region Algorithm for Heterogeneous Multiobjective Optimization
  type: article
  url: https://epubs.siam.org/doi/10.1137/18M1173277
  venue: '{SIAM} Journal on Optimization'
  volume: '29'
  web: null
  year: 2019
tian2017platemo:
  address: null
  articleno: null
  authors:
  - - Ye
    - Tian
  - - Ran
    - Cheng
  - - Xingyi
    - Zhang
  - - Yaochu
    - Jin
  chapter: null
  descrip: An open source MATLAB platform for implementing and running wide-scale
    comparisons against other multiobjective evolutionary algorithms on standard multiobjective
    test problems
  doi: 10.1109/MCI.2017.2742868
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1556-603X
  month: '11'
  note: null
  number: '4'
  pages:
  - '73'
  - '87'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - benchmarking
  - evolutionary algorithms
  - EA
  - software
  - open source
  - OSS
  - Matlab
  title: '{PlatEMO}: A {MATLAB} Platform for Evolutionary Multi-Objective Optimization
    [Educational Forum]'
  type: article
  url: http://ieeexplore.ieee.org/document/8065138
  venue: IEEE Computational Intelligence Magazine
  volume: '12'
  web: null
  year: 2017
tian2022improving:
  address: Chongqing, China
  articleno: null
  authors:
  - - Chunsheng
    - Tian
  - - Lei
    - Chen
  - - Yuan
    - Wang
  - - Shuo
    - Wang
  - - Jing
    - Zhou
  - - Yaowei
    - Zhang
  - - Guang
    - Li
  chapter: null
  descrip: Simulated annealing and reinforcement learning based FPGA placement. The
    RL contributions are tenuous at best, but still an example of potential RL impact
    in industry
  doi: 10.1109/ITAIC54216.2022.9836761
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: ''
  pages:
  - '1912'
  - '1919'
  publisher: IEEE
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - reinforcement learning
  - RL
  - optimization
  - reinforcment learning
  - simulated annealing
  - SA
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  title: Improving Simulated Annealing Algorithm for {FPGA} Placement Based on Reinforcement
    Learning
  type: inproceedings
  url: https://ieeexplore.ieee.org/document/9836761/
  venue: 2022 IEEE 10th Joint International Information Technology and Artificial
    Intelligence Conference (ITAIC)
  volume: '10'
  web: null
  year: 2022
touvron2023llama:
  address: null
  articleno: null
  authors:
  - - Hugo
    - Touvron
  - - Thibaut
    - Lavril
  - - Gautier
    - Izacard
  - - Xavier
    - Martinet
  - - Marie-Anne
    - Lachaux
  - - Timoth{\'e}e
    - Lacroix
  - - Baptiste
    - Rozi{\`e}re
  - - Naman
    - Goyal
  - - Eric
    - Hambro
  - - Faisal
    - Azhar
  - - ''
    - others
  chapter: null
  descrip: 'Meta''s original LLaMA tech report, introducing the first "herd" of LLaMA
    models, which were the first open source (really just open weight) models, which
    one can download and host locally. The original models were 7B and 65B parameters,
    and trained on only publicly available data. They list their data sources, model
    architectures (i.e., embedding layers, number of transformer / attention heads,
    number of layers, number of feed-forward network dimensions, learning rates, number
    of input tokens, activation function, and optimizer. They trained the models in
    PyTorchs. Then they share performance on common benchmarks. The model architectures
    and instructions to download the weights (form HuggingFace) are obtained from
    github.com/meta-llama/llama TODO: read this report carefully'
  doi: 10.48550/arXiv.2302.13971
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2302.13971
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - transformers
  - open source
  title: '{LLaMA}: Open and efficient foundation language models'
  type: techreport
  url: null
  venue: null
  volume: null
  web: null
  year: 2023
touvron2023llama2:
  address: null
  articleno: null
  authors:
  - - Hugo
    - Touvron
  - - Louis
    - Martin
  - - Kevin
    - Stone
  - - Peter
    - Albert
  - - Amjad
    - Almahairi
  - - Yasmine
    - Babaei
  - - Nikolay
    - Bashlykov
  - - Soumya
    - Batra
  - - Prajjwal
    - Bhargava
  - - Shruti
    - Bhosale
  - - ''
    - others
  chapter: null
  descrip: 'The LLaMA 2 model publication. In addition to the information from LLaMA
    1 tech report, they are now discussing fine-tuning and RLHF and chat safety features.
    The latest models range from 7B to 70B total parameters. The model architectures
    and instructions to download the weights (form HuggingFace) are obtained from
    github.com/meta-llama/llama TODO: read this report carefully'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:2307.09288
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - LLMs
  - open source
  title: '{LLaMA}~2: Open foundation and fine-tuned chat models'
  type: techreport
  url: https://arxiv.org/abs/2307.09288
  venue: null
  volume: null
  web: null
  year: 2023
trott2022kokkos:
  address: null
  articleno: null
  authors:
  - - Christian R.
    - Trott
  - - Damien
    - "Lebrun-Grandi\xE9"
  - - Daniel
    - Arndt
  - - Jan
    - Ciesko
  - - Vinh
    - Dang
  - - Nathan
    - Ellingwood
  - - Rahulkumar
    - Gayatri
  - - Evan
    - Harvey
  - - Daisy S.
    - Hollman
  - - Dan
    - Ibanez
  - - Nevin
    - Liber
  - - Jonathan
    - Madsen
  - - Jeff
    - Miles
  - - David
    - Poliakoff
  - - Amy
    - Powell
  - - Sivasankaran
    - Rajamanickam
  - - Mikael
    - Simberg
  - - Dan
    - Sunderland
  - - Bruno
    - Turcksin
  - - Jeremiah
    - Wilke
  chapter: null
  descrip: Kokkos 3 latest paper from Sandia's CCR lab -- an open source, parallel
    computing software library for performance portable parallel and distributed computing
    in C++. Primarily designed and used for high-performance numerical software, e.g.,
    Trilinos
  doi: 10.1109/TPDS.2021.3097283
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1045-9219
  month: '4'
  note: null
  number: '4'
  pages:
  - '805'
  - '817'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - parallel computing
  - software
  - open source
  - OSS
  - parallel programming
  title: 'Kokkos 3: Programming Model Extensions for the Exascale Era'
  type: article
  url: https://ieeexplore.ieee.org/document/9485033/
  venue: IEEE Transactions on Parallel and Distributed Systems
  volume: '33'
  web: null
  year: 2022
tuvsar2015visualization:
  address: null
  articleno: null
  authors:
  - - Tea
    - Tu\vsar
  - - Bogdan
    - Filipi\vc
  chapter: null
  descrip: A survey of Pareto front visualization techniques in multiobjective optimization
  doi: 10.1109/TEVC.2014.2313407
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1089-778X
  month: '4'
  note: null
  number: '2'
  pages:
  - '225'
  - '245'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  title: 'Visualization of {P}areto Front Approximations in Evolutionary Multiobjective
    Optimization: A Critical Review and the Prosection Method'
  type: article
  url: https://ieeexplore.ieee.org/document/6777535
  venue: IEEE Transactions on Evolutionary Computation
  volume: '19'
  web: null
  year: 2015
urquhart2020surrogatebased:
  address: null
  articleno: null
  authors:
  - - Magnus
    - Urquhart
  - - Emil
    - Ljungskog
  - - Simone
    - Sebben
  chapter: null
  descrip: A robust RBF surrogate-based model, with adaptive scaling of the basis
    function radii to maintain numerical stability and a custom LHS sampling technique
  doi: 10.1016/j.asoc.2019.106050
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1568-4946
  month: '3'
  note: null
  number: null
  pages:
  - 106050
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: Surrogate-based optimisation using adaptively scaled radial basis functions
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S1568494619308324
  venue: Applied Soft Computing
  volume: '88'
  web: null
  year: 2020
vandermaaten2008visualizing:
  address: null
  articleno: null
  authors:
  - - Laurens
    - van der Maaten
  - - Geoffrey
    - Hinton
  chapter: null
  descrip: 'Original publication of t-SNE, which performs latent space embedding and
    dimension reduction for high-dimensional data. The idea is to minimize the distribution
    error via KL-divergence between a target distribution and the a low-dimensional
    embedding of that distribution that is trained via gradient descent. The KL-divergence
    is calculated based on a similarity score between points in the target distribution
    (which perfectly maintains the relative distances / exponentiated relative distances
    between points in the original high-dimensional space) and the low-dimensional
    embedding, so the objective is ultimately to maintain the relative distances between
    points in the embedding. The method was originally proposed as a visualization
    technique for high-dimensional data (where the embedding produces clusters in
    2 or 3 dimensions that can be plotted), but t-SNE can be and is also used for
    generic dimension reduction and latent-space embedding. The original software
    is not open source, but many open source implementations are linked on the original
    author''s website: lvdmaaten.github.io/tsne'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: '86'
  pages:
  - '2579'
  - '2605'
  publisher: null
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - representation learning
  - high dimension
  - dimension reduction
  - software
  - open source
  - OSS
  - C
  - C++
  - Python
  - Java
  - Julia
  - Matlab
  - CUDA
  title: Visualizing Data using {t-SNE}
  type: article
  url: http://jmlr.org/papers/v9/vandermaaten08a.html
  venue: Journal of Machine Learning Research
  volume: '9'
  web: null
  year: 2008
vandevender1982slatec:
  address: null
  articleno: null
  authors:
  - - Walter H.
    - Vandevender
  - - Karen H.
    - Haskell
  chapter: null
  descrip: The (recently open source) numerical software library SLATEC from Sandia
    is something of a precursor to a modern library like scipy. SLATEC provides highly
    optimized, numerically stable, Fortran implementations for nearly every basic
    numerical algorithm that one would encounter in scientific computing
  doi: 10.1145/1057594.1057595
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0163-5778
  month: '9'
  note: null
  number: '3'
  pages:
  - '16'
  - '21'
  publisher: Association for Computing Machinery (ACM)
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - convex optimization
  - constrained optimization
  - software
  - open source
  - OSS
  - Fortran
  title: The {SLATEC} Mathematical Subroutine Library
  type: article
  url: https://dl.acm.org/doi/10.1145/1057594.1057595
  venue: SIGNUM Newsletter
  volume: '17'
  web: null
  year: 1982
vaswani2017attention:
  address: Long Beach, California, USA
  articleno: null
  authors:
  - - Ashish
    - Vaswani
  - - Noam
    - Shazeer
  - - Niki
    - Parmar
  - - Jakob
    - Uszkoreit
  - - Llion
    - Jones
  - - Aidan N
    - Gomez
  - - '{\L}ukasz'
    - Kaiser
  - - Illia
    - Polosukhin
  chapter: null
  descrip: The landmark paper showing that transformers alone are capable of capturing
    all structure needed for language conversion, i.e., recurence relations and the
    sequential structure of time-series predictions can be encoded into which tokens
    to "pay attention to" during next word (or any next item) predictions. This paper
    is often cited as the inspiration for large language models (LLMs), which rely
    heavily on the transformer architecture, which became a standard after this
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '1'
  - '11'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - transformers
  - neural networks
  - scientific machine learning
  - SciML
  title: Attention is all you need
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
  venue: Proceedings of the 31st International Conference on Neural Information Processing
    Systems (NeurIPS '17)
  volume: null
  web: null
  year: 2017
viana2016tutorial:
  address: null
  articleno: null
  authors:
  - - Felipe AC
    - Viana
  chapter: null
  descrip: A tutorial on how to compute Latin hypercube samples (LHS) and some basic
    properties and ongoing research related to design-of-experiments
  doi: 10.1002/qre.1924
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0748-8017
  month: '7'
  note: null
  number: '5'
  pages:
  - '1975'
  - '1985'
  publisher: Wiley Online Library
  series: null
  tags:
  - design of experiments
  - DoE
  - Latin hypercube sampling
  - LHS
  title: A tutorial on Latin hypercube design of experiments
  type: article
  url: https://onlinelibrary.wiley.com/doi/10.1002/qre.1924
  venue: Quality and reliability engineering international
  volume: '32'
  web: null
  year: 2016
vinyals2019grandmaster:
  address: null
  articleno: null
  authors:
  - - Oriol
    - Vinyals
  - - Igor
    - Babuschkin
  - - Wojciech M.
    - Czarnecki
  - - Micha{\"e}l
    - Mathieu
  - - Andrew
    - Dudzik
  - - Junyoung
    - Chung
  - - David H.
    - Choi
  - - Richard
    - Powell
  - - Timo
    - Ewalds
  - - Petko
    - Georgiev
  - - Junhyuk
    - Oh
  - - Dan
    - Horgan
  - - Manuel
    - Kroiss
  - - Ivo
    - Danihelka
  - - Aja
    - Huang
  - - Laurent
    - Sifre
  - - Trevor
    - Cai
  - - John P.
    - Agapiou
  - - Max
    - Jaderberg
  - - Alexander S.
    - Vezhnevets
  - - R{\'e}mi
    - Leblond
  - - Tobias
    - Pohlen
  - - Valentin
    - Dalibard
  - - David
    - Budden
  - - Yury
    - Sulsky
  - - James
    - Molloy
  - - Tom L.
    - Paine
  - - Caglar
    - Gulcehre
  - - Ziyu
    - Wang
  - - Tobias
    - Pfaff
  - - Yuhuai
    - Wu
  - - Roman
    - Ring
  - - Dani
    - Yogatama
  - - Dario
    - W{\"u}nsch
  - - Katrina
    - McKinney
  - - Oliver
    - Smith
  - - Tom
    - Schaul
  - - Timothy
    - Lillicrap
  - - Koray
    - Kavukcuoglu
  - - Demis
    - Hassabis
  - - Chris
    - Apps
  - - David
    - Silver
  chapter: null
  descrip: Google DeepMind's AlphaStar II was the first reinforcement learning agent
    that could beat pro players in video games, which is a signficantly more complex
    environment than a board game environment with a finite state and set of possible
    actions
  doi: 10.1038/s41586-019-1724-z
  edition: null
  editors: []
  git: null
  isbn: 1476-4687
  issn: 0028-0836
  month: '11'
  note: null
  number: '7782'
  pages:
  - '350'
  - '354'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - neural networks
  - scientific machine learning
  - SciML
  title: Grandmaster level in {StarCraft II} using multi-agent reinforcement learning
  type: article
  url: https://doi.org/10.1038/s41586-019-1724-z
  venue: Nature
  volume: '575'
  web: null
  year: 2019
virtanen2020scipy:
  address: null
  articleno: null
  authors:
  - - Pauli
    - Virtanen
  - - Ralf
    - Gommers
  - - Travis E.
    - Oliphant
  - - Matt
    - Haberland
  - - Tyler
    - Reddy
  - - David
    - Cournapeau
  - - Evgeni
    - Burovski
  - - Pearu
    - Peterson
  - - Warren
    - Weckesser
  - - Jonathan
    - Bright
  - - St{\'e}fan J.
    - van der Walt
  - - Matthew
    - Brett
  - - Joshua
    - Wilson
  - - K.
    - Jarrod Millman
  - - Nikolay
    - Mayorov
  - - Andrew R.~J.
    - Nelson
  - - Eric
    - Jones
  - - Robert
    - Kern
  - - Eric
    - Larson
  - - CJ
    - Carey
  - - '{\.I}lhan'
    - Polat
  - - Yu
    - Feng
  - - Eric W.
    - Moore
  - - Jake
    - VanderPlas
  - - Denis
    - Laxalde
  - - Josef
    - Perktold
  - - Robert
    - Cimrman
  - - Ian
    - Henriksen
  - - E.~A.
    - Quintero
  - - Charles R
    - Harris
  - - Anne M.
    - Archibald
  - - Ant{\^o}nio H.
    - Ribeiro
  - - Fabian
    - Pedregosa
  - - Paul
    - van Mulbregt
  - - SciPy 1.0
    - Contributors
  chapter: null
  descrip: 'SciPy official publication: Scipy is an open source numerical software
    package which is the standard for advanced numerical methods and scientific software
    packages in Python. Most of scipy are wrappers for much older Fortran or C++ code,
    that has been highly optimized.'
  doi: 10.1038/s41592-019-0686-2
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1548-7091
  month: '3'
  note: null
  number: '3'
  pages:
  - '261'
  - '272'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - computational geometry
  - design of experiments
  - DoE
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - linear programming
  - quadratic programming
  - LP
  - QP
  - convex optimization
  - constrained optimization
  - derivative-free optimization
  - DFO
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - interpolation
  - software
  - open source
  - OSS
  - Fortran
  - C++
  - Python
  title: '{SciPy} 1.0: {F}undamental Algorithms for Scientific Computing in {P}ython'
  type: article
  url: https://www.nature.com/articles/s41592-019-0686-2
  venue: Nature Methods
  volume: '17'
  web: null
  year: 2020
voss2019tbb:
  address: Berkeley, CA
  articleno: null
  authors:
  - - Michael
    - Voss
  - - Rafael
    - Asenjo
  - - James
    - Reinders
  chapter: null
  descrip: Chapter on how to bind an older version of TBB to NUMA nodes using Intel's
    TBB C++ software interface
  doi: 10.1007/978-1-4842-4398-5_20
  edition: null
  editors: []
  git: null
  isbn: 978-1-4842-4398-5
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '581'
  - '604'
  publisher: Apress
  series: null
  tags:
  - high-performance computing
  - HPC
  - parallel computing
  - software
  - C++
  - parallel programming
  title: '{TBB} on {NUMA} Architectures'
  type: inbook
  url: https://doi.org/10.1007/978-1-4842-4398-5_20
  venue: 'Pro {TBB}: {C++} Parallel Programming with Threading Building Blocks'
  volume: null
  web: null
  year: 2019
vtrdevelopers2024verilogtorouting:
  address: null
  articleno: null
  authors:
  - - ''
    - VTR~Developers
  chapter: null
  descrip: 'VTR docs with a bit more detail than above reference, and latest version
    (most up-to-date) information VTR is currently the standard in open source placement
    and routing software, written primarilly in C++ with a Python interface, available
    for download at: github.com/verilog-to-routing/vtr-verilog-to-routing'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Feb 2025'
  number: Version 8.1.0-dev
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - software
  - open source
  - OSS
  - Python
  - C++
  title: Verilog-to-routing documentation
  type: misc
  url: https://readthedocs.org/projects/vtr/downloads/pdf/latest
  venue: null
  volume: null
  web: null
  year: 2024
wang2018numerical:
  address: null
  articleno: null
  authors:
  - - Ruoxi
    - Wang
  - - Yingzhou
    - Li
  - - Eric
    - Darve
  chapter: null
  descrip: An analysis and empirical study on how RBF interpolants interpolation matrices
    tend to always become ill-conditioned in high dimensions
  doi: 10.1137/17m1135803
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0895-4798
  month: '1'
  note: null
  number: '4'
  pages:
  - '1810'
  - '1835'
  publisher: SIAM
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - high dimension
  - interpolation
  - approximation theory
  title: On the numerical rank of radial basis function kernels in high dimensions
  type: article
  url: https://epubs.siam.org/doi/10.1137/17M1135803
  venue: SIAM Journal on Matrix Analysis and Applications
  volume: '39'
  web: null
  year: 2018
wang2022pyomodoe:
  address: null
  articleno: e17813
  authors:
  - - Jialu
    - Wang
  - - Alexander W.
    - Dowling
  chapter: null
  descrip: Open source numerical Python software pyomo.DOE, implementing model-driven
    design-of-experiments generation in Pyomo
  doi: 10.1002/aic.17813
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0001-1541
  month: '12'
  note: null
  number: '12'
  pages: null
  publisher: Wiley
  series: null
  tags:
  - design of experiments
  - DoE
  - model-based sampling
  - software
  - Python
  title: 'Pyomo.DOE: An open-source package for model-based design of experiments
    in Python'
  type: article
  url: https://aiche.onlinelibrary.wiley.com/doi/10.1002/aic.17813
  venue: AIChE Journal
  volume: '68'
  web: null
  year: 2022
wang2023design:
  address: null
  articleno: null
  authors:
  - - Yueyao
    - Wang
  - - Li
    - Xu
  - - Yili
    - Hong
  - - Rong
    - Pan
  - - Tyler H.
    - Chang
  - - Thomas C. H.
    - Lux
  - - Jon
    - Bernard
  - - Layne T.
    - Watson
  - - Kirk W.
    - Cameron
  chapter: null
  descrip: Survey of design-of-experiments techniques and modifications for HPC system
    analysis, specifically related to linearly constrained and integer lattice design
    spaces
  doi: 10.1080/00224065.2022.2035285
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0022-4065
  month: '1'
  note: null
  number: '1'
  pages:
  - '88'
  - '103'
  publisher: Taylor \& Francis
  series: null
  tags:
  - design of experiments
  - DoE
  - Latin hypercube sampling
  - LHS
  - high-performance computing
  - HPC
  title: Design strategies and approximation methods for high-performance computing
    variability management
  type: article
  url: https://www.tandfonline.com/doi/full/10.1080/00224065.2022.2035285
  venue: Journal of Quality Technology
  volume: '55'
  web: null
  year: 2023
wang2024mathshepherd:
  address: Bangkok, Thailand
  articleno: null
  authors:
  - - Peiyi
    - Wang
  - - Lei
    - Li
  - - Zhihong
    - Shao
  - - Runxin
    - Xu
  - - Damai
    - Dai
  - - Yifei
    - Li
  - - Deli
    - Chen
  - - Yu
    - Wu
  - - Zhifang
    - Sui
  chapter: null
  descrip: 'Math-Shepherd paper describing how the Math-Shepherd model was trained
    via RLHF to evaluate each step in a LLM''s logical process. The resulting model
    can be used to validate other LLM''s mathematical and reasoning skills. TODO:
    read this report carefully'
  doi: 10.18653/v1/2024.acl-long.510
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '9426'
  - '9439'
  publisher: Association for Computational Linguistics
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - LLMs
  title: 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations'
  type: inproceedings
  url: https://aclanthology.org/2024.acl-long.510
  venue: Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (ACL)
  volume: '1'
  web: null
  year: 2024
wang2024mmlupro:
  address: Miami, Florida, USA
  articleno: null
  authors:
  - - Yubo
    - Wang
  - - Xueguang
    - Ma
  - - Ge
    - Zhang
  - - Yuansheng
    - Ni
  - - Abhranil
    - Chandra
  - - Shiguang
    - Guo
  - - Weiming
    - Ren
  - - Aaran
    - Arulraj
  - - Xuan
    - He
  - - Ziyan
    - Jiang
  - - Tianle
    - Li
  - - Max
    - Ku
  - - Kai
    - Wang
  - - Alex
    - Zhuang
  - - Rongqi
    - Fan
  - - Xiang
    - Yue
  - - Wenhu
    - Chen
  chapter: null
  descrip: The MMLU-Pro benchmark. The current go to benchmark for multitask understanding
    and reasoning in large language models (LLMs). Focuses on logical reasoning capabilities.
    The benchmark interface is available in Python from github.com/TIGER-AI-Lab/MMLU-Pro
  doi: 10.18653/v1/2024.genbench-1.5
  edition: null
  editors:
  - - A.
    - Globerson
  - - L.
    - Mackey
  - - D.
    - Belgrave
  - - A.
    - Fan
  - - U.
    - Paquet
  - - J.
    - Tomczak
  - - C.
    - Zhang
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages:
  - '95266'
  - '95290'
  publisher: Curran Associates, Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - benchmarking
  title: '{MMLU-Pro}: A More Robust and Challenging Multi-Task Language Understanding
    Benchmark'
  type: inproceedings
  url: https://proceedings.neurips.cc/paper_files/paper/2024/file/ad236edc564f3e3156e1b2feafb99a24-Paper-Datasets_and_Benchmarks_Track.pdf
  venue: Advances in Neural Information Processing Systems
  volume: '37'
  web: null
  year: 2024
wathen2015spectral:
  address: null
  articleno: null
  authors:
  - - Andrew J
    - Wathen
  - - Shengxin
    - Zhu
  chapter: null
  descrip: A spectral analysis on the conditioning of the kernel matrics (i.e., interpolation
    matrices) for RBF interpolants and Gaussian processes, showing the singular values
    trending to zero exponentially in both theory and practice
  doi: 10.1007/s11075-015-9970-0
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1017-1398
  month: '12'
  note: null
  number: '4'
  pages:
  - '709'
  - '726'
  publisher: Springer
  series: null
  tags:
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - RBFs
  - Gaussian process
  - interpolation
  - approximation theory
  title: On spectral distribution of kernel matrices related to radial basis functions
  type: article
  url: http://link.springer.com/10.1007/s11075-015-9970-0
  venue: Numerical Algorithms
  volume: '70'
  web: null
  year: 2015
watson1981computing:
  address: null
  articleno: null
  authors:
  - - David F.
    - Watson
  chapter: null
  descrip: The Bowyer-Watson algorithm is one of the first algorithms for computing
    Delaunay triangulations in arbitrary dimensions. It is not particularly scalable,
    but a first step toward thinking about Delaunay triangulation in more than 3D.
    Was published by both Bowyer and Watson in the same issue of the same journal,
    with a footnote from the publisher that they both submitted at the same time and
    after investigation, it was determined that this was purely coincidental and no
    plagiarism was involved. Therefore, both papers were published together and both
    authors are credited equally for discovery
  doi: 10.1093/comjnl/24.2.167
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0010-4620
  month: '2'
  note: null
  number: '2'
  pages:
  - '167'
  - '172'
  publisher: Oxford University Press (OUP)
  series: null
  tags:
  - computational geometry
  - Delaunay triangulation
  - algorithms
  title: Computing the n-dimensional {D}elaunay tessellation with application to {V}oronoi
    polytopes
  type: article
  url: https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/24.2.167
  venue: The Computer Journal
  volume: '24'
  web: null
  year: 1981
watson1997algorithm:
  address: null
  articleno: null
  authors:
  - - Layne T
    - Watson
  - - Maria
    - Sosonkina
  - - Robert C
    - Melville
  - - Alexander P
    - Morgan
  - - Homer F
    - Walker
  chapter: null
  descrip: 'The latest release of HOMPACK: An open source numerical software package
    written in Fortran 90 for solving nonlinear and polynomial systems of equations
    via homotopy methods.'
  doi: 10.1145/279232.279235
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '12'
  note: null
  number: '4'
  pages:
  - '514'
  - '549'
  publisher: ACM
  series: null
  tags:
  - optimization
  - convex optimization
  - global optimization
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 777: {HOMPACK90}: A suite of {Fortran} 90 codes for globally convergent
    homotopy algorithms'
  type: article
  url: https://dl.acm.org/doi/10.1145/279232.279235
  venue: ACM Transactions on Mathematical Software (TOMS)
  volume: '23'
  web: null
  year: 1997
wei2022chainofthought:
  address: New Orleans, LA, USA
  articleno: '1800'
  authors:
  - - Jason
    - Wei
  - - Maarten
    - Bosma
  - - Vincent
    - Zhao
  - - Kelvin
    - Guu
  - - Adams Wei
    - Yu
  - - Brian
    - Lester
  - - Nan
    - Du
  - - Andrew M.
    - Dai
  - - Quoc V
    - Le
  - - Jason
    - Wei
  - - Xuezhi
    - Wang
  - - Dale
    - Schuurmans
  - - Maarten
    - Bosma
  - - Brian
    - Ichter
  - - Fei
    - Xia
  - - Ed H.
    - Chi
  - - Quoc V.
    - Le
  - - Denny
    - Zhou
  chapter: null
  descrip: Original publication on "chain of thought" (CoT) reasoning in LLMs The
    authors take a 137B parameter foundational LLM and fine tune if for about 60 types
    of instruction-based tasks (they refer to this as "instruction tuning"). Then
    they evaluate the model on task types not in the training list, and it performs
    adequately. They claim this shows that LLMs are zero-shot learners. I think this
    may be a stretch as the tasks likely have a lot of overlapping similarities. However,
    the idea that LLMs can complete semi-related but new tasks (not in the training
    data) is obviously true to anyone who has used one.
  doi: null
  edition: null
  editors: []
  git: null
  isbn: '9781713871088'
  issn: null
  month: null
  note: null
  number: null
  pages:
  - 14
  publisher: Curran Associates Inc.
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  title: Chain-of-thought prompting elicits reasoning in large language models
  type: inproceedings
  url: https://openreview.net/forum?id=_VjQlMeSB_J
  venue: Proceedings of the 36th International Conference on Neural Information Processing
    Systems
  volume: null
  web: null
  year: 2022
weinan2020machine:
  address: null
  articleno: null
  authors:
  - - E
    - Weinan
  chapter: null
  descrip: A summary paper for an influential line of work in the field of scientific
    machine learning. Some standard techniques for computing error bounds for neural
    networks using traditional techniques from numerical analysis and approximation
    theory, and introducing a universal approximation theorem for neural networks.
    I.e., showing the existence of a two-layer multilayer perceptron or similar neural
    network that can approximate any Sobolev function to arbitrary required accuracy.
    Equivalently, we could say that such neural networks are dense in this Sobolev
    space
  doi: 10.4208/cicp.OA-2020-0185
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1991-7120
  month: '1'
  note: null
  number: '5'
  pages:
  - '1639'
  - '1670'
  publisher: Global Science Press
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - scientific machine learning
  - SciML
  - regression
  - approximation theory
  title: Machine learning and computational mathematics
  type: article
  url: https://global-sci.com/article/79736/machine-learning-and-computational-mathematics
  venue: Communications in Computational Physics
  volume: '28'
  web: null
  year: 2020
whaley2001automated:
  address: null
  articleno: null
  authors:
  - - R. Clint
    - Whaley
  - - Antoine
    - Petitet
  - - Jack J.
    - Dongarra
  chapter: null
  descrip: How optimization is used to autotune the configuration of the BLAS subroutines
    and kernels for the ATLAS project -- since most numerical software relies on BLAS,
    it is often prudent to spend time optimizing BLAS configurations (such as matrix
    block sizes, etc.) to match machine specific values (such as cache sizes, etc.)
    when installing on a HPC that will have a high numerical (compute bound) workload.
    This can be done automatically via numerical optimization, so that users can just
    run the ATLAS setup scripts to configure BLAS automatically if they want an optimized
    installation
  doi: 10.1016/s0167-8191(00)00087-9
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0167-8191
  month: '1'
  note: null
  number: 1--2
  pages:
  - '3'
  - '35'
  publisher: Elsevier BV
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - mixed-variable optimization
  - autotuning
  - software
  - parallel programming
  title: Automated empirical optimizations of software and the {ATLAS} project
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0167819100000879
  venue: Parallel Computing
  volume: '27'
  web: null
  year: 2001
wierzbicki1999reference:
  address: Boston, MA
  articleno: null
  authors:
  - - Andrzej P.
    - Wierzbicki
  chapter: null
  descrip: The original publication of the reference point method for scalarizing
    multiobjective optimization problems (minimize the distance to a reference point)
  doi: 10.1007/978-1-4615-5025-9_9
  edition: null
  editors:
  - - Tomas
    - Gal
  - - Theodor J.
    - Stewart
  - - Thomas
    - Hanne
  git: null
  isbn: '9781461372837'
  issn: 0884-8289
  month: null
  note: null
  number: null
  pages:
  - '237'
  - '275'
  publisher: Springer US
  series: International Series in Operations Research &amp; Management Science
  tags:
  - optimization
  - multiobjective optimization
  - scalarization
  title: Reference Point Approaches
  type: incollection
  url: http://link.springer.com/10.1007/978-1-4615-5025-9_9
  venue: 'Multicriteria Decision Making: Advances in MCDM Models, Algorithms, Theory,
    and Applications'
  volume: null
  web: null
  year: 1999
wild2008orbit:
  address: null
  articleno: null
  authors:
  - - Stefan M.
    - Wild
  - - Rommel G.
    - Regis
  - - Christine A.
    - Shoemaker
  chapter: null
  descrip: 'ORBIT: the original algorithm for solving optimization problems via sequentially
    minimizing RBF interpolants with a linear tail inside a sequence of trust-regions.
    I can''t remember if it''s open source, but Stefan has a high quality numerical
    software in MATLAB'
  doi: 10.1137/070691814
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1064-8275
  month: '1'
  note: null
  number: '6'
  pages:
  - '3197'
  - '3219'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: '{ORBIT:} {O}ptimization by Radial Basis Function Interpolation in Trust-Regions'
  type: article
  url: http://epubs.siam.org/doi/10.1137/070691814
  venue: SIAM Journal on Scientific Computing
  volume: '30'
  web: null
  year: 2008
wild2011global:
  address: null
  articleno: null
  authors:
  - - Stefan M.
    - Wild
  - - Christine A.
    - Shoemaker
  chapter: null
  descrip: Analysis of the convergence rate of RBF-based surrogates with linear tail
    (fully linear model) inside a sequence of trust regions. This is the theory used
    in ORBIT
  doi: 10.1137/09074927X
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '7'
  note: null
  number: '3'
  pages:
  - '761'
  - '781'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  - RBFs
  title: Global Convergence of Radial Basis Function Trust Region Derivative-Free
    Algorithms
  type: article
  url: http://epubs.siam.org/doi/10.1137/09074927X
  venue: SIAM Journal on Optimization
  volume: '21'
  web: null
  year: 2011
wild2017solving:
  address: null
  articleno: null
  authors:
  - - Stefan M.
    - Wild
  chapter: null
  descrip: The POUNDERS composite blackbox / simulation optimization algorithm, which
    is an open source numerical software package for exploiting the sum-of-squares
    structure in derivative-free least squares problems. Specifically, POUNDERS models
    the blackbox function / simulation's outputs using a fully linear model then uses
    the sum-of-squares structure to get a free Hessian approximation, and achieve
    second-order convergence for the price of first-order convergence. Although not
    included, open source numerical software implementations are now available in
    Python and Matlab through the PyOptUs GitHub group
  doi: 10.1137/1.9781611974683.ch40
  edition: null
  editors:
  - - Tamas
    - Terlaky
  - - Miguel F.
    - Anjos
  - - Shabbir
    - Ahmed
  git: null
  isbn: 978-1-611974-67-6
  issn: null
  month: '4'
  note: null
  number: null
  pages:
  - '529'
  - '540'
  publisher: SIAM
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: Solving Derivative-Free Nonlinear Least Squares Problems with {POUNDERS}
  type: incollection
  url: http://www.mcs.anl.gov/papers/P5120-0414.pdf
  venue: Advances and Trends in Optimization with Engineering Applications
  volume: null
  web: null
  year: 2017
wong1997sampling:
  address: null
  articleno: null
  authors:
  - - Tien-Tsin
    - Wong
  - - Wai-Shing
    - Luk
  - - Pheng-Ann
    - Heng
  chapter: null
  descrip: Hammersley's and Halton sequences -- other low-discrepancy sequences that
    are commonly used in design-of-experiments
  doi: 10.1080/10867651.1997.10487471
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1086-7651
  month: '1'
  note: null
  number: '2'
  pages:
  - '9'
  - '24'
  publisher: Taylor \& Francis
  series: null
  tags:
  - design of experiments
  - DoE
  - low-discrepancy sequences
  title: Sampling with {H}ammersley and {H}alton points
  type: article
  url: http://www.tandfonline.com/doi/abs/10.1080/10867651.1997.10487471
  venue: Journal of graphics tools
  volume: '2'
  web: null
  year: 1997
wong2016hypervolumebased:
  address: Denver, CO, USA
  articleno: null
  authors:
  - - Cheryl Sze Yin
    - Wong
  - - Abdullah
    - Al-Dujaili
  - - Suresh
    - Sundaram
  chapter: null
  descrip: A hypervolume-based approach to a multiobjective DIRECT algorithm for multiobjective
    blackbox / simulation optimization
  doi: 10.1145/2908961.2931702
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages:
  - '1201'
  - '1208'
  publisher: ACM
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - DIRECT
  - global optimization
  - scalarization
  title: Hypervolume-Based {DIRECT} for Multi-Objective Optimisation
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/2908961.2931702
  venue: Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO
    '16)
  volume: null
  web: null
  year: 2016
wu2025ytopt:
  address: null
  articleno: e8322
  authors:
  - - Xingfu
    - Wu
  - - Prasanna
    - Balaprakash
  - - Michael
    - Kruse
  - - Jaehoon
    - Koo
  - - Brice
    - Videau
  - - Paul
    - Hovland
  - - Valerie
    - Taylor
  - - Brad
    - Geltz
  - - Siddhartha
    - Jana
  - - Mary
    - Hall
  chapter: null
  descrip: 'Official publication for ytopt: Argonne''s HPC and scientific library
    autotuning software using Bayesian optimization to portably autotune numerical
    libraries for optimal performance on a given HPC as part of the exasale computing
    project. The software uses a random forest surrogate model and calculates their
    model-form uncertainties through resampling models. Then, they use random sampling
    of their acquisition function to perform Bayesian optimization at scale with fully
    distributed evaluation of the selected configurations. The results scale well
    on the HPCs Theta and Summit. The open source Python software is available at
    github.com/ytopt-team/ytopt'
  doi: 10.1002/cpe.8322
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1532-0626
  month: '1'
  note: null
  number: '1'
  pages: null
  publisher: Wiley
  series: null
  tags:
  - high-performance computing
  - HPC
  - distributed computing
  - optimization
  - decision trees
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - autotuning
  - software
  - open source
  - OSS
  - Python
  title: 'ytopt: Autotuning Scientific Applications for Energy Efficiency at Large
    Scales'
  type: article
  url: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.8322
  venue: 'Concurrency and Computation: Practice and Experience'
  volume: '37'
  web: null
  year: 2025
xin2018interactive:
  address: null
  articleno: null
  authors:
  - - Bin
    - Xin
  - - Lu
    - Chen
  - - Jie
    - Chen
  - - Hisao
    - Ishibuchi
  - - Kaoru
    - Hirota
  - - Bo
    - Liu
  chapter: null
  descrip: A survey of interactive techniques and software in multiobjective optimization
    including the known challenges limitations
  doi: 10.1109/ACCESS.2018.2856832
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2169-3536
  month: null
  note: null
  number: null
  pages:
  - '41256'
  - '41279'
  publisher: Institute of Electrical and Electronics Engineers (IEEE)
  series: null
  tags:
  - optimization
  - multiobjective optimization
  title: 'Interactive Multiobjective Optimization: A Review of the State-of-the-Art'
  type: article
  url: https://ieeexplore.ieee.org/document/8412189
  venue: IEEE Access
  volume: '6'
  web: null
  year: 2018
xiong2023dreamplacefpgamp:
  address: null
  articleno: null
  authors:
  - - Zhili
    - Xiong
  - - Rachel Selina
    - Rajarathnam
  - - Zhixing
    - Jiang
  - - Hanqing
    - Zhu
  - - David Z.
    - Pan
  chapter: null
  descrip: 'DREAMPlaceFPGA-MP: GPU-accelerated macro-placer for DREAMPlaceFPGA (rajrathnam2022)
    DREAMPlace is an open source using analytical placement with some deep learning
    for FPGA placement acceleration -- written in C++ with Python interfaces and available
    at github.com/rachelselinar/DREAMPlaceFPGA'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.AR preprint
  series: null
  tags:
  - high-performance computing
  - HPC
  - FPGA
  - GPU computing
  - software
  - open source
  - OSS
  - Python
  - CUDA
  - C++
  title: '{DREAMPlaceFPGA-MP}: An Open-Source {GPU}-Accelerated Macro Placer for Modern
    {FPGAs} with Cascade Shapes and Region Constraints'
  type: techreport
  url: https://arxiv.org/abs/2311.08582
  venue: null
  volume: null
  web: null
  year: 2023
xu2020modeling:
  address: null
  articleno: null
  authors:
  - - Li
    - Xu
  - - Yueyao
    - Wang
  - - Thomas
    - Lux
  - - Tyler
    - Chang
  - - Jon
    - Bernard
  - - Bo
    - Li
  - - Yili
    - Hong
  - - Kirk
    - Cameron
  - - Layne
    - Watson
  chapter: null
  descrip: Our VarSys paper on the modeling methodologies that we use for modeling
    the bimodal performance distributions that we observe in IO throughput on our
    experimental performance modeling HPC cluster
  doi: 10.1016/j.jpdc.2020.01.005
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0743-7315
  month: '5'
  note: null
  number: null
  pages:
  - '87'
  - '89'
  publisher: Elsevier BV
  series: null
  tags:
  - high-performance computing
  - HPC
  - performance modeling
  title: Modeling {I/O} performance variability in high-performance computing systems
    using mixture distributions
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S0743731519302746
  venue: Journal of Parallel and Distributed Computing
  volume: '139'
  web: null
  year: 2020
yang2019multiobjective:
  address: null
  articleno: null
  authors:
  - - Kaifeng
    - Yang
  - - Michael
    - Emmerich
  - - Andr{\'{e}}
    - Deutz
  - - Thomas
    - B\"{a}ck
  chapter: null
  descrip: Multiobjective Bayesian optimization with differentiable hypervolume improvement-based
    acquisition
  doi: 10.1016/j.swevo.2018.10.007
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2210-6502
  month: '2'
  note: null
  number: null
  pages:
  - '945'
  - '956'
  publisher: Elsevier BV
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - scalarization
  - evolutionary algorithms
  - EA
  title: Multi-Objective {Bayesian} Global Optimization Using Expected {Hypervolume}
    Improvement Gradient
  type: article
  url: https://linkinghub.elsevier.com/retrieve/pii/S2210650217307861
  venue: Swarm and Evolutionary Computation
  volume: '44'
  web: null
  year: 2019
yang2019multipoint:
  address: Prague Czech Republic
  articleno: null
  authors:
  - - Kaifeng
    - Yang
  - - Pramudita Satria
    - Palar
  - - Michael
    - Emmerich
  - - Koji
    - Shimoyama
  - - Thomas
    - B\"{a}ck
  chapter: null
  descrip: Parallel multiobjective Bayesian optimization with differentiable hypervolume
    improvement-based acquisition
  doi: 10.1145/3321707.3321784
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '7'
  note: null
  number: null
  pages: null
  publisher: ACM
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  - surrogate modeling
  - scalarization
  - evolutionary algorithms
  - EA
  title: A multi-point mechanism of expected hypervolume improvement for parallel
    multi-objective {Bayesian} global optimization
  type: inproceedings
  url: https://dl.acm.org/doi/10.1145/3321707.3321784
  venue: Proc. Genetic and Evolutionary Computation Conference (GECCO19)
  volume: null
  web: null
  year: 2019
yann1998mnist:
  address: null
  articleno: null
  authors:
  - - LeCun
    - Yann
  chapter: null
  descrip: The MNIST dataset presents the first and most popular benchmark problem
    for image classification via neural networks and other AI and machine learning
    methods
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 2025'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - benchmarking
  - classification
  - neural networks
  title: The {MNIST} database of handwritten digits
  type: misc
  url: yann.lecun.com/exdb/mnist
  venue: null
  volume: null
  web: null
  year: 1998
yuan2023active:
  address: null
  articleno: null
  authors:
  - - Xiaoze
    - Yuan
  - - Yuwei
    - Zhou
  - - Qing
    - Peng
  - - Yong
    - Yang
  - - Yongwang
    - Li
  - - Xiaodong
    - Wen
  chapter: null
  descrip: Solving DFT model calibrations for chemical design via active learning
  doi: 10.1038/s41524-023-00967-z
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 2057-3960
  month: '1'
  note: null
  number: '1'
  pages:
  - 12
  publisher: Nature Publishing Group UK London
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - Bayesian optimization
  - global optimization
  title: Active learning to overcome exponential-wall problem for effective structure
    prediction of chemical-disordered materials
  type: article
  url: https://www.nature.com/articles/s41524-023-00967-z
  venue: Nature Computational Materials
  volume: '9'
  web: null
  year: 2023
yukish2004algorithms:
  address: null
  articleno: null
  authors:
  - - Michael
    - Yukish
  chapter: null
  descrip: PhD thesis on extracting high-dimensional (many objective) Pareto fronts
    including a thorough survey of such algorithms
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: The Pennsylvania State University, Dept. of Mechanical Engineering
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - high dimension
  title: Algorithms to identify {P}areto points in multi-dimensional data sets
  type: phdthesis
  url: https://etda.libraries.psu.edu/catalog/6336
  venue: null
  volume: null
  web: null
  year: 2004
zhang2010derivativefree:
  address: null
  articleno: null
  authors:
  - - Hongchao
    - Zhang
  - - Andrew R.
    - Conn
  - - Katya
    - Scheinberg
  chapter: null
  descrip: An algorithm for solving composite sum-of-squares blackbox / simulation
    optimization problems by modeling the blackbox function / simulation's outputs
    using a fully linear model then using the sum-of-squares structure to get a free
    Hessian approximation
  doi: 10.1137/09075531X
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 1052-6234
  month: '1'
  note: null
  number: '6'
  pages:
  - '3555'
  - '3576'
  publisher: Society for Industrial & Applied Mathematics (SIAM)
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: A Derivative-Free Algorithm for Least-Squares Minimization
  type: article
  url: http://epubs.siam.org/doi/10.1137/09075531X
  venue: SIAM Journal on Optimization
  volume: '20'
  web: null
  year: 2010
zhang2012local:
  address: null
  articleno: null
  authors:
  - - Hongchao
    - Zhang
  - - Andrew R.
    - Conn
  chapter: null
  descrip: Convergence analysis when solving composite sum-of-squares blackbox / simulation
    optimization problems by modeling the blackbox function / simulation's outputs
    using a fully linear model then using the sum-of-squares structure to get a free
    Hessian approximation. Basically, one can achieve second-order convergence for
    the price of first-order convergence
  doi: 10.1007/s10589-010-9367-x
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0926-6003
  month: '3'
  note: null
  number: '2'
  pages:
  - '481'
  - '507'
  publisher: Springer Science and Business Media LLC
  series: null
  tags:
  - optimization
  - blackbox optimization
  - simulation optimization
  - derivative-free optimization
  - DFO
  - surrogate modeling
  title: On the Local Convergence of a Derivative-Free Algorithm for Least-Squares
    Minimization
  type: article
  url: http://link.springer.com/10.1007/s10589-010-9367-x
  venue: Computational Optimization and Applications
  volume: '51'
  web: null
  year: 2012
zhang2017understanding:
  address: null
  articleno: null
  authors:
  - - Chiyuan
    - Zhang
  - - Samy
    - Bengio
  - - Moritz
    - Hardt
  - - Benjamin
    - Recht
  - - Oriol
    - Vinyals
  chapter: null
  descrip: An experiment showing that when training labels are replaced with random
    values (pure noise), most neural network classifier methods can still be trained
    to zero training error. (I.e., obviously overfitting). However, regularization
    techniques don't prevent them from doing so. The authors conclude that regularization
    isn't doing what we think it's doing and may not actually be related to generalization
    error
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - neural networks
  - regularization
  - overfitting
  - scientific machine learning
  - SciML
  - interpolation
  - classification
  title: Understanding deep learning requires rethinking generalization
  type: inproceedings
  url: https://openreview.net/forum?id=Sy8gdB9xx
  venue: International Conference on Learning Representations
  volume: null
  web: null
  year: 2017
zhang2023prima:
  address: null
  articleno: null
  authors:
  - - Zaikun
    - Zhang
  chapter: null
  descrip: 'Prima: open source reference implementation of all of Powell''s numerical
    optimization solvers for blackbox / simulation optimization problems in modern
    Fortran. IMO, these should be considered the state-of-the-art and reference implementations
    for all blackbox optimization research'
  doi: 10.5281/zenodo.8052654
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: 'Last accessed: Apr 2025'
  number: null
  pages: null
  publisher: null
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - blackbox optimization
  - derivative-free optimization
  - DFO
  - constrained optimization
  - software
  - open source
  - OSS
  - Fortran
  title: '{PRIMA: Reference Implementation for Powell''s Methods with Modernization
    and Amelioration}'
  type: github repository
  url: http://www.libprima.net
  venue: null
  volume: null
  web: null
  year: 2023
zhao2018multiobjective:
  address: Atlanta, GA, USA
  articleno: null
  authors:
  - - Wei
    - Zhao
  - - Rakesh K.
    - Kapania
  chapter: null
  descrip: An application for multiobjective optimization in the context of aircraft
    wing design. We are optimizing one objective that is the lift/drag ratio, and
    another that describes the controllability. Problem is solved using multiobjective
    particle swarm
  doi: 10.2514/6.2018-3424
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: '6'
  note: null
  number: null
  pages:
  - 3424
  publisher: AIAA
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - blackbox optimization
  - simulation optimization
  title: Multiobjective Optimization of Composite Flying-wings with {SpaRibs} and
    Multiple Control Surfaces
  type: inproceedings
  url: https://arc.aiaa.org/doi/10.2514/6.2018-3424
  venue: Proc. 2018 Multidisciplinary Analysis and Optimization Conference
  volume: null
  web: null
  year: 2018
zhou2023instructionfollowing:
  address: null
  articleno: null
  authors:
  - - Jeffrey
    - Zhou
  - - Tianjian
    - Lu
  - - Swaroop
    - Mishra
  - - Siddhartha
    - Brahma
  - - Sujoy
    - Basu
  - - Yi
    - Luan
  - - Denny
    - Zhou
  - - Le
    - Hou
  chapter: null
  descrip: 'Publication of Google Research IF-Eval benchmark, testing whether a LLM
    is capable of following very particular instructions (including formatting instructions)
    such as "answer in at least 400 words" or "mention AI at least 3 times". The open
    source Python benchmark code is available at: github.com/google-research/google-research/tree/master/instruction_following_eval'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv cs.CL preprint arxXiv:2211.07911
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - LLMs
  - benchmarking
  - open source
  title: Instruction-Following Evaluation for Large Language Models
  type: techreport
  url: https://arxiv.org/abs/2311.07911
  venue: null
  volume: null
  web: null
  year: 2023
zhu1997algorithm:
  address: null
  articleno: null
  authors:
  - - Ciyou
    - Zhu
  - - Richard H.
    - Byrd
  - - Peihuang
    - Lu
  - - Jorge
    - Nocedal
  chapter: null
  descrip: The original publication for L-BFGS-B software, which solves bound-constrained
    optimization problems using a limited-memory BFGS. This is the standard implementation
    that is used in all L-BFGS-B codes to date, such as scipy, all machine learning
    codes, and most engineering codes and nonlinear systems solvers. The code is open
    source high-quality numerical software, written in old-style Fortran
  doi: 10.1145/279232.279236
  edition: null
  editors: []
  git: null
  isbn: null
  issn: 0098-3500
  month: '12'
  note: null
  number: '4'
  pages:
  - '550'
  - '560'
  publisher: ACM
  series: null
  tags:
  - high-performance computing
  - HPC
  - computational linear algebra
  - optimization
  - constrained optimization
  - convex optimization
  - machine learning
  - ML
  - scientific machine learning
  - SciML
  - software
  - open source
  - OSS
  - Fortran
  title: 'Algorithm 778: {L-BFGS-B}: {F}ortran Subroutines for Large-Scale Bound-Constrained
    Optimization'
  type: article
  url: https://dl.acm.org/doi/10.1145/279232.279236
  venue: ACM Transactions on Mathematical Software
  volume: '23'
  web: null
  year: 1997
ziegler2019finetuning:
  address: null
  articleno: null
  authors:
  - - Daniel M.
    - Ziegler
  - - Nisan
    - Stiennon
  - - Jeffrey
    - Wu
  - - Tom B.
    - Brown
  - - Alec
    - Radford
  - - Dario
    - Amodei
  - - Paul
    - Christiano
  - - Geoffrey
    - Irving
  chapter: null
  descrip: 'The original publication from OpenAI for reinforcement learning with human
    feedback (RLHF), a method for fine-tuning large language models (LLMs) and other
    neural network type models using human provided labels (i.e., "cold data") to
    better align model outputs with desired behavior. This is now one of the standard
    approaches to fine-tuning foundational models. The authors propose collecting
    cold start data encompassing human preferences, taking a pre-trained model, and
    further training the weights on the cold start data using proximal policy optimization
    (PPO). Since this predates large language models, this examples are with convolutional
    neural networks (CNNs). There is publicly available code for reproducing their
    results here: github.com/openai/lm-human-preferences'
  doi: null
  edition: null
  editors: []
  git: null
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: arXiv preprint arXiv:1909.08593
  series: null
  tags:
  - AI
  - artificial intelligence
  - machine learning
  - ML
  - reinforcement learning
  - RL
  - LLMs
  - neural networks
  title: Fine-Tuning Language Models from Human Preferences
  type: techreport
  url: https://arxiv.org/abs/1909.08593
  venue: null
  volume: null
  web: null
  year: 2019
zitzler2001spea2:
  address: null
  articleno: null
  authors:
  - - Eckart
    - Zitzler
  - - Marco
    - Laumanns
  - - Lothar
    - Thiele
  chapter: null
  descrip: SPEA2 strength Pareto evolutionary algorithm -- an old evolutionary algorithm
    that was once a competitor to NSGA-II (and with significant overlap in co-authorship),
    but is now largely obsolete.
  doi: 10.3929/ethz-a-004284029
  edition: null
  editors: []
  git: https://github.com/manuparra/spea2
  isbn: null
  issn: null
  month: null
  note: null
  number: null
  pages: null
  publisher: Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH Zurich), Institut
    f{\"u}r Technische
  series: null
  tags:
  - optimization
  - multiobjective optimization
  - evolutionary algorithms
  - EA
  title: '{SPEA2}: Improving the strength {Pareto} evolutionary algorithm'
  type: article
  url: null
  venue: TIK-report
  volume: '103'
  web: null
  year: 2001

% The Dakota blackbox and derivative-free simulation optimization framework, a numerical software package (in C++) maintained by Sandia that offers support for AI/ML surrogate modeling, multifidelity modeling, uncertainty quantification (UQ), and distributed and parallel computing
@techreport{adams2022dakota,
	author = {Adams, Brian M. and Bohnhoff, William J. and Dalbey, Keith R. and Ebeida, Mohamed S. and Eddy, John P. and Eldred, Michael S. and Hooper, Russell W. and Hough, Patricia D. and Hu, Kenneth T. and Jakeman, John D. and Khalil, Mohammad and Maupin, Kathryn A. and Monschke, Jason A. and Ridgeway, Elliott M. and Rushdi, Ahmad A. and Seidl, D. Thomas and Stephens, J. Adam and Swiler, Laura P. and Tran, Anh and Winokur, Justin G.},
	title = {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.16 User's Manual},
	year = {2022},
	number = {SAND2022-6171 version 6.16},
	institution = {Sandia National Laboratory},
	address = {Albuquerque, NM, USA},
	url = {https://dakota.sandia.gov/sites/default/files/docs/6.16.0/Users-6.16.0.pdf},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Using RBF surrogates to solve blackbox / derivative-free multiobjective optimization problems
@article{akhtar2016multi,
	author = {Akhtar, Taimoor and Shoemaker, Christine A.},
	title = {Multi objective optimization of computationally expensive multi-modal functions with {RBF} surrogates and multi-rule selection},
	year = {2016},
	month = {1},
	journal = {Journal of Global Optimization},
	volume = {64},
	number = {1},
	pages = {17--32},
	publisher = {Springer},
	doi = {10.1007/s10898-015-0270-y},
	url = {http://link.springer.com/10.1007/s10898-015-0270-y},
	issn = {0925-5001},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% Optuna open source software for fully distributed hyperparameter optimization. Maintained by a private startup in Japan (named Optuna). This software provides high-quality distributed wrappers for many neural architecture search and generic optimization algorithms.
@inproceedings{akiba2019optuna,
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	title = {Optuna: A next-generation hyperparameter optimization framework},
	year = {2019},
	month = {7},
	booktitle = {Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
	pages = {2623--2631},
	organization = {ACM},
	location = {Anchorage AK USA},
	doi = {10.1145/3292500.3330701},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
	keywords = {optimization, decision trees, Bayesian optimization, global optimization, autotuning, hyperparameter optimization, evolutionary algorithms, EA, Gaussian process},
}

% Introducing an algorithm for solving multiobjective optimization (MOO) problems via the global optimization algorithm DIRECT. Some theory and preliminary results, but no software. Likely not scalable for real-world computationally expensive problems, due to the number of boxes that would need to be divided per iteration using this method. More of a theoretical foundation for a later practical algorithm
@inproceedings{aldujaili2016dividing,
	author = {Al-Dujaili, Abdullah and Suresh, Sundaram},
	title = {Dividing rectangles attack multi-objective optimization},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 IEEE Congress on Evolutionary Computation (CEC '16)},
	pages = {3606--3613},
	organization = {IEEE},
	location = {Vancouver, BC, Canada},
	doi = {10.1109/CEC.2016.7744246},
	url = {http://ieeexplore.ieee.org/document/7744246},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% A numerical software package written in MATLAB -- provides a surrogate modeling toolbox for multiobjective optimization problems
@inproceedings{aldujaili2016matlab,
	author = {Al-Dujaili, Abdullah and Suresh, Sundaram},
	title = {A {MATLAB} toolbox for surrogate-assisted multi-objective optimization: A preliminary study},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
	pages = {1209--1216},
	organization = {ACM},
	location = {Denver, CO, USA},
	doi = {10.1145/2908961.2931703},
	url = {https://dl.acm.org/doi/10.1145/2908961.2931703},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, surrogate modeling, RBFs},
}

% A survey and review of surrogate modeling and response-surface modeling (RSM) techniques used in engineering
@article{alizadeh2020managing,
	author = {Alizadeh, Reza and Allen, Janet K. and Mistree, Farrokh},
	title = {Managing computational complexity using surrogate models: a critical review},
	year = {2020},
	month = {7},
	journal = {Research in Engineering Design},
	volume = {31},
	number = {3},
	pages = {275--298},
	publisher = {Springer},
	doi = {10.1007/s00163-020-00336-7},
	url = {https://link.springer.com/10.1007/s00163-020-00336-7},
	issn = {0934-9839},
	keywords = {optimization, blackbox optimization, simulation optimization, surrogate modeling, RBFs},
}

% A Fortran 90 implementation of quasi-Newton stochastic optimization algorithms. This open source numerical software solves both determinisitc and stochastic blackbox optimization problems via a quasi-Newton trust-region method. It is a bit wasteful in terms of the number of function evaluations per iteration as it performs a fully Latin hypercube sampling of the trust region in each iteration, and does not explicitly re-use previous iterates to reduce iteration costs, like some of the more advanced model based methods. Still, it is extremely robust and a good choice in stochastic situations. Also includes a good Fortran implementation of Latin hypercube sampling and efficient sorting algorithms
@article{amos2020algorithm,
	author = {Amos, Brandon D. and Easterling, David R. and Watson, Layne T. and Thacker, William I. and Castle, Brent S. and Trosset, Michael W.},
	title = {Algorithm 1007: {QNSTOP}: {Q}uasi-{N}ewton algorithm for stochastic optimization},
	year = {2020},
	month = {6},
	journal = {ACM Transactions on Mathematical Software},
	volume = {46},
	number = {2},
	numpages = {17},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3374219},
	url = {https://dl.acm.org/doi/10.1145/3374219},
	issn = {0098-3500},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, stochastic optimization, convex optimization},
}

% The MOSEK solver is a numerical optimization software solver that is used in older versions of scipy.optimize to solve linear programming problems via the interior point method
@inproceedings{andersen2000mosek,
	author = {Andersen, Erling D and Andersen, Knud D},
	title = {The {MOSEK} interior point optimizer for linear programming: an implementation of the homogeneous algorithm},
	year = {2000},
	booktitle = {High performance optimization},
	pages = {197--232},
	organization = {Springer},
	url = {https://link.springer.com/chapter/10.1007/978-1-4757-3216-0_8},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% Is derivative / gradient information counterproductive for solving MOOs? In this paper, it appears to make direct-search type methods perform worse by the metrics used. It could be that the metrics favor diversity over convergence, in which case one can get better diversity by taking bad evaluations. But I need to read more carefully to decide whether that is what's going on here
@article{andreani2022using,
	author = {Andreani, R. and Cust{\'o}dio, Ana Lu{\'i}sa and Raydan, M.},
	title = {Using first-order information in direct multisearch for multiobjective optimization},
	year = {2022},
	month = {11},
	journal = {Optimization Methods and Software},
	volume = {37},
	number = {6},
	pages = {2135--2156},
	publisher = {Taylor \& Francis},
	doi = {10.1080/10556788.2022.2060971},
	url = {https://www.tandfonline.com/doi/full/10.1080/10556788.2022.2060971},
	issn = {1055-6788},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% The SOLAR monte carlo simulation software suite contains a collection of ten derivative-free / blackbox / simulation optimization problem instances for bench-marking blackbox optimization solvers. The problems can be configured with one or two objectives, constrained or unconstrained (including hidden constraints), discrete or continuous, single or multi-fidelity, and with varying levels of stochasticity. The numerical simulation code models a solar power plant's yield as a function of various design factors. The open source C++ software with Python bindings is available at: github.com/bbopt/solar
@article{andresthio2025solar,
	author = {Andr\'{e}s-Thi\'{o}, Nicolau and Audet, Charles and Diago, Miguel and Gheribi, A{\"i}men E. and Le~Digabel, Sebastien and Lebeuf, Xavier and Lemyre~Garneau, Mathieu and Tribes, Christophe},
	title = {solar: A solar thermal power plant simulator for blackbox optimization benchmarking},
	year = {2025},
	month = {3},
	journal = {Optimization and Engineering},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11081-024-09952-x},
	url = {https://link.springer.com/10.1007/s11081-024-09952-x},
	issn = {1389-4420},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, stochastic optimization, benchmarking},
}

% Introducing the open source PDLP solver (now part of Google OR tools) for solving large scale linear programming problems, where the constraint matrices would never fit in memory using first-order (matrix free) descent methods. The algorithm is based on PDHG, which is an improvement to ADMM specialized for solving LPs. The main contributions of this solver and its predecessor PDHG are adaptive step-sizes and preconditioning to prevent oscillatory motions plus adpaptive an adaptive restart procedures and presolve to prevent stalling out. This solver is not as fast simplex or interior point methods on problems that do fit in memory, but is extremely scalable and robust on larger problems. Obviously, it cannot give a basic solution since first-order methods are always approximate (with order 10^-8 accuracy). Open source code is available through Google OR tools: github.com/google/or-tools
@inproceedings{applegate2021practical,
	author = {Applegate, David and Diaz, Mateo and Hinder, Oliver and Lu, Haihao and Lubin, Miles and O\textquotesingle Donoghue, Brendan and Schudy, Warren},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	title = {Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient},
	year = {2021},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {34},
	pages = {20243--20257},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a8fbbd3b11424ce032ba813493d95ad7-Paper.pdf},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization, high dimension},
}

% Applying grey-box bayesian optimization tutorial: using Bayesian optimization on structured problems, where a blackbox function is composed with an algebraic function, just like with ParMOO and Jeff's GOOMBAH paper. Tutorial performed using BoTorch
@inproceedings{astudillo2021thinking,
	author = {Astudillo, Raul and Frazier, Peter I.},
	title = {Thinking inside the box: a tutorial on grey-box bayesian optimization},
	year = {2021},
	month = {12},
	booktitle = {Proc. 2021 Winter Simulation Conference (WSC 2021)},
	articleno = {2},
	numpages = {15},
	organization = {IEEE},
	location = {Phoenix, Arizona},
	doi = {10.1109/WSC52266.2021.9715343},
	url = {https://ieeexplore.ieee.org/document/9715343},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, Gaussian process},
}

% BiMADS -- a biobjective direct search / generalized pattern search via the MADS algorithm with an adaptive weighting scheme to trace the Pareto front from one end to the other. The numerical software implementation is part of the NOMAD software package (written in C++)
@article{audet2008multiobjective,
	author = {Audet, Charles and Savard, Gilles and Zghal, Walid},
	title = {Multiobjective optimization through a series of single-objective formulations},
	year = {2008},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {19},
	number = {1},
	pages = {188--210},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/060677513},
	url = {http://epubs.siam.org/doi/10.1137/060677513},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search},
}

% Introducing the MADS algorithm of variable neighborhood search plus a generalized pattern search adapted to a mesh. Also includes the STYRENE benchmark problem and application for derivative-free blackbox and simulation optimization algorithms. The objective is to maximize the net present value subject to several process and economic constraints. The open source C++ numerical simulation code that defines the problem is available at github.com/bbopt/styrene
@article{audet2008nonsmooth,
	author = {Audet, Charles and B\'echard, Vincent and Le~Digabel, Sebastien},
	title = {Nonsmooth optimization through Mesh Adaptive Direct Search and Variable Neighborhood Search},
	year = {2008},
	month = {6},
	journal = {Journal of Global Optimization},
	volume = {41},
	number = {2},
	pages = {299--318},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10898-007-9234-1},
	url = {http://link.springer.com/10.1007/s10898-007-9234-1},
	issn = {0925-5001},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, constrained optimization, benchmarking},
}

% The progressive barrier penalty for nonlinear blackbox optimization methods. Basically adds a progressive penalty for violating constraints based on the distance to feasibility
@article{audet2009progressive,
	author = {Audet, Charles and Dennis, John E.},
	title = {A Progressive Barrier for Derivative-Free Nonlinear Programming},
	year = {2009},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {20},
	number = {1},
	pages = {445--472},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/070692662},
	url = {http://epubs.siam.org/doi/10.1137/070692662},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization},
}

% Multi-MADS -- a multiobjective direct search / generalized pattern search via MADS algorithm, using normal boundary intersection (NBI) for adaptive weighting Also includes a multiobjective (three objective) variation of the STYRENE benchmark for derivative-free blackbox and simulation optimization problems. The STYRENE benchmark problem and application for derivative-free blackbox and simulation optimization algorithms. The objective is to maximize the net present value subject to several process and economic constraints. The open source C++ numerical simulation code that defines the problem is available at github.com/bbopt/styrene
@article{audet2010mesh,
	author = {Audet, Charles and Savard, Gilles and Zghal, Walid},
	title = {A mesh adaptive direct search algorithm for multiobjective optimization},
	year = {2010},
	month = {8},
	journal = {European Journal of Operational Research},
	volume = {204},
	number = {3},
	pages = {545--556},
	publisher = {Elsevier BV},
	doi = {10.1016/j.ejor.2009.11.010},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221709008601},
	issn = {0377-2217},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, constrained optimization, benchmarking},
}

% Book on fundamental methods, terminology, and theory in blackbox and derivative-free optimization (DFO) -- covers topics such as definitions of blackbox functions, heuristics, classical methods, positive bases and minimum spanning sets, generalized pattern search, and direct search, fully linear and quadratic models, model-drive descent and trust-region methods and ensuring model quality, general surrogate modeling, constraints, and multiobjective basics.
@book{audet2017derivativefree,
	author = {Audet, Charles and Hare, Warren},
	title = {Derivative-free and blackbox optimization},
	year = {2017},
	booktitle = {Springer Series in Operations Research and Financial Engineering},
	series = {Springer Series in Operations Research and Financial Engineering},
	publisher = {Springer International},
	address = {Charm, Switzerland},
	doi = {10.1007/978-3-319-68913-5},
	url = {http://link.springer.com/10.1007/978-3-319-68913-5},
	isbn = {9783319689128},
	issn = {1431-8598},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, constrained optimization, surrogate modeling},
}

% A thorough survey of over 50 commonly used performance indicators in multiobjective optimization -- key takeaways: performance metrics can measure different properties of an algorithm, such as whether it is converging to the true Pareto front, the coverage of the true Pareto front, and the average diversity of solutions. One of the only metrics that is monotonic (i.e., cannot become worse when a solution contains a previous solution) is the hypervolume indicator, which is the standard in evolutionary algorithms
@article{audet2021performance,
	author = {Audet, Charles and Bigeon, Jean and Cartier, Dominique and Digabel, Sébastien Le and Salomon, Ludovic},
	title = {Performance indicators in multiobjective optimization},
	year = {2021},
	month = {7},
	journal = {European Journal of Operational Research},
	volume = {292},
	number = {2},
	pages = {397--422},
	publisher = {Elsevier BV},
	doi = {10.1016/j.ejor.2020.11.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221720309620},
	issn = {0377-2217},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% Official StoMADS reference: StoMADS is the stochastic version of the MADS (generalized pattern search) optimization algorithm and software. The open source StoMADS numerical software is available at github.com/bbopt/StoMADS. It is implemented in MATLAB. The main contribution of StoMADS is to calculate how many resamples and shrinking step sizes are needed to guarantee convergence of the MADS algorithm on problems where the blackbox function / simulation evaluation is stochastic
@article{audet2021stochastic,
	author = {Audet, Charles and Dzahini, Kwassi Joseph and Kokkolaras, Michael and Le~Digabel, Sebastien},
	title = {Stochastic mesh adaptive direct search for blackbox optimization using probabilistic estimates},
	year = {2021},
	month = {5},
	journal = {Computational Optimization and Applications},
	volume = {79},
	number = {1},
	pages = {1--34},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-020-00249-0},
	url = {https://doi.org/10.1007/s10589-020-00249-0},
	issn = {0926-6003},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, stochastic optimization},
}

% NOMAD v4 -- open source numerical software package (in C++) for blackbox and derivative-free optimization via the MADS algorithms. After publication, they have added support for multiobjective optimization, mixed variables, nonlinear constraints, etc. Great example of high-impact open source numerical and optimization software. Improvements over NOMAD v3 include improvements to fundamental algorithms, coding practices, release process, and general project structure to support continuous research and development into the future
@article{audet2022algorithm,
	author = {Audet, Charles and Le Digabel, S\'{e}bastien and Rochon Montplaisir, Viviane and Tribes, Christophe},
	title = {{Algorithm 1027}: {NOMAD} Version 4: Nonlinear Optimization with the {MADS} Algorithm},
	year = {2022},
	month = {9},
	journal = {ACM Transactions on Mathematical Software},
	volume = {48},
	number = {3},
	articleno = {35},
	numpages = {22},
	publisher = {ACM},
	doi = {10.1145/3544489},
	url = {https://dl.acm.org/doi/10.1145/3544489},
	issn = {0098-3500},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, constrained optimization},
}

% Handling categorical/integer/mixed variables in blackbox optimization: This is the method used to perform hyperparameter tuning of neural-networks and other AI models via MADS. In general, they decompose variables into meta variables (which determine whether other variables are active or not, such as the number of layers in the network which can deactivate variables associated with inactive layers), categorical variables (which either need to be embedded somehow or can be explored in an unordered manner via direct search / generalized pattern search), and finally standard variables which includes both continuous and relaxed integer variables
@inproceedings{audet2023general,
	author = {Audet, Charles and Hall{\'e}-Hannan, Edward and Le Digabel, S{\'e}bastien},
	title = {A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables},
	year = {2023},
	month = {2},
	booktitle = {Operations Research Forum},
	volume = {4},
	number = {1},
	numpages = {12},
	organization = {Springer},
	doi = {10.1007/s43069-022-00180-6},
	url = {https://link.springer.com/10.1007/s43069-022-00180-6},
	issn = {2662-2556},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, mixed-variable optimization, autotuning, hyperparameter optimization},
}

% BoTorch: Modular bayesian optimization framework and numerical software package. Uses Ax for deploying A/B testing applications. Uses pytorch for autograd, and uses monte carlo sampling and kernel reparameterization tricks in order to efficiently evaluate composite objective and acquisition functions and non gaussian kernels. Great example of high-impact open source numerical software and optimization software
@inproceedings{balandat2020botorch,
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	title = {{BoTorch}: A Framework for Efficient {M}onte-{C}arlo {B}ayesian Optimization},
	year = {2020},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {21524--21538},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, mixed-variable optimization, surrogate modeling, autograd, backpropagation, Gaussian process},
}

% Prasanna and Stefan's original DeepHyper publication -- DeepHyper is an open source extreme-scale distributed optimization package, designed to scale to Argonne's exascale HPCs. Able to train ensembles of neural networks with diverse architectures at scale, with both single and multiobjective hyperparameter tuning support
@inproceedings{balaprakash2018deephyper,
	author = {Balaprakash, Prasanna and Salim, Michael and Uram, Thomas D. and Vishwanath, Venkat and Wild, Stefan M.},
	title = {{DeepHyper}: Asynchronous hyperparameter search for deep neural networks},
	year = {2018},
	month = {12},
	booktitle = {IEEE 25th international conference on high performance computing (HiPC)},
	pages = {42--51},
	organization = {IEEE},
	location = {Bengaluru, India},
	doi = {10.1109/hipc.2018.00014},
	url = {https://ieeexplore.ieee.org/document/8638041},
	keywords = {optimization, decision trees, Bayesian optimization, global optimization, mixed-variable optimization, autotuning, hyperparameter optimization, surrogate modeling},
}

% The PETSc user's guide. I haven't used it but PETSc is a widely-used C++ numerical software library and linear algebra / iterative algorithms framework developed at Argonne and used for implementing many well-known iterative solvers, especially in the area of CFD. This is a great example of high-impact open source numerical software and best practices in open source scientific software. Now ships together with TAO, a similar simulation optimization software package
@techreport{balay2022petsctao,
	author = {Balay, Satish and Abhyankar, Shrirang and Adams, Mark F. and Benson, Steven and Brown, Jed and Brune, Peter and Buschelman, Kris and Constantinescu, Emil and Dalcin, Lisandro and Dener, Alp and Eijkhout, Victor and Gropp, William D. and Hapla, V\'{a}clav and Isaac, Tobin and Jolivet, Pierre and Karpeev, Dmitry and Kaushik, Dinesh and Knepley, Matthew G. and Kong, Fande and Kruger, Scott and May, Dave A. and McInnes, Lois Curfman and Mills, Richard Tran and Mitchell, Lawrence and Munson, Todd and Roman, Jose E. and Rupp, Karl and Sanan, Patrick and Sarich, Jason and Smith, Barry F. and Zampini, Stefano and Zhang, Hong and Zhang, Hong and Zhang, Junchao},
	title = {{PETSc/TAO} Users Manual},
	year = {2022},
	number = {ANL-21/39 - Revision 3.17},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://petsc.org/release/docs/manual/manual.pdf},
	keywords = {optimization, simulation optimization, convex optimization},
}

% Official PROX-QP paper -- the best open source QP solver according to several benchmarks. Open source C++ code is availabe through the proxsuite github: https://github.com/Simple-Robotics/proxsuite and a Python interface is available through CvxPy
@inproceedings{bambade2022proxqp,
	author = {Bambade, Antoine and El-Kazdadi, Sarah and Taylor, Adrien and Carpentier, Justin},
	title = {{PROX-QP}: Yet another Quadratic Programming Solver for Robotics and beyond},
	year = {2022},
	month = {June},
	booktitle = {RSS 2022 - Robotics: Science and Systems},
	organization = {Robotics: Science and Systems Foundation},
	location = {New York, United States},
	doi = {10.15607/rss.2022.xviii.040},
	url = {https://hal.inria.fr/hal-03683733},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% AMOSA algorithm -- apparently this is a widely-known standard in multiobjective simulated annealing because reviewers regularly ask me to cite this. But I've never met anyone who uses this and I can't find the software anywhere. The algorithm seems very reasonable though
@article{bandyopadhyay2008simulated,
	author = {Bandyopadhyay, Sanghamitra and Saha, Sriparna and Maulik, Ujjwal and Deb, Kalyanmoy},
	title = {A Simulated Annealing-Based Multiobjective Optimization Algorithm: {AMOSA}},
	year = {2008},
	month = {6},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {12},
	number = {3},
	pages = {269--283},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TEVC.2007.900837},
	url = {http://ieeexplore.ieee.org/document/4358775},
	issn = {1941-0026},
	keywords = {optimization, multiobjective optimization, simulated annealing, SA},
}

% JAHS-Bench-201: The latest test suite of benchmark problems for neural architecture search. The baseline is random search, but you can solve the problems in their parameterized search space with any optimization algorithm, record the number of true function / simulation evaluations (i.e., networks trained) and submit this to the JAHS-Bench leaderboards on GitHub. This is a good representative test problem for NAS. Both single and multiobjective benchmarks are provided, also most problems can be run in both single or multifidelity evaluation modes
@inproceedings{bansal2022jahsbench201,
	author = {Bansal, Archit and Stoll, Danny and Janowski, Maciej and Zela, Arber and Hutter, Frank},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	title = {{JAHS-Bench-201}: A Foundation For Research On Joint Architecture And Hyperparameter Search},
	year = {2022},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {35},
	pages = {38788--38802},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/fd78f2f65881c1c7ce47e26b040cf48f-Paper-Datasets_and_Benchmarks.pdf},
	keywords = {optimization, multiobjective optimization, mixed-variable optimization, autotuning, hyperparameter optimization, benchmarking},
}

% An open source numerical software library for solving multiobjective optimization problems in java in real-time via heuristics. The authors combine jMetal with data streaming via Apache Spark to solve distributed multiobjective optimization problems with streaming data in real-time
@article{barbagonzález2018jmetalsp,
	author = {Barba-González, Cristóbal and García-Nieto, José and Nebro, Antonio J. and Cordero, José A. and Durillo, Juan J. and Navas-Delgado, Ismael and Aldana-Montes, José F.},
	title = {{jMetalSP}: A framework for dynamic multi-objective big data optimization},
	year = {2018},
	month = {8},
	journal = {Applied Soft Computing},
	volume = {69},
	pages = {737--748},
	publisher = {Elsevier BV},
	doi = {10.1016/j.asoc.2017.05.004},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494617302557},
	issn = {1568-4946},
	keywords = {optimization, multiobjective optimization, evolutionary algorithm, EA},
}

% jMetalPy -- a Python framework for solving MOOs with EAs -- this open source numerical software is a Python implementation of jMetal, with some improvements to code quality and new features for better open source software development and parallelism
@article{benitezhidalgo2019jmetalpy,
	author = {Ben{\'i}tez-Hidalgo, Antonio and Nebro, Antonio J. and Garc{\'i}a-Nieto, Jos{\'e} and Oregi, Izaskun and Ser, },
	title = {{jMetalPy}: A {P}ython framework for multi-objective optimization with metaheuristics},
	year = {2019},
	month = {12},
	journal = {Swarm and Evolutionary Computation},
	volume = {51},
	numpages = {100598},
	publisher = {Elsevier BV},
	doi = {10.1016/j.swevo.2019.100598},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650219301397},
	issn = {2210-6502},
	keywords = {optimization, multiobjective optimization},
}

% Original tree-parzen estimator (TPE) reference. The authors observe that the structure of most hyperparameter optimization problems is tree-like in that the values of certain parameters are only relevant given the choices of earlier parameters. This leads them to train a statistical distribution over the tree of decisions, which will allow them to traverse the tree with high probability of sampling good models. They show that this is better than Bayesian optimization with Gaussian processes and random search. They also have the insight (still holds true today) that for these problems, it is difficult to do better than random search
@inproceedings{bergstra2011algorithms,
	author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
	editor = {Shawe-Taylor, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K.Q.},
	title = {Algorithms for Hyper-Parameter Optimization},
	year = {2011},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {24},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
	keywords = {optimization, decision trees, mixed-variable optimization, autotuning, hyperparameter optimization},
}

% A numerical algorithm for multiobjective descent, using RBF surrogates + trust regions. Builds heavily off of Stefan's PhD thesis (ORBIT)
@article{berkemeier2021derivativefree,
	author = {Berkemeier, Manuel and Peitz, Sebastian},
	title = {Derivative-Free Multiobjective Trust Region Descent Method Using Radial Basis Function Surrogate Models},
	year = {2021},
	month = {4},
	journal = {Mathematical and Computational Applications},
	volume = {26},
	number = {2},
	numpages = {31},
	publisher = {Multidisciplinary Digital Publishing Institute},
	doi = {10.3390/mca26020031},
	url = {https://www.mdpi.com/2297-8747/26/2/31},
	issn = {2297-8747},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% Proof that the complexity of calculating the hypervolume indicator with o objectives is exponential. Roughly the same reasons that calculating simplices in an o-dimensional Delaunay triangulation or computing the facets of an o-dimensional convex hull are exponential
@article{beume2009complexity,
	author = {Beume, Nicola and Fonseca, Carlos M. and Lopez-Ibanez, Manuel and Paquete, Luis and Vahrenhold, Jan},
	title = {On the complexity of computing the hypervolume indicator},
	year = {2009},
	month = {10},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {13},
	number = {5},
	pages = {1075--1082},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2009.2015575},
	url = {http://ieeexplore.ieee.org/document/5208224},
	issn = {1941-0026},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% Comparison between VPR placement runtime and modern ASIC-based analytic placement runtimes. Conclusion is that simulated annealing is more accurate, but analytic placement is faster
@inproceedings{bian2010towards,
	author = {Bian, Huimin and Ling, Andrew C. and Choong, Alexander and Zhu, Jianwen},
	title = {Towards scalable placement for {FPGA}s},
	year = {2010},
	month = {2},
	booktitle = {Proceedings of the 18th Annual ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
	series = {FPGA '10},
	numpages = {10},
	organization = {Association for Computing Machinery},
	location = {Monterey, California, USA},
	doi = {10.1145/1723112.1723140},
	url = {https://doi.org/10.1145/1723112.1723140},
	isbn = {9781605589114},
	keywords = {optimization, mixed-variable optimization, simulated annealing, SA},
}

% NASA's FUN3D CFD solver. This is one of the oldest and standard numerical softwares for solving CFD problems. Written in mostly Fortran 90. Uses a form of the problem that yields the adjoints, which can be used to optimize structures in fewer steps and perform sensitivity analyses. The kernel uses an iterative solver to solve a massive block-sparse linear system (I think derived from the weak form). Some a priori multiobjective optimization solvers are described in Section 9.9
@techreport{biedron2019fun3d,
	author = {Biedron, Robert T. and Carlson, Jan Renee and Derlaga, Joseph M. and Gnoffo, Peter A. and Hammond, Dana P. and Jones, William T. and Kleb, Bill and Lee-Rausch, Elizabeth M. and Nielson, Eric J. and Park, Michael A. and Rumsey, Christopher L. and Thomas, James L. and Thompson, Kyle B. and Wood, William A.},
	title = {{FUN3D Manual}: 13.6},
	year = {2019},
	number = {{NASA} {T}echnical {M}emorandum ({TM}) 2019-220416},
	institution = {NASA Langley Research Center},
	address = {Hampton, VA, USA},
	url = {https://fun3d.larc.nasa.gov/papers/FUN3D_Manual-13.6.pdf},
	keywords = {optimization, multiobjective optimization, simulation optimization, autotuning},
}

% DMulti-MADS: Improved Multi-MADS using direct search / generalized pattern search plus some improvements to the Multi-MADS algorithm -- I need to re-read this to remember what the improvements were
@article{bigeon2020dmultimads,
	author = {Bigeon, Jean and Le Digabel, S{\'e}bastien and Salomon, Ludovic},
	title = {{DM}ulti-{MADS}: {M}esh adaptive direct multisearch for blackbox multiobjective optimization},
	year = {2020},
	month = {6},
	journal = {Computational Optimization and Applications},
	volume = {79},
	number = {2},
	pages = {301--338},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-021-00272-9},
	url = {https://link.springer.com/10.1007/s10589-021-00272-9},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search},
}

% pagmo/pygmo - Parallel frameworks for solving multiobjective optimization problems (MOO) in Java and Python. Great example of open source numerical software, published in JOSS
@article{biscani2020parallel,
	author = {Biscani, Francesco and Izzo, Dario},
	title = {A parallel global multiobjective framework for optimization: pagmo},
	year = {2020},
	month = {9},
	journal = {Journal of Open Source Software},
	volume = {5},
	number = {53},
	numpages = {2338},
	publisher = {The Open Journal},
	doi = {10.21105/joss.02338},
	url = {https://joss.theoj.org/papers/10.21105/joss.02338},
	issn = {2475-9066},
	keywords = {optimization, multiobjective optimization},
}

% pymoo is an open source software package implementing NSGA-II, NSGA-III, and many other multiobjective evolutionary algorithms (MOEAs), plus extensions for handling things such as categorical variables. This is a well-maintained and well-documented open-source numerical software package. It is maintained by the lab of the original NSGA-II author, and therefore could be considered the official NSGA-II implementation. all source code in Python
@article{blank2020pymoo,
	author = {Blank, Julian and Deb, Kalyanmoy},
	title = {{pymoo}: Multi-Objective Optimization in {Python}},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {89497--89509},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2020.2990567},
	url = {https://ieeexplore.ieee.org/document/9078759},
	issn = {2169-3536},
	git = {http://github.com/anyoptimization/pymoo},
	keywords = {optimization, multiobjective optimization, mixed-variable optimization, evolutionary algorithms, EA, evolutionary algorithm},
}

% Methodolgies for calibrating the Fayans EDF model to experimental data. Data is expensive and limited and the model itself is computationally expensive, so this is a classical inverse problem. The problem is actually multiobective because the data themselves come from various categories representing different types of observations, and the standard deviations for each of these observables is not known. Could be configured as a 3 or 9-objective problem
@article{bollapragada2020optimization,
	author = {Bollapragada, Raghu and Menickelly, Matt and Nazarewicz, Witold and O'Neal, Jared and Reinhard, Paul-Gerhard and Wild, Stefan M.},
	title = {Optimization and supervised machine learning methods for fitting numerical physics models without derivatives},
	year = {2020},
	month = {2},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	volume = {48},
	number = {2},
	numpages = {24001},
	publisher = {IOP Publishing},
	doi = {10.1088/1361-6471/abd009},
	url = {https://iopscience.iop.org/article/10.1088/1361-6471/abd009},
	issn = {0954-3899},
	keywords = {optimization, multiobjective optimization, simulation optimization},
}

% The open source numerical software package (in Python) pySMT. This is a surrogate modeling and Bayesian optimization toolbox for solving multidisciplinary engineering design optimization (MDO) problems, while utilizing derivatives and providing numerical stability analysis for each surrogate model class.
@article{bouhlel2019python,
	author = {Bouhlel, Mohamed Amine and Hwang, John T. and Bartoli, Nathalie and Lafage, Rémi and Morlier, Joseph and Martins, Joaquim R.R.A.},
	title = {A {P}ython surrogate modeling framework with derivatives},
	year = {2019},
	month = {9},
	journal = {Advances in Engineering Software},
	volume = {135},
	pages = {102--662},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2019.03.005},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0965997818309360},
	issn = {0965-9978},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, mixed-variable optimization, surrogate modeling, RBFs, Gaussian process},
}

% The famous textbook on convex optimization (from which I learned most concepts) covering concepts such as basic convexity definitions and theorems, basic algorithms and optimality conditions, handling constraints, Lagrangian duality, multiobjective optimization basics, gradient descent and newton's method, sequential quadratic programming, linear programming, and a few applications and modeling basics
@book{boyd2004convex,
	author = {Boyd, Stephen P and Vandenberghe, Lieven},
	title = {Convex optimization},
	year = {2004},
	publisher = {Cambridge university press},
    url = {https://stanford.edu/~boyd/cvxbook/},
	keywords = {optimization, multiobjective optimization, constrained optimization, linear programming, quadratic programming, QP, LP, convex optimization},
}

% A Multiobjective Bayesian optimization algorithm, very similar to ParEGO -- this algorithm uses the Gaussian process surrogates with NSGA-II to solve the problem. However, spectral sampling and thompson sampling are then employed to subselect a diverse set of candidates for batch evaluation. The resulting algorithm is called TSEMO
@article{bradford2018efficient,
	author = {Bradford, Eric and Schweidtmann, Artur M. and Lapkin, Alexei},
	title = {Efficient multiobjective optimization employing {Gaussian} processes, spectral sampling and a genetic algorithm},
	year = {2018},
	month = {6},
	journal = {Journal of Global Optimization},
	volume = {71},
	number = {2},
	pages = {407--438},
	publisher = {Springer},
	doi = {10.1007/s10898-018-0609-2},
	url = {http://link.springer.com/10.1007/s10898-018-0609-2},
	issn = {0925-5001},
	keywords = {optimization, blackbox optimization, Bayesian optimization, global optimization, surrogate modeling, evolutionary algorithm, EA, Gaussian process},
}

% A study on utilizing polynomial surrogate models during multiobjective direct search and generalized pattern search techniques
@article{bras2020use,
	author = {Br{\'a}s, Carmo P. and Cust{\'o}dio, Ana Lu{\'\i}sa},
	title = {On the use of polynomial models in multiobjective directional direct search},
	year = {2020},
	month = {12},
	journal = {Computational Optimization and Applications},
	volume = {77},
	number = {3},
	pages = {897--918},
	publisher = {Springer},
	doi = {10.1007/s10589-020-00233-8},
	url = {https://link.springer.com/10.1007/s10589-020-00233-8},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, surrogate modeling},
}

% Analysis of hypervolume indicator as proxy for solution set quality -- results for 2-objectives only, this paper shows that the hypervolume indicator is the best single indicator we have, but with some caveates
@article{bringmann2013approximation,
	author = {Bringmann, Karl and Friedrich, Tobias},
	title = {Approximation quality of the hypervolume indicator},
	year = {2013},
	month = {2},
	journal = {Artificial Intelligence},
	volume = {195},
	number = {0004-3702},
	pages = {265--290},
	publisher = {Elsevier BV},
	doi = {10.1016/j.artint.2012.09.005},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370212001178},
	issn = {0004-3702},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% The open source numerical software MODIR is proposed here. MODIR can be used to solve multiobjective blackbox optimization problems via a multiobjective variant of the DIRECT blackbox algorithm (direct search method). The motivating application is a multidisciplinary shiphull engineering design problem
@article{campana2018multiobjective,
	author = {Campana, Emilio Fortunato and Diez, Matteo and Liuzzi, Giampaolo and Lucidi, Stefano and Pellegrini, Riccardo and Piccialli, Veronica and Rinaldi, Francesco and Serani, Andrea},
	title = {A multi-objective {DIRECT} algorithm for ship hull optimization},
	year = {2018},
	month = {9},
	journal = {Computational Optimization and Applications},
	volume = {71},
	number = {1},
	pages = {53--72},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-017-9955-0},
	url = {http://link.springer.com/10.1007/s10589-017-9955-0},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% A really nice suite of quadratic programming (QP) benchmark problems
@misc{caron2024qpbenchmark,
	author = {Caron, Stéphane and Zaki, Akram and Otta, Pavel and Arnström, Daniel and Carpentier, Justin and Yang, Fengyu and Leziart, Pierre-Alexandre},
	title = {qpbenchmark: Benchmark for quadratic programming solvers available in {Python}},
	year = {2024},
	booktitle = {GitHub repository},
	number = {2.4.0},
	publisher = {GitHub},
	git = {https://github.com/qpsolvers/qpbenchmark},
	note = {Last accessed: Apr 29, 2025},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization, benchmarking},
}

% The DelaunaySparse software, demonstrates how to calculate simplices from a Delauay triangulation in very high dimensions scalably (and in parallel) using a highly customized simplex method like solver. The resulting Fortran numerical software is fully open source with a C and Python interface
@article{chang2020algorithm,
	author = {Chang, Tyler H. and Watson, Layne T. and Lux, Thomas C. H. and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Algorithm 1012: {DELAUNAYSPARSE}: {I}nterpolation via a sparse subset of the {D}elaunay triangulation in medium to high dimensions},
	year = {2020},
	month = {12},
	journal = {home},
	series = {Collections of the ACM},
	volume = {46},
	number = {4},
	articleno = {38},
	numpages = {20},
	publisher = {Association of Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3422818},
	url = {https://dl.acm.org/doi/10.1145/3422818},
	issn = {0098-3500},
	git = {https://github.com/vtopt/DelaunaySparse},
	web = {https://vtopt.github.io/DelaunaySparse},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% Paper on the challenges of integrating VTMOP into the libEnsemble parallel computing Python software library at Argonne
@inproceedings{chang2020managing,
	author = {Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T. and Lux, Thomas C. H.},
	title = {Managing computationally expensive blackbox multiobjective optimization problems using {libEnsemble}},
	year = {2020},
	booktitle = {Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20)},
	numpages = {31},
	organization = {SCS},
	location = {Fairfax, VA, USA},
	doi = {10.22360/SpringSim.2020.HPC.001},
	url = {https://dl.acm.org/doi/abs/10.5555/3408207.3408245},
	keywords = {optimization, multiobjective optimization, simulation optimization, blackbox optimization, derivative-free optimization, DFO},
}

% My PhD thesis, including multiobjective optimization techniques, algorithm, performance analysis, and software review; description of VTMOP, running parallel simulations, integrating with libE. Also scientific machine learning via Delaunay interpolation and algorithms and proofs for doing so. Several applications related to HPC performance modeling and autotuning.
@phdthesis{chang2020mathematical,
	author = {Chang, Tyler H.},
	title = {Mathematical Software for Multiobjective Optimization Problems},
	year = {2020},
	school = {Virginia Tech, Dept. of Computer Science},
	url = {https://vtechworks.lib.vt.edu/handle/10919/98915},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% A study on the multiobjective optimization of the LINPACK benchmark's config files on the leadership class HPC Bebop at Argonne National Laboratory. We used VTMOP but some modifications were required to ensure that mixed variables were properly handled. Some of the techniques that we used here inspired me to provide automatic support in ParMOO. Ultimately, we achieve 3x reduction in performance variability without sacrificing max/mean throughput.
@inproceedings{chang2020multiobjective,
	author = {Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T.},
	title = {Multiobjective optimization of the variability of the high-performance {LINPACK} solver},
	year = {2020},
	month = {12},
	booktitle = {Proc. 2020 Winter Simulation Conference (WSC 2020)},
	pages = {3081--3092},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC48552.2020.9383875},
	url = {https://ieeexplore.ieee.org/document/9383875},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, autotuning, surrogate modeling},
}

% Publication of my second open source numerical software package: VTMOP a Fortran software for solving blackbox multiobjective optimization problems. Uses surrogate modeling (RSM), with an adaptive weighting scheme, within a trust region framework. The motivating application is a particle accelerator tuning problem at SLAC
@article{chang2022algorithm,
	author = {Chang, Tyler H. and Watson, Layne T. and Larson, Jeffrey and Neveu, Nicole and Thacker, William I. and Deshpande, Shubhangi and Lux, Thomas C. H.},
	title = {{Algorithm 1028}: {VTMOP}: Solver for Blackbox Multiobjective Optimization Problems},
	year = {2022},
	month = {9},
	journal = {{ACM} Transactions on Mathematical Software},
	volume = {48},
	number = {3},
	numpages = {36},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3529258},
	url = {https://dl.acm.org/doi/10.1145/3529258},
	issn = {0098-3500},
	git = {https://github.com/Libensemble/libe-community-examples/tree/main/vtmop},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Using ParMOO software with the MDML software (wrapper on Apache Kafka with automatic data logging and analysis dashboard) in order to optimize chemical manufacturing processes in a wet-lab environment. The kafka querries are sent directly to a continuous flow reactor (CFR) through a smart-lab setup in the MERF at Argonne. Through this setup, ParMOO is able to automatically steer the solvents, bases, temperatures, flow rates, and mixing ratios of a complex chemical manufacturing process in order to produce optimized yields and purities -- achieving multi-hundred-fold improvement over the previous manual process
@inproceedings{chang2023framework,
	author = {Chang, Tyler H. and Elias, Jakob R. and Wild, Stefan M. and Chaudhuri, Santanu and Libera, Joseph A.},
	title = {A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps},
	year = {2023},
	booktitle = {11th International Conference on Learning Representations (ICLR 2023), Workshop on Machine Learning for Materials (ML4Materials)},
	numpages = {10},
	location = {Kigali, Rwanda},
	url = {https://openreview.net/forum?id=8KJS7RPjMqG},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, mixed-variable optimization, surrogate modeling, RBFs},
}

% The ParMOO JOSS article -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@article{chang2023parmoo,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {{ParMOO}: A {P}ython library for parallel multiobjective simulation optimization},
	year = {2023},
	month = {2},
	journal = {Journal of Open Source Software},
	volume = {8},
	number = {82},
	numpages = {4468},
	publisher = {The Open Journal},
	doi = {10.21105/joss.04468},
	url = {https://joss.theoj.org/papers/10.21105/joss.04468},
	issn = {2475-9066},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, mixed-variable optimization, surrogate modeling, RBFs},
}

% The ParMOO docs -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@techreport{chang2024parmoo,
	author = {Chang, Tyler H. and Wild, Stefan M. and Dickinson, Hyrum},
	title = {{ParMOO}: {P}ython library for parallel multiobjective simulation optimization},
	year = {2024},
	number = {Version 0.4.1},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://parmoo.readthedocs.io/en/latest},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, mixed-variable optimization, autograd, backpropagation, surrogate modeling, RBFs, high dimension},
}

% An update to DelaunaySparse where we use BQPD to solve the problem of projection onto the convex hull. We rigorously show that this is the only method that achieves a 100% success rate among open source quadratic programming / NNLS solvers for this size and shape of problem. Then we demonstrate robustness on synthetic test problems and real applications from computer security
@article{chang2024remark,
	author = {Chang, Tyler H. and Watson, Layne T. and Leyffer, Sven and Lux, Thomas C. H. and Almohri, Hussain M. J.},
	title = {Remark on {Algorithm} 1012: Computing projections with large data sets},
	year = {2024},
	month = {6},
	journal = {ACM Transactions on Mathematical Software},
	volume = {50},
	number = {2},
	articleno = {12},
	numpages = {8},
	publisher = {Association of Computing Machinery},
	doi = {10.1145/3656581},
	url = {https://dl.acm.org/doi/10.1145/3656581},
	issn = {0098-3500},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization, high dimension},
}

% The ParMOO IJOC article describing the design of the ParMOO software, motivation, and providing examples of how ParMOO can be used to solve common scientific problems more efficiently with low effort -- ParMOO is my open source numerical software package and library ParMOO, written in Python, which can be used for implementing custom solvers for multiobjective simulation optimization problems, while supporting mixed variables, non linear constraints, and diverse and custom surrogte models, and composite structures where some components of the problem are blackbox, but the rest are algebraic. In later releases, ParMOO also supports interactive visualization of results via Plotly + Dash, parallel and distributed function evaluations via libEnsemble, and automatic gradient calculations and just-in-time compilation via jax
@article{chang2025designing,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
	year = {2025},
	month = {3},
	journal = {INFORMS Journal on Computing},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2023.0250},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2023.0250},
	issn = {1091-9856},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, mixed-variable optimization, surrogate modeling, RBFs},
}

% This is the IJOC ParMOO repository DOI -- this is an archive of the software experiments for obtaining our test problems and reproducing our experimental results on those test problems with customized ParMOO solvers.
@misc{chang2025repository,
	author = {Chang, Tyler H. and Wild, Stefan M.},
	title = {Repository for ``Designing a Framework for Solving Multiobjective Simulation Optimization Problems''},
	year = {2025},
	month = {3},
	booktitle = {INFORMS Journal on Computing},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2023.0250.cd},
	url = {https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0250},
	issn = {1091-9856},
	git = {https://github.com/INFORMSJoC/2023.0250},
	note = {Last accessed: May 1, 2025},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, mixed-variable optimization, surrogate modeling, RBFs},
}

% Survey of common FPGA placement and routing algorithms/techniques, mentioning partition-based placement, analytic placement, and simulated annealing placement as various viable techniques ranging from least computationally expensive/least accurate to most expensive/most accurate. They also mention the different stages in a moder analytic placer (since analytical placement is the current state-of-the-art), which includes packing and netlist optimizations, global floorplanning and global placement (via quadratic programming), legalization (similar to integer/categorical binning), and detailed placement (typically via simulated annealing)
@inproceedings{chen2017fpga,
	author = {Chen, Shih-Chun and Chang, Yao-Wen},
	title = {{FPGA} placement and routing},
	year = {2017},
	month = {11},
	booktitle = {2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	pages = {914--921},
	organization = {IEEE},
	location = {Irvine, CA},
	doi = {10.1109/ICCAD.2017.8203878},
	url = {http://ieeexplore.ieee.org/document/8203878/},
	keywords = {optimization, quadratic programming, QP, convex optimization, mixed-variable optimization, constrained optimization, simulated annealing, SA},
}

% Publication of our work on multiobjective shape optimization of the RF-gun cavity for the Argonne wakefield accelerator using ParMOO with the POISSON/SUPERFISH simulation software
@inproceedings{chen2023integrated,
	author = {Chen, Gongxiaohui and Chang, Tyler H. and Power, John and Jing, Chungunag},
	title = {An Integrated Multi-Physics Optimization Framework for Particle Accelerator Design},
	year = {2023},
	booktitle = {Proc. 2023 Winter Simulation Conference (WSC 2023), Industrial Applications Track},
	numpages = {2},
	location = {Orlando, FL, USA},
	doi = {10.48550/arXiv.2311.09415},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% A really thorough review of most common scalarization functions used in multiobjective optimization with a focus on Bayesian optimization. Provides all scalarization functions, their equations, and a few adaptive schemes very concisely, then presents numerical results on DTLZ2 benchmarks
@inproceedings{chugh2020scalarizing,
	author = {Chugh, Tinkle},
	title = {Scalarizing functions in Bayesian multiobjective optimization},
	year = {2020},
	month = {7},
	booktitle = {2020 IEEE Congress on Evolutionary Computation (CEC)},
	pages = {1--8},
	organization = {IEEE},
	location = {Glasgow, United Kingdom},
	doi = {10.1109/cec48606.2020.9185706},
	url = {https://ieeexplore.ieee.org/document/9185706/},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, scalarization},
}

% An algorithm for multiobjective implicit filtering (MOIF). Not mentioned here, but the open source numerical software for MOIF on the author's GitHub is often cited via this paper
@article{cocchi2018implicit,
	author = {Cocchi, Guido and Liuzzi, Giampaolo and Papini, Alessandra and Sciandrone, Marco},
	title = {An implicit filtering algorithm for derivative-free multiobjective optimization with box constraints},
	year = {2018},
	month = {3},
	journal = {Computational Optimization and Applications},
	volume = {69},
	number = {2},
	pages = {267--296},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-017-9953-2},
	url = {http://link.springer.com/10.1007/s10589-017-9953-2},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% Building a multiobjective augmented Lagrangian -- basically, use a standard augmented Lagrangian penalty but apply it to all components of the objective. There is a proof that this will work. I don't use this method, but I use the same trick with the progressive barrier of Audet all the time and usually cite both papers
@article{cocchi2020augmented,
	author = {Cocchi, Guido and Lapucci, Matteo},
	title = {An augmented {Lagrangian} algorithm for multi-objective optimization},
	year = {2020},
	month = {9},
	journal = {Computational Optimization and Applications},
	volume = {77},
	number = {1},
	pages = {29--56},
	publisher = {Springer},
	doi = {10.1007/s10589-020-00204-z},
	url = {https://link.springer.com/10.1007/s10589-020-00204-z},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, convex optimization, constrained optimization},
}

% Andrew Conn's landmark paper on interpolation dataset geometry -- leads to the definition of sets being "well-poised" for interpolation, meaning that when the interpolation set's geometry meats some local geometric conditions (basically bounded away from singularity), then the resulting interpolant's error (and gradient / hessian errors) can be bounded and the resulting models can be used to perform gradient descent or SQP within a trust-region framework with guaranteed convergence
@article{conn2008geometry,
	author = {Conn, Andrew R and Scheinberg, Katya and Vicente, Lu{\'\i}s N},
	title = {Geometry of interpolation sets in derivative free optimization},
	year = {2008},
	month = {6},
	journal = {Mathematical programming},
	volume = {111},
	number = {1-2},
	pages = {141--172},
	publisher = {Springer},
	doi = {10.1007/s10107-006-0073-5},
	url = {http://link.springer.com/10.1007/s10107-006-0073-5},
	issn = {0025-5610},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Conn and Scheinberg book on DFO -- describes the geometry of good interpolation sets, linear and quadratic interpolants and how to use them for derivative-free gradient descent and SQP frameworks, bounds on interpolation and gradient errors of various models, and how to efficiently restore good geometry when the optimization algorithm samples points in a subspace
@book{conn2009introduction,
	author = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
	title = {Introduction to derivative-free optimization},
	year = {2009},
	month = {1},
	series = {MPS-SIAM Series on Optimization},
	publisher = {SIAM},
	address = {Philadelphia, PA, USA},
	doi = {10.1137/1.9780898718768},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898718768},
	isbn = {9780898716689},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% PyMOSO: an open source Python numerical software library for solving multiobjective simulation optimization problems with integer and discrete variables via direct search / pattern search like techniques
@article{cooper2020pymoso,
	author = {Cooper, Kyle and Hunter, Susan R.},
	title = {{PyMOSO}: {S}oftware for multi-objective simulation optimization with {R-PERLE} and {R-MinRLE}},
	year = {2020},
	month = {4},
	journal = {INFORMS Journal on Computing},
	volume = {32},
	number = {4},
	pages = {1101--1108},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2019.0902},
	url = {http://pubsonline.informs.org/doi/10.1287/ijoc.2019.0902},
	issn = {1091-9856},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, pattern search, mixed-variable optimization, stochastic optimization},
}

% RBFOpt an open source library for solving single-objective blackbox optimization
@article{costa2018rbfopt,
	author = {Costa, Alberto and Nannicini, Giacomo},
	title = {{RBFOpt}: an open-source library for black-box optimization with costly function evaluations},
	year = {2018},
	month = {12},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {4},
	pages = {597--629},
	publisher = {Springer},
	doi = {10.1007/s12532-018-0144-7},
	url = {http://link.springer.com/10.1007/s12532-018-0144-7},
	issn = {1867-2949},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% ParEGO latest code and update introducing algorithmic updates, improved software quality, and (I think) some parallel computing -- ParEGO is the first open source numerical multiobjective bayesian optimization software package and written in C++. It is basically EGO (the original Bayesian optimization software using expected improvment acquisition) plus augmented Lagrangian scalarization. Available here: github.com/CristinaCristescu/ParEGO_Eigen
@inproceedings{cristescu2015surrogatebased,
	author = {Cristescu, Cristina and Knowles, Joshua},
	title = {Surrogate-based multiobjective optimization: {ParEGO} update and test},
	year = {2015},
	booktitle = {Workshop on Computational Intelligence (UKCI)},
	volume = {770},
	url = {https://www.cs.bham.ac.uk/~jdk/UKCI-2015.pdf},
	git = {https://github.com/CristinaCristescu/ParEGO_Eigen},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, scalarization, Gaussian process},
}

% Direct multisearch (DMS) is one of Custodio's earlier multiobjective direct search algorithms, which I think is a precursor to MutiGLODS. The numerical MATLAB software can be obtained by contacting her lab, but I'm not sure if they still distribute it as part of BoostDFO
@article{custodio2011direct,
	author = {Cust\'odio, Ana Lu\'isa and Madeira, Jose F. A. and Vaz, A. Ismael F. and Vicente, Lu\'is N.},
	title = {Direct Multisearch for Multiobjective Optimization},
	year = {2011},
	month = {7},
	journal = {SIAM Journal on Optimization},
	volume = {21},
	number = {3},
	pages = {1109--1140},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/10079731x},
	url = {http://epubs.siam.org/doi/10.1137/10079731X},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search},
}

% The MultiGLODS numerical software package is written in MATLAB and used for solving multiobjective optimization problems via direct search / pattern search with a clever restart algorithm for selecting directions to explore in order obtain good coverage of the Pareto front. I believe this version also can use polynomial surrogates to pre-select good search directions and filter out unneeded blackbox function / simulation evaluations. It is now part of the BoostDFO MATLAB numerical software toolkit, obtainable from contacting Custodio
@article{custodio2018multiglods,
	author = {Cust{\'{o}}dio, Ana Lu\'isa and Madeira, Jose F. A.},
	title = {{MultiGLODS}: global and local multiobjective optimization using direct search},
	year = {2018},
	month = {10},
	journal = {Journal of Global Optimization},
	volume = {72},
	number = {2},
	pages = {323--345},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10898-018-0618-1},
	url = {http://link.springer.com/10.1007/s10898-018-0618-1},
	issn = {0925-5001},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, surrogate modeling},
}

% Description and proof of coverage for a quadratic scalarization scheme for scalarizing multiobjective optimization problems
@article{dandurand2016quadratic,
	author = {Dandurand, Brian and Wiecek, Margaret M.},
	title = {Quadratic scalarization for decomposed multiobjective optimization},
	year = {2016},
	month = {10},
	journal = {{OR} Spectrum},
	volume = {38},
	number = {4},
	pages = {1071--1096},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00291-016-0453-z},
	url = {http://link.springer.com/10.1007/s00291-016-0453-z},
	issn = {0171-6468},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% Dantzig's original (landmark) textbook on solving linear programming problems via the simplex method. This was obviously a landmark achievement in how to solve linear programming problems and more generally in the field of numerical optimization
@book{dantzig1998linear,
	author = {Dantzig, George B.},
	title = {Linear Programming and Extensions},
	year = {1998},
	series = {Princeton Landmarks in Mathematics and Physics},
	edition = {11},
	publisher = {Princeton University Press},
	address = {Princeton, NJ, USA},
    url = {https://d1wqtxts1xzle7.cloudfront.net/56278680/Libro_Linear_Programming_George_Dantzig-libre.pdf?1523307505=&response-content-disposition=inline%3B+filename%3DLinear_Programming_and_Extensions.pdf&Expires=1746491165&Signature=WQRD07CTKkhpfjxG1R6Kb2tSq0cRnDUia1ETKdgTQX2wbUxpA2p7ZGudVpOpbsKgUZzsKL-U3CddGBaVVSTr~TSLwPadmYe8xHRVZ4KqyB~ms5zyu08vntJ0V-pRNY0sws9H~ktLJTgoABlZMkoYDA23Dbrh07yQqukyaqHsDuoTEZRzng6AIqN7CXO1KW2M4J~rS-M1mmM3bdTSMAoWPozK7Suea-HJPd7QbCMq2hB0JY5mhhi6nUHa6zIQVmjTCcPPdnX9O4lYgYPQgOBiMlIJ5yhYolhlHKXMA~2-g3rbpe4kqJXIEqICSWPByh72uohGvRJkDgUX-CkBw7FZNA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% The normal boundary intersection (NBI) method was one of the first adaptive scalarization schemes for multiobjective optimization problems. It uses linear scalarization but does so adaptively using the angle of the intersecting vector at a target point to set the weights in order to adaptively fill in gaps on the Pareto front
@article{das1998normalboundary,
	author = {Das, Indraneel and Dennis, John E.},
	title = {Normal-boundary intersection: A new method for generating the {P}areto surface in nonlinear multicriteria optimization problems},
	year = {1998},
	month = {8},
	journal = {SIAM Journal on Optimization},
	volume = {8},
	number = {3},
	pages = {631--657},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/S1052623496307510},
	url = {http://epubs.siam.org/doi/10.1137/S1052623496307510},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% An early paper on using surrogate models to reduce the cost (in terms of true simulation / blackbox function evaluations) when using multiobjective evolutionary algorithm to solve computationally expensive blackbox and simulation optimization problems
@article{datta2016surrogateassisted,
	author = {Datta, Rituparna and Regis, Rommel G.},
	title = {A surrogate-assisted evolution strategy for constrained multi-objective optimization},
	year = {2016},
	month = {9},
	journal = {Expert Systems with Applications},
	volume = {57},
	pages = {270--284},
	publisher = {Elsevier BV},
	doi = {10.1016/j.eswa.2016.03.044},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417416301452},
	issn = {0957-4174},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, evolutionary algorithms, EA},
}

% A technique for differentiating expected hypervolume improvement EHVI (and its monte carlo variant qEHVI), which can be used as the acquisition function for solving multiobjective blackbox optimization problems with BoTorch
@inproceedings{daulton2020differentiable,
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	title = {Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective {B}ayesian Optimization},
	year = {2020},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {9851--9864},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2020/file/6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, autograd, backpropagation, scalarization},
}

% The first paper on performing parallel Bayesian optimization using the expected hypervolume improvement acquisition function in BoTorch
@inproceedings{daulton2021parallel,
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	title = {Parallel {B}ayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement},
	year = {2021},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {34},
	pages = {2187--2200},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, stochastic optimization, scalarization, Gaussian process},
}

% Interesting paper on why it is generally OK to use local optimizers when solving non convex optimization problems in high-dimensional spaces. In general, in high-dimensional spaces, almost every critical point will be a saddle point with high probability. Therefore, first-order methods tend to perform very well on these problems as they converge quickly but are not attracted to saddle points and therefore tend to find the global optimum in the limit. The analysis of the probability that a critical point will be a saddle point is based on a spectral analysis of the hessian at each critical point other than the global minimum/maximum -- all of the eigenvalues must be positive or negative for the critical point to be a local minima / maxima, and the probability of this occurring decays as the number of eigenvalues grows with the dimension of the Hessian. The authors also experimentally validate these claims by extracting critical points from the loss landscapes of single layer MLPs trained on down-sampled versions of MNIST and CIFAR-10.
@inproceedings{dauphin2014identifying,
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	year = {2014},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {27},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf},
	keywords = {optimization, convex optimization, stochastic optimization, global optimization, high dimension},
}

% The original NSGA-II paper: a multiobjective evolutionary algorithm (MOEA) that scales well and performs extremely well in practice. The main contribution is a fast method for performing nondominated sorting so that the authors can ensure all efficient points persist in every generation. This method is generally considered to be the baseline in multiobjective optimization. While the algorithm is a simple heuristic that is extremely wasteful in terms of the number of true blackbox function / simulation evaluations, it performs extremely well in practice by the hypervolume indicator. I have found that it is very difficult to obtain better performance than NSGA-II on both benchmark and real-world problems in the limit, unless you have some "secret sauce" to exploit for your particular problem
@article{deb2002fast,
	author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwel, Sameer and Meyarivan, T.},
	title = {A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
	year = {2002},
	month = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {6},
	number = {2},
	pages = {182--197},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/4235.996017},
	url = {http://ieeexplore.ieee.org/document/996017},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization, scalarization, evolutionary algorithms, EA},
}

% The DTLZ test problems are the standard test problems used in all multiobjective evolutionary optimization papers. They are algebraic test problems that can scale to as many objectives and variables as one desires. Each problem also has a pathological property that makes it extremely difficult or degenerate for multiobjective optimization algorithms. This maeks the suite as a whole extremely convenient for testing, scaling, and evaluating results. However, several of the problems are so difficult that no solvers can reliably solve them. Additionally, all the problems have the property that the last "n" variables are essentially unused, with their optimum being 0.5 and not changing as we move across the Pareto front, which could be unrealistic for certain problems
@inproceedings{deb2002scalable,
	author = {Deb, Kalyanmoy and Thiele, Lothar and Laumanns, Marco and Zitzler, Eckart},
	title = {Scalable multi-objective optimization test problems},
	year = {2002},
	booktitle = {Proc. 2002 IEEE Congress on Evolutionary Computation (CEC '02)},
	volume = {1},
	pages = {825--830},
	organization = {IEEE},
	location = {Honolulu, HI, USA},
	doi = {10.1109/CEC.2002.1007032},
	url = {http://ieeexplore.ieee.org/document/1007032},
	keywords = {optimization, multiobjective optimization, global optimization, benchmarking, high dimension, evolutionary algorithms, EA},
}

% The orgiinal NSGA-III paper part 1: a multiobjective evolutionary algorithm similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many objectives by having the user provide a collection of well-spaced reference points and optimizing toward those
@article{deb2013evolutionary,
	author = {Deb, Kalyanmoy and Jain, Himanshu},
	title = {An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part {I}: solving problems with box constraints},
	year = {2013},
	month = {8},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {18},
	number = {4},
	pages = {577--601},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2013.2281535},
	url = {http://ieeexplore.ieee.org/document/6600851},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization, high dimension, evolutionary algorithms, EA},
}

% A study of various surrogate models for optimizing with WBCSim design tool which simulates and analyzes the behavior of wood-based composites: in summary, Shepard's method (locally linear) performed best in this analysis, and therefore was chosen as the main surrogate model in Deshpande's 2016 algorithm (above). It is hypothesized that Shepard's method's ability to extrapolate (typically an undesirable feature in interpolation) makes it good for surrogate modeling for optimization, which requires extrapolation based on local trends
@article{deshpande2011data,
	author = {Deshpande, Shubhangi and Watson, Layne T. and Shu, Jiang and Ramakrishnan, Naren},
	title = {Data driven surrogate-based optimization in the problem solving environment {WBCSim}},
	year = {2011},
	month = {7},
	journal = {Engineering with Computers},
	volume = {27},
	number = {3},
	pages = {211--223},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00366-010-0192-8},
	url = {http://link.springer.com/10.1007/s00366-010-0192-8},
	issn = {0177-0667},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% An algorithm for solving blackbox multiobjective optimization problems via trust region descent, using locally linear (shepard's method) surrogate models, and a multiobjective variant of DIRECT. Also, the authors propose a novel adaptive weighting scheme within the trust regions. The motivating application is an aircraft design optimization problem.
@article{deshpande2016multiobjective,
	author = {Deshpande, Shubhangi and Watson, Layne T. and Canfield, Robert A.},
	title = {Multiobjective optimization using an adaptive weighting scheme},
	year = {2016},
	month = {1},
	journal = {Optimization Methods and Software},
	volume = {31},
	number = {1},
	pages = {110--133},
	publisher = {Informa UK Limited},
	doi = {10.1080/10556788.2015.1048861},
	url = {http://www.tandfonline.com/doi/full/10.1080/10556788.2015.1048861},
	issn = {1055-6788},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization, surrogate modeling},
}

% CVXPY is an open source Python optimization and modeling language for solving convex optimization problems in a disciplined way (meaning that we ensure convexity through hard rules on the problem definition). From the lab of Stephen Boyd
@article{diamond2016cvxpy,
	author = {Diamond, Steven and Boyd, Stephen},
	title = {{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization},
	year = {2016},
	journal = {Journal of Machine Learning Research},
	volume = {17},
	number = {83},
	pages = {1--5},
	url = {http://jmlr.org/papers/v17/15-408.html},
	keywords = {optimization, linear programming, quadratic programming, LP, QP, convex optimization, constrained optimization},
}

% ECOS is an open source numerical software for solving second-order cone optimization problems, from the lab of Stephen Boyd. In my experience, this software is the best tool from Boyd's lab and the most robust to degeneracy
@inproceedings{domahidi2013ecos,
	author = {Domahidi, Alexander and Chu, Eric and Boyd, Stephen},
	title = {{ECOS}: {A}n {SOCP} solver for embedded systems},
	year = {2013},
	month = {7},
	booktitle = {European Control Conference (ECC)},
	pages = {3071--3076},
	organization = {IEEE},
	location = {Z{\"u}rich, Switzerland},
	doi = {10.23919/ECC.2013.6669541},
	url = {https://ieeexplore.ieee.org/document/6669541},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% NAS-Bench-201 introduces a cell-based neural architecture search space representation (i.e., problem embedding) that is used in many neural architecture search softwares circa ~2022. This includes the benchmark problems in JAHS-Bench-201
@inproceedings{dong2020nasbench201,
	author = {Dong, Xuanyi and Yang, Yi},
	title = {{NAS-Bench-201}: Extending the Scope of Reproducible Neural Architecture Search},
	year = {2020},
	booktitle = {8th International Conference on Learning Representations (ICLR 2020)},
	url = {https://openreview.net/forum?id=HJxyZkBKDr},
	keywords = {optimization, mixed-variable optimization, autotuning, hyperparameter optimization, benchmarking},
}

% The original paper for AdaGrad (adaptive subgradient method) which replaced the subgradient method with an adaptive estimate for the gradient, where each component of the gradient is rescaled by an adaptive estimate for the standard deviation in that direction based on previous iterates. This adaptive estimate for standard deviation in each axis-aligned direction serves as a diagonal approximation to the Hessian matrix, giving second-order like properties to the method and greatly improving the practical convergence. AdaGrad was very popular and considered the state-of-the-art optimization algorithm for training neural networks upon its initial release, but was quickly replaced by Adam, which added a Nesterov momentum esque smoothing to this adaptive gradient estimation in order to further improve convergence rates on nonsmooth, highly stochastic, and ill-conditioned problems
@article{duchi2011adaptive,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	year = {2011},
	journal = {Journal of Machine Learning Research},
	volume = {12},
	number = {61},
	pages = {2121--2159},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	keywords = {optimization, stochastic optimization, convex optimization},
}

% An example of using genetic algorithms for autotuning HPC libraries (such as BLAS and LAPACK)
@inproceedings{dunlop2008use,
	author = {Dunlop, Dominic and Varrette, Sebastien and Bouvry, Pascal},
	title = {On the use of a genetic algorithm in high performance computing benchmark tuning},
	year = {2008},
	booktitle = {Proceedings of the 2008 International Symposium on Performance Evaluation of Computer and Telecommunication Systems},
	pages = {105--113},
	organization = {IEEE},
	location = {Edinburgh, UK},
	keywords = {optimization, autotuning, evolutionary algorithms, EA},
}

% The JuMP modeling language in Julia -- a modeling language for modeling and solving linear and nonlinear programming (optimization) problems in Julia. The implementation is an open source numerical software
@article{dunning2017jump,
	author = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
	title = {{JuMP}: A Modeling Language for Mathematical Optimization},
	year = {2017},
	month = {1},
	journal = {SIAM Review},
	volume = {59},
	number = {2},
	pages = {295--320},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/15M1020575},
	url = {https://epubs.siam.org/doi/10.1137/15M1020575},
	issn = {0036-1445},
	keywords = {optimization, linear programming, LP, quadratic programming, QP, convex optimization, constrained optimization},
}

% jMetal is an open source numerical software library implementing multiobjective optimization solvers in Java. Last I checked, most of the solvers were heuristic methods such as evolutionary algorithms and/or simulated annealing. This is widely used in some fields of engineering
@article{durillo2011jmetal,
	author = {Durillo, Juan J. and Nebro, Antonio J.},
	title = {{jMetal}: A {J}ava framework for multi-objective optimization},
	year = {2011},
	month = {10},
	journal = {Advances in Engineering Software},
	volume = {42},
	number = {10},
	pages = {760--771},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2011.05.014},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0965997811001219},
	issn = {0965-9978},
	keywords = {optimization, multiobjective optimization, evolutionary algorithms, EA, simulated annealing, SA},
}

% A careful analysis of Johnson-Lindenstrauss transforms, and how we can sample dimensions from hashing matrices to ensure that the resulting random subspace method converges into the true optimum in the limit
@article{dzahini2025class,
	author = {Dzahini, Kwassi Joseph and Wild, Stefan M.},
	title = {A Class of Sparse {Johnson–Lindenstrauss} Transforms and Analysis of their Extreme Singular Values},
	year = {2025},
	month = {3},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {46},
	number = {1},
	pages = {416--438},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/23M1605661},
	url = {https://doi.org/10.1137/23M1605661},
	issn = {0895-4798},
	keywords = {optimization, high dimension},
}

% SimOpt is a really nice suite of real-world and real-world inspired simulation optimization problems. They are mostly stochastic. This is the standard benchmark suite in the operations research and business operations community. In order to publish work in those venues, one needs to perform well on these problems. The paper details how SimOpt has been refactored to provide new problems, features, reproducability, and better interfaces in the latest patch. The open source Python software is available at github.com/simopt-admin/simopt
@article{eckman2023simopt,
	author = {Eckman, David J. and Henderson, Shane G. and Shashaani, Sara},
	title = {{SimOpt}: A Testbed for Simulation-Optimization Experiments},
	year = {2023},
	month = {3},
	journal = {INFORMS Journal on Computing},
	volume = {35},
	number = {2},
	pages = {495--508},
	publisher = {INFORMS},
	address = {Linthicum, MD, USA},
	doi = {10.1287/ijoc.2023.1273},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2023.1273},
	issn = {1526-5528},
	keywords = {optimization, blackbox optimization, simulation optimization, stochastic optimization, benchmarking},
}

% Joint work with the DeepHyper team to bring multiobjective optimization to DeepHyper. Focusing on the challenges of keeping the optimizer focused on interesting regions of the Pareto front and how to rescale objectives so that traditional scalarization approaches can work
@techreport{egele2023parallel,
	author = {Egele, Romain and Chang, Tyler H. and Sun, Yixuan and Vishwanath, Venkatram and Balaprakash, Prasanna},
	title = {Parallel multi-objective hyperparameter optimization with uniform normalization and bounded objectives},
	year = {2023},
	institution = {arXiv cs.LG},
	doi = {10.48550/arXiv.2309.14936},
	keywords = {optimization, decision trees, multiobjective optimization, Bayesian optimization, global optimization, surrogate modeling, autotuning, hyperparameter optimization, scalarization},
}

% HPOBench is another suite of automl (hyperparameter tuning) benchmark problems from the automl research group. The problems can be configured to be either single or multi-fidelity. They can either be run tabular (meaning the raw data is accessed in a table and only configurations in the dataset can be evaluated) or with a regression model (XGBoost). The open source software is available for download in Python at: github.com/automl/HPOBench
@inproceedings{eggensperger2021hpobench,
	author = {Eggensperger, Katharina and M{\"u}ller, Philipp and Mallik, Neeratyoy and Feurer, Matthias and Sass, Rene and Klein, Aaron and Awad, Noor and Lindauer, Marius and Hutter, Frank},
	title = {{HPOB}ench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for {HPO}},
	year = {2021},
	booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
	url = {https://openreview.net/forum?id=1k4rJYEwda-},
	keywords = {optimization, autotuning, hyperparameter optimization, benchmarking},
}

% A classical textbook on the fundamentals of multiobjective optimization theory. Topics include: basic definitions in multiobjective optimization, partial orderings and cones and basic theories, linear scalarization and its theory and drawbacks, other scalarization methods and their theory and drawbacks, standard algorithms for common types of multiobjective optimization problems, and sample applications
@book{ehrgott2005multicriteria,
	author = {Ehrgott, Matthias},
	title = {Multicriteria Optimization},
	year = {2005},
	series = {Lecture Notes in Economics and Mathematical Systems Series},
	edition = {2},
	publisher = {Springer Verlag},
	address = {Heidelberg, Germany},
	doi = {10.1007/3-540-27659-9},
	url = {http://link.springer.com/10.1007/3-540-27659-9},
	isbn = {3540213988},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% The Pascoletti-Serafini scalarization and its variations, this method involves drawing a line through the target to reach various points on the Pareto front. It is effective with nonconvex Pareto fronts, but it is not adaptive and not commonly used in modern algorithms
@article{eichfelder2009scalarizations,
	author = {Eichfelder, Gabriele},
	title = {Scalarizations for adaptively solving multi-objective optimization problems},
	year = {2009},
	month = {11},
	journal = {Computational Optimization and Applications},
	volume = {44},
	number = {2},
	pages = {249--273},
	publisher = {Springer},
	doi = {10.1007/s10589-007-9155-4},
	url = {http://link.springer.com/10.1007/s10589-007-9155-4},
	issn = {0926-6003},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% A nice survey paper on neural architecture search covering common search (i.e., architecture) space representations, search (i.e., optimization) strategies, and how to evaluate the performance of NAS methods
@article{elsken2019neural,
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	title = {Neural architecture search: A survey},
	year = {2019},
	journal = {The Journal of Machine Learning Research},
	volume = {20},
	number = {1},
	pages = {1997--2017},
	publisher = {JMLR.org},
	keywords = {optimization, autotuning, hyperparameter optimization, decision trees, Bayesian optimization, evolutionary algorithms, EA, Gaussian process},
}

% TURBO -- This is an open source numerical software for solving high-dimensional optimization problems via Bayesian optimization using BoTorch. Since Bayesian optimization performs poorly in high dimensions, they have resorted to applying a rudimentary trust region framework on top of their Bayesian optimization algorithm. By squeesing the trust region inward (standard practice in derivative-free optimization) they are able to force the Bayesian optimization algorithm to converge in a reasonable number of true blackbox function evaluations
@article{eriksson2019scalable,
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
	title = {Scalable global optimization via local bayesian optimization},
	year = {2019},
	journal = {Advances in Neural Information Processing Systems},
	volume = {32},
	pages = {1--12},
	publisher = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, Bayesian optimization, global optimization, surrogate modeling, high dimension, autograd, backpropagation, Gaussian process},
}

% Performing accuracy and latency aware neural architecture search via multiobjective optimization using BoTorch. While not part of the publication, the software is available open source on the BoTorch website
@inproceedings{eriksson2021latencyaware,
	author = {Eriksson, David and Chuang, Pierce I-Jen and Daulton, Samuel and Xia, Peng and Shrivastava, Akshat and Babu, Arun and Zhao, Shicong and Aly, Ahmed A and Venkatesh, Ganesh and Balandat, Maximilian},
	title = {Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization},
	year = {2021},
	booktitle = {8th ICML Workshop on Automated Machine Learning (AutoML)},
	url = {https://openreview.net/forum?id=0ciyfd4SvbI},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, global optimization, autotuning, hyperparameter optimization, Gaussian process},
}

% Publication and whitepaper on Pathmind, an RL-based solver for simulation optimization problems. They also offer multiobjective support but only by using a priori scalarization provided by the user (so not real multiobjective support). This tool is not open source, it is a service provided by a YC startup of the same name. Therefore, it could be considered industry software
@inproceedings{farhan2020reinforcement,
	author = {Farhan, Mohammed and G{\"o}hre, Brett},
	title = {Reinforcement Learning in {AnyLogic} Simulation Models: A Guiding Example using {Pathmind}},
	year = {2020},
	month = {12},
	booktitle = {Proc. 2020 Winter Simulation Conference (WSC 2020)},
	pages = {3212--3223},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC48552.2020.9383916},
	url = {https://ieeexplore.ieee.org/document/9383916},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, reinforcement learning, RL, scalarization},
}

% A biobjective ranking and selection algorithm from Hunter's NSF Career award
@article{feldman2018score,
	author = {Feldman, Guy and Hunter, Susan R.},
	title = {{SCORE} Allocations for Bi-objective Ranking and Selection},
	year = {2018},
	month = {1},
	journal = {ACM Transactions on Modeling Computer and Simulation},
	volume = {28},
	number = {1},
	articleno = {7},
	numpages = {28},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3158666},
	url = {https://dl.acm.org/doi/10.1145/3158666},
	issn = {1049-3301},
	keywords = {optimization, multiobjective optimization, stochastic optimization},
}

% A Bayesian optimization algorithm for solving constrained optimization problems that are both single and multiobjective -- the authors propose expected hypervolume improvement (EHVI) which merges expected improvement acquisition from Bayesian optimization with the hypervolume indicator for multiobjective optimization
@article{feliot2016bayesian,
	author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
	title = {A {B}ayesian approach to constrained single- and multi-objective optimization},
	year = {2016},
	month = {1},
	journal = {Journal of Global Optimization},
	volume = {67},
	number = {1-2},
	pages = {97--133},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10898-016-0427-3},
	url = {http://link.springer.com/10.1007/s10898-016-0427-3},
	issn = {0925-5001},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, constrained optimization, surrogate modeling, scalarization},
}

% The Fiduccia-Matheyses graph partitioning algorithm, which calculates the minimum cut in a hypergraph via the heuristic of generating a random initial cut then moving nodes across the cut (greedily) and remembering the best observed cut until all nodes have been moved. The magic of this algorithm is the linear complexity due to a heap-like data structure that produces the next node to move in constant time in each iteration. However, this trick only works for integer-valued cost functions (such as min-cut)
@inproceedings{fiduccia1982lineartime,
	author = {Fiduccia, C. M. and Mattheyses, R. M.},
	title = {A linear-time heuristic for improving network partitions},
	year = {1982},
	booktitle = {Proceedings of the 19th Design Automation Conference},
	series = {DAC '82},
	numpages = {7},
	organization = {IEEE Press},
	location = {Las Vegas, NV, USA},
	doi = {10.1109/dac.1982.1585498},
	url = {http://ieeexplore.ieee.org/document/1585498/},
	isbn = {0897910206},
	keywords = {optimization, mixed-variable optimization, partitioning},
}

% Fletcher's paper on how to handle degeneracy in active set methods for solving quadratic programming problems. This technique would become the basis for the BQPD software package, which is a recently open source (post mortem) numerical software for solving (degenerate) quadratic programs via active set methods
@article{fletcher1993resolving,
	author = {Fletcher, Roger},
	title = {Resolving degeneracy in quadratic programming},
	year = {1993},
	month = {9},
	journal = {Annals of Operations Research},
	volume = {46},
	number = {2},
	pages = {307--334},
	publisher = {Springer},
	doi = {10.1007/BF02023102},
	url = {http://link.springer.com/10.1007/BF02023102},
	issn = {0254-5330},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% Fletcher's paper on an efficient Hessian update method which can be used in an active set method for solving quadratic programming optimization problems
@article{fletcher2000stable,
	author = {Fletcher, Roger},
	title = {Stable reduced Hessian updates for indefinite quadratic programming},
	year = {2000},
	month = {4},
	journal = {Mathematical programming},
	volume = {87},
	number = {2},
	pages = {251--264},
	publisher = {Springer},
	doi = {10.1007/s101070050113},
	url = {http://link.springer.com/10.1007/s101070050113},
	issn = {0025-5610},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% The DEAP framework is a Python framework for easily implementing and deploying parallel and distributed evolutionary algorithms. Fairly high quality open source software. This is widely used by optimization practitioners, e.g., engineers and scientists that read an evolutionary algorithm paper and want to try it out on their problem
@article{fortin2012deap,
	author = {Fortin, F\'elix-Antoine and {De Rainville}, Fran\c{c}ois-Michel and Gardner, Marc-Andr\'e and Parizeau, Marc and Gagn\'e ", Christian},
	title = {{DEAP}: Evolutionary Algorithms Made Easy},
	year = {2012},
	journal = {Journal of Machine Learning Research},
	volume = {13},
	number = {1},
	pages = {2171--2175},
	url = {https://www.jmlr.org/papers/v13/fortin12a.html},
	keywords = {optimization, blackbox optimization, simulation optimization, evolutionary algorithms, EA},
}

% PyCUTEst is a very nice, easily installed open source Python wrapper for the CUTEst test suite for benchmarking blackbox optimization problems. The CUTEst software is the standard suite of constrained and unconstrained test problems for derivative-free / blackbox / simulation optimization problems. CUTEst is available open source and written in Fortran, but now offer C, Python, Matlab, and Julia interfaces (plus the default command line interface). Available for download at: github.com/jfowkes/pycutest
@article{fowkes2022pycutest,
	author = {Fowkes, Jaroslav and Roberts, Lindon and Bűrmen, Árpád},
	title = {PyCUTEst: an open source Python package of optimization test problems},
	year = {2022},
	month = {10},
	journal = {Journal of Open Source Software},
	volume = {7},
	number = {78},
	numpages = {4377},
	publisher = {The Open Journal},
	doi = {10.21105/joss.04377},
	url = {https://joss.theoj.org/papers/10.21105/joss.04377},
	issn = {2475-9066},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, benchmarking},
}

% A tutorial by Peter Frazier on Bayesian optimization -- not much that isn't in the textbooks, except a reference where someone acknowledges the perspective that traditional Bayesian optimization (in pursuit of true global convergence with no localization strategy) doesn't scale well past 20 dimensions. In my experience, even this would be optimistic. I would say that it usually doesn't scale well past 5-10 dimensions depending on the computational budget
@article{frazier2018tutorial,
	author = {Frazier, Peter I.},
	title = {A Tutorial on {B}ayesian Optimization},
	year = {2018},
	journal = {arXiv preprint arXiv:1807.02811},
	url = {http://arxiv.org/abs/1807.02811},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, high-dimension, surrogate modeling, Gaussian process},
}

% A community reviewed textbook on Bayesian optimization theory and implementation. Very thorough description of Gaussian process and Bayesian optimization fundamentals and theory, common techniques and acquisition functions, and implementation details, drawbacks, and real-world challenges
@book{garnett2023bayesian,
	author = {Garnett, Roman},
	title = {Bayesian Optimization},
	year = {2023},
	publisher = {Cambridge University Press},
	url = {https://bayesoptbook.com},
	isbn = {978-1108425780},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, Gaussian process},
}

% Commonly cited lecture notes on using the Levenberg-Marquardt algorithm to solve least-squares curve-fitting (optimization) problems via a Gauss-Newton esque method
@techreport{gavin2019levenbergmarquardt,
	author = {Gavin, HP.},
	title = {The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
	year = {2019},
	institution = {Duke University, Department of Civil and Environmental Engineering},
    url = {https://people.duke.edu/~hpgavin/lm.pdf},
	keywords = {optimization, convex optimization},
}

% Online paper with interactive visualizations explaining what Nesterov's momentum is and how it works intuitively by smoothing out optimization sample paths and preventing oscillations in the optimizer that occur do to poor problem conditioning. Then, they show how the problem conditioning appears as an often ignored constant in the convergence rate of gradient descent. All this is to show intuitively and mathematically that gradient descent with Nesterov's momentum will convergence faster in practice for ill-conditioned problems
@article{goh2017why,
	author = {Goh, Gabriel},
	title = {Why Momentum Really Works},
	year = {2017},
	month = {4},
	journal = {Distill},
	volume = {2},
	number = {4},
	publisher = {Distill Working Group},
	doi = {10.23915/distill.00006},
	url = {http://distill.pub/2017/momentum},
	issn = {2476-0757},
	keywords = {optimization, stochastic optimization, convex optimization},
}

% Google's OSS Vizier service is a (now open source) blackbox / derivative-free optimization numerical software package and service. As far as I can tell, the package is primarily used for solving system optimization and A/B testing type problems
@inproceedings{golovin2017google,
	author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D.},
	title = {{Google Vizier}: A Service for Black-Box Optimization},
	year = {2017},
	month = {8},
	booktitle = {Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17)},
	pages = {1487--1495},
	organization = {ACM},
	location = {Halifax, NS, Canada},
	doi = {10.1145/3097983.3098043},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098043},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, Bayesian optimization, Gaussian process},
}

% The CUTEst software is the standard suite of constrained and unconstrained test problems for derivative-free / blackbox / simulation optimization problems. CUTEst is available open source and written in Fortran, but now offer C, Python, Matlab, and Julia interfaces (plus the default command line interface). Available for download at: github.com/ralna/CUTEst
@article{gould2015cutest,
	author = {Gould, Nicholas I. M. and Orban, Dominique and Toint, Philippe L.},
	title = {{CUTEst}: a Constrained and Unconstrained Testing Environment with safe threads for mathematical optimization},
	year = {2015},
	month = {4},
	journal = {Computational Optimization and Applications},
	volume = {60},
	number = {3},
	pages = {545--557},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-014-9687-3},
	url = {https://doi.org/10.1007/s10589-014-9687-3},
	issn = {0926-6003},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization, benchmarking},
}

% The OpenMDAO open source numerical software library for modeling and solving multidisciplinary engineering design optimization problems. Combines surrogate modeling, gradient based optimization, parallel computing frameworks, and derivative-free optimization techniques in one package so in order to solve large mixed-variable blackbox optimization problems. Developed by NASA Glenn
@article{gray2019openmdao,
	author = {Gray, Justin S. and Hwang, John T. and Martins, Joaquim R.R.A. and Moore, Kenneth T. and Naylor, Bret A.},
	title = {{OpenMDAO}: An open-source framework for multidisciplinary design, analysis, and optimization},
	year = {2019},
	month = {4},
	journal = {Structural and Multidisciplinary Optimization},
	volume = {59},
	number = {4},
	pages = {1075--1104},
	publisher = {Springer},
	doi = {10.1007/s00158-019-02211-z},
	url = {http://link.springer.com/10.1007/s00158-019-02211-z},
	issn = {1615-147X},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, autotuning, hyperparameter optimization, surrogate modeling},
}

% Platypus: an open source numerical software package for performing multiobjective optimization in Python and comparing results
@techreport{hadka2015platypus,
	author = {Hadka, David},
	title = {Platypus -- multiobjective optimization in {P}ython},
	year = {2015},
	number = {Version 1.0.4},
	institution = {GitHub},
	url = {https://platypus.readthedocs.io/en/latest},
	keywords = {optimization, multiobjective optimization},
}

% Hanson's Fortran numerical software for solving equality constrained nonnegative least-squares (NNLS) problems via an iterative weighted least squares (WNNLS) solver. This is the default constrained least-squares optimization problem solver in the Fortran library SLATEC from Sandia
@article{hanson1982algorithm,
	author = {Hanson, Richard J. and Haskell, Karen H.},
	title = {Algorithm 587: Two Algorithms for the Linearly Constrained Least Squares Problem},
	year = {1982},
	month = {9},
	journal = {ACM Trans. Math. Softw.},
	volume = {8},
	number = {3},
	pages = {323--333},
	publisher = {ACM},
	address = {New York, NY, USA},
	doi = {10.1145/356004.356010},
	url = {https://dl.acm.org/doi/10.1145/356004.356010},
	issn = {0098-3500},
	keywords = {optimization, constrained optimization, quadratic programming, QP, convex optimization},
}

% The official textbook on the Pyomo modeling language: an open source optimization modeling language and scientific software developed at Sandia by Bill Hart et al. Pyomo is a standard for solving large-scale mathematical programming (linear and nonlinear optimization) problems in Python
@book{hart2017pyomo,
	author = {Hart, William E. and Laird, Carl D. and Watson, Jean-Paul and Woodruff, David L. and Hackebeil, Gabriel A. and Nicholson, Bethany L. and Siirola, John D.},
	title = {Pyomo -- optimization modeling in {P}ython},
	year = {2017},
	booktitle = {Springer Optimization and Its Applications},
	series = {Springer Optimization and Its Applications},
	edition = {2},
	publisher = {Springer Cham},
	address = {Cham, Switzerland},
	doi = {10.1007/978-3-319-58821-6},
	url = {http://link.springer.com/10.1007/978-3-319-58821-6},
	isbn = {9783319588193},
	issn = {1931-6828},
	keywords = {optimization, linear programming, quadratic programming, LP, QP, constrained optimization, convex optimization, mixed-variable optimization},
}

% Chimera: a scientific software package for steering self-driving labs via multiobjective optimization -- the software is similar to what we did with ParMOO + MDML in the MERF at Argonne. They focus on applications in robot calibration and molecular system design. The multiobjective component helps them to explore the solution space with experimentation
@article{hase2018chimera,
	author = {H{\"a}se, Florian and Roch, Lo{\"\i}c M and Aspuru-Guzik, Al{\'a}n},
	title = {Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories},
	year = {2018},
	journal = {Chemical science},
	volume = {9},
	number = {39},
	pages = {7642--7655},
	publisher = {Royal Society of Chemistry},
	doi = {10.1039/C8SC02239A},
	url = {https://xlink.rsc.org/?DOI=C8SC02239A},
	issn = {2041-6520},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, mixed-variable optimization},
}

% A survey paper on multiobjective reinforcement learning -- RL is basically optimization with the addition of a dynamically changing state variable, so this is sort of relevant to multiobjecte optimization research
@article{hayes2022practical,
	author = {Hayes, Conor F and R{\u{a}}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M and Dazeley, Richard and Heintz, Fredrik and others, },
	title = {A practical guide to multi-objective reinforcement learning and planning},
	year = {2022},
	month = {4},
	journal = {Autonomous Agents and Multi-Agent Systems},
	volume = {36},
	number = {1},
	pages = {1--59},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10458-022-09552-y},
	url = {https://link.springer.com/10.1007/s10458-022-09552-y},
	issn = {1387-2532},
	keywords = {optimization, reinforcement learning, RL, multiobjective optimization},
}

% VTDIRECT95 reference: a high-performance parallel Fortran implementation of the famous single-objective blackbox (direct search) optimization algorithm DIRECT. The numerical software is now open source (maintained by me) on Dr. Watson's GitHub page.
@article{he2009algorithm,
	author = {He, Jian and Watson, Layne T. and Sosonkina, Masha},
	title = {Algorithm 897: {VTDIRECT95}: Serial and Parallel Codes for the Global Optimization Algorithm {DIRECT}},
	year = {2009},
	month = {7},
	journal = {ACM Transactions on Mathematical Software},
	volume = {36},
	number = {3},
	numpages = {17},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1527286.1527291},
	url = {https://dl.acm.org/doi/10.1145/1527286.1527291},
	issn = {0098-3500},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% Studying the parallel performance of VTDIRECT95 at a massive scale: in summary VTDIRECT95 scales very well after the initial "warmup" period since there are not many boxes in the first couple iterations
@article{he2009performance,
	author = {He, Jian and Verstak, Alex and Watson, Layne T. and Sosonkina, Masha},
	title = {Performance modeling and analysis of a massively parallel {DIRECT} -- part 1},
	year = {2009},
	month = {2},
	journal = {The International Journal of High Performance Computing Applications},
	volume = {23},
	number = {1},
	pages = {14--28},
	publisher = {SAGE Publications},
	doi = {10.1177/1094342008098463},
	url = {https://journals.sagepub.com/doi/10.1177/1094342008098463},
	issn = {1094-3420},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% Techniques for optimizing molecule properties via a latent-space embedding that comes from a molecule autoencoder, from IBM Research
@article{hoffman2022optimizing,
	author = {Hoffman, Samuel C. and Chenthamarakshan, Vijil and Wadhawan, Kahini and Chen, Pin-Yu and Das, Payel},
	title = {Optimizing molecules using efficient queries from property evaluations},
	year = {2022},
	month = {12},
	journal = {Nature Machine Intelligence},
	volume = {4},
	number = {1},
	pages = {21--31},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s42256-021-00422-y},
	url = {https://www.nature.com/articles/s42256-021-00422-y},
	issn = {2522-5839},
	keywords = {optimization, mixed-variable optimization, blackbox optimization, simulation optimization, Bayesian optimization},
}

% The official publication for HiGHS numerical optimization open source software package and solver for linear programming problems. The first successful effort to parallelize a simplex method based solver, specifically, they have successfully parallelized the dual revised simplex approach. HiGHS is now the default solver in most scipy.optimize. HiGHS is written in C++ with Fortran, C, and Python interfaces -- support CPU and GPU parallelism, the authors don't get perfect scaling by any means but this is the first successful effort to parallelize linear programming and still an achievement
@article{huangfu2018parallelizing,
	author = {Huangfu, Qi and Hall, JA Julian},
	title = {Parallelizing the dual revised simplex method},
	year = {2018},
	month = {3},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {1},
	pages = {119--142},
	publisher = {Springer},
	doi = {10.1007/s12532-017-0130-5},
	url = {http://link.springer.com/10.1007/s12532-017-0130-5},
	issn = {1867-2949},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% A thorough survey on techniques and algorithms for solving multiobjective simulation optimization problems, including algorithms and techniques for handling small finite design spaces, discrete integer design spaces, and continuous design spaces. Covers theory, algorithms, and popular heuristics for all.
@article{hunter2019introduction,
	author = {Hunter, Susan R. and Applegate, Eric A. and Arora, Viplove and Chong, Bryan},
	title = {An introduction to multiobjective simulation optimization},
	year = {2019},
	month = {1},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	volume = {29},
	number = {1},
	pages = {1--36},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3299872},
	url = {https://dl.acm.org/doi/10.1145/3299872},
	issn = {1049-3301},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, mixed-variable optimization, surrogate modeling},
}

% A thorough analysis of the generational distance (GD), modified generational distance (GD+), and inverted generational distance (IGD) performance indicators for evaluating solutions to multiobjective optimization problems. I like pairing these performance indicators with the hypervolume indicator as they tend to measure convergence while hypervolume is more spread focused. The authors show that of the GD, IGD, and GD+ indicators, only GD+ is "Pareto compliant", which we often refer to as monotonic, meaning it strictly improves when a solution set is a strict superset of another. IGD is weakly monotonic in this sense, and standard GD is the worst option. The MGD equation is provided -- I have used it in some of my HPO work and it works very well
@inproceedings{ishibuchi2015modified,
	author = {Ishibuchi, Hisao and Masuda, Hiroyuki and Tanigaki, Yuki and Nojima, Yusuke},
	title = {Modified distance calculation in generational distance and inverted generational distance},
	year = {2015},
	month = {7},
	booktitle = {Evolutionary Multi-Criterion Optimization: 8th International Conference, EMO 2015, Guimar{\~a}es, Portugal, March 29--April 1, 2015. Proceedings, Part II 8},
	pages = {110--125},
	organization = {Springer},
	location = {Madrid Spain},
	doi = {10.1145/2739480.2754792},
	url = {https://dl.acm.org/doi/10.1145/2739480.2754792},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% The orgiinal NSGA-III paper part 2: a multiobjective evolutionary algorithm similar to NSGA-II, solving the drawback of NSGA-II performing poorly with many objectives by having the user provide a collection of well-spaced reference points and optimizing toward those -- this paper focuses on how to handle constraints
@article{jain2013evolutionary,
	author = {Jain, Himanshu and Deb, Kalyanmoy},
	title = {An evolutionary many-objective optimization algorithm using reference-point based nondominated sorting approach, part {II}: Handling constraints and extending to an adaptive approach},
	year = {2013},
	month = {8},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {18},
	number = {4},
	pages = {602--622},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2013.2281534},
	url = {http://ieeexplore.ieee.org/document/6595567},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization, constrained optimization, high dimension, evolutionary algorithms, EA},
}

% D.R. Jones' original paper on the landmark DIRECT (DIviding RECTangles) algorithm for direct search global blackbox optimization. The idea is that you can perform branch-and-bound style Lipschitzian optimization without knoweldge of the Lipschitz constant by dividing a rectangular design space into rectangular regions (rectangles) and subdividing those rectangles that could be potentially optimal given any Lipschitz constant by choosing those boxes on the lower left convex hull of the objective value at the center vs box diameter scatter plot
@article{jones1993lipschitzian,
	author = {Jones, Donald R. and Perttunen, Cary D. and Stuckman, Bruce E.},
	title = {Lipschitzian optimization without the Lipschitz constant},
	year = {1993},
	month = {10},
	journal = {Journal of optimization Theory and Applications},
	volume = {79},
	number = {1},
	pages = {157--181},
	publisher = {Springer},
	doi = {10.1007/bf00941892},
	url = {http://link.springer.com/10.1007/BF00941892},
	issn = {0022-3239},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% D.R. Jones' original paper on "efficient global optimization" (EGO). This is often credited as the original implementation of multivariate Bayesian optimization. Jones proposes using Gaussian processes to produce a Gaussian posterior, whose expected improvement function can be efficiently optimized to select the next candidate. This is also one of the early works in sequential optimization via a generic (i.e., non polynomial) surrogate model. However, earlier work on design-of-experiments and response surface modeling did exist in the engineering design optimization space. Earlier papers had explored the idea of optimizing Gaussian processes to select experiments (especially in one and two-dimensions). However, this paper is a landmark in that it gave rise to the field of multivariate Bayesian optimization. EGO is still often used as the benchmark Bayesian optimization algorithm.
@article{jones1998efficient,
	author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
	title = {Efficient global optimization of expensive black-box functions},
	year = {1998},
	journal = {Journal of Global optimization},
	volume = {13},
	pages = {455--492},
	publisher = {Springer},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, Gaussian process},
}

% Introducing Dragonfly: an open source numerical software package for solving neural architecture search problems via Bayesian optimization and solving an optimal transport problem to evaluate the distance between two networks. Considered a bit of a landmark paper for neural network architecture search problems. The open source Python software is widely used for a variety of applications outside NAS, including molecular discovery
@article{kandasamy2020tuning,
	author = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabus and Xing, Eric P.},
	title = {Tuning Hyperparameters without Grad Students: Scalable and Robust {Bayesian} Optimisation with {Dragonfly}},
	year = {2020},
	journal = {Journal of Machine Learning Research},
	volume = {21},
	number = {81},
	pages = {1--27},
	url = {http://jmlr.org/papers/v21/18-223.html},
	git = {http://github.com/dragonfly/dragonfly},
	keywords = {optimization, Bayesian optimization, global optimization, mixed-variable optimization, autotuning, hyperparameter optimization, surrogate modeling, Gaussian process},
}

% A survey of multiobjective optimization algorithms for hyperparameter tuning in the context of automatic machine learning (autoML)
@article{karl2023multiobjective,
	author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
	title = {Multi-Objective Hyperparameter Optimization in Machine Learning -- An Overview},
	year = {2023},
	month = {12},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	volume = {3},
	number = {4},
	pages = {1--50},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/3610536},
	url = {https://dl.acm.org/doi/10.1145/3610536},
	issn = {2688-299X},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, global optimization, mixed-variable optimization, autotuning, hyperparameter optimization, surrogate modeling, decision trees, Gaussian process},
}

% Karypis journal paper on numerical algorithms for multilevel graph partitioning. The idea in multilevel graph partitioning is that when given a very large graph, we first coarsen the graph to a manageable size. Then we compute the cut at the coursest level and refine this cut at the finer levels as we flatten (un-coarsen) the graph in a typical V-cycle. The meat of this paper is actually a detailed comparison of various algorithms for coarsening and their quality tradeoffs, time complexities, and compression factors. Additionally a similar comparison of cut and cut-refinement algorithms
@article{karypis1998fast,
	author = {Karypis, George and Kumar, Vipin},
	title = {A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs},
	year = {1998},
	month = {1},
	journal = {SIAM Journal on Scientific Computing},
	volume = {20},
	number = {1},
	pages = {359--392},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/S1064827595287997},
	url = {http://epubs.siam.org/doi/10.1137/S1064827595287997},
	issn = {1064-8275},
	keywords = {optimization, partitioning},
}

% Official documentation and techreport for the widely-used multilevel hypergraph partitioning software hMETIS, which is a standard in partitioning based placement
@techreport{karypis1998hmetis,
	author = {Karypis, George and Kumar, Vipin},
	title = {{hMETIS}: A hypergraph partitioning package},
	year = {1998},
	number = {version 1.5.3},
	institution = {Department of Computer Science \& Engineering, University of Minnesota},
	address = {Minneapolis, MN, USA},
	url = {https://course.ece.cmu.edu/~ee760/760docs/hMetisManual.pdf},
	keywords = {optimization, partitioning},
}

% Karypis paper on multilevel hypergraph partitioning for VLSI applications. This is also kind of describing the algorithm used in the hMETIS partitioning software
@article{karypis1999multilevel,
	author = {Karypis, G. and Aggarwal, R. and Kumar, V. and Shekhar, S.},
	title = {Multilevel hypergraph partitioning: applications in {VLSI} domain},
	year = {1999},
	month = {3},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	volume = {7},
	number = {1},
	pages = {69--79},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/92.748202},
	url = {http://ieeexplore.ieee.org/document/748202/},
	issn = {1063-8210},
	keywords = {optimization, partitioning},
}

% Proposes a new technique called manifold sampling to solve blackbox optimization problems where a smooth blackbox function is composed with a piecewise linear (non blackbox) function. Normally, this would create a nonsmooth blackbox function, but by modeling the blackbox function separately and sampling the different manifolds produced by the changes in active components of the piecewise function, we can still model the blackbox function with smooth techniques and solve the optimization problem efficiently
@article{khan2018manifold,
	author = {Khan, Kamil A. and Larson, Jeffrey and Wild, Stefan M.},
	title = {Manifold Sampling for Optimization of Nonconvex Functions that are Piecewise Linear Compositions of Smooth Components},
	year = {2018},
	month = {1},
	journal = {{SIAM} Journal on Optimization},
	volume = {28},
	number = {4},
	pages = {3001--3024},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/17m114741x},
	url = {https://epubs.siam.org/doi/10.1137/17M114741X},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% The SPEA2+ algorithm for solving multiobjective optimization problems with evolutionary algorithms. Apparently this is widely-used numerical software, but I can't find the download
@inproceedings{kim2004spea2,
	author = {Kim, Mifa and Hiroyasu, Tomoyuki and Miki, Mitsunori and Watanabe, Shinya},
	title = {{SPEA2+}: Improving the performance of the {S}trength {P}areto {E}volutionary {A}lgorithm 2},
	year = {2004},
	booktitle = {Proc. International Conference on Parallel Problem Solving from Nature (PPSN VIII)},
	pages = {742--751},
	organization = {Springer},
	location = {Birmingham, UK},
	isbn = {978-3-540-30217-9_75},
	keywords = {optimization, multiobjective optimization, evolutionary algorithms, EA},
}

% The original paper on Adam: an adaptive gradient and moment estimator that uses second order moments to approximate curvature (i.e., Hessian information) in order to accelerate the convergence of AdaGrad. In particular, this means applying Nesterov's momentum to both the gradient and curvature estimations. From 2015-2024 this was the state-of-the-art algorithm for optimization of neural network weights during training, and was what was typically meant when people talked about stochastic gradient descent.
@inproceedings{kingma2015adam,
	author = {Kingma, Diedrik and Ba, Jimmy},
	title = {Adam: A method for stochastic optimization},
	year = {2015},
	booktitle = {3rd International Conference on Learning Representations (ICLR 2015)},
	numpages = {11},
	location = {San Diego, CA, USA},
	url = {https://arxiv.org/abs/1412.6980},
	keywords = {optimization, stochastic optimization, convex optimization},
}

% The Klee Minty cube: A famous counterexample showing that for every pivoting strategy for the simplex method, we can construct a pathological problem where that strategy will visit every vertex of the cube before the solution. This proves that the simplex method cannot be used to solve linear programming problems in strongly polynomial time
@article{klee1972how,
	author = {Klee, Victor and Minty, George J.},
	title = {How good is the simplex algorithm?},
	year = {1972},
	journal = {Inequalities},
	volume = {III},
	pages = {159--175},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% The original ParEGO algorithm paper: in each iteration of the algorithm, the authors use NSGA-II to optimize the expected improvement of the Gaussian process surrogates of each objective. Then, results are scalarized using augmented chebyshev and the best results are evaluated for the next iteration
@article{knowles2006parego,
	author = {Knowles, Joshua},
	title = {{ParEGO:} A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
	year = {2006},
	month = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {8},
	number = {5},
	pages = {1341--66},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/tevc.2005.851274},
	url = {http://ieeexplore.ieee.org/document/1583627},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, scalarization, evolutionary algorithm, EA, Gaussian process},
}

% Often credited as the first paper where the idea of Bayesian optimization (in the 1D case) was proposed -- personally, I usually credit Jones with EGO
@article{kushner1964new,
	author = {Kushner, H. J.},
	title = {A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise},
	year = {1964},
	month = {03},
	journal = {Journal of Basic Engineering},
	volume = {86},
	number = {1},
	pages = {97--106},
	publisher = {ASME International},
	doi = {10.1115/1.3653121},
	url = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/86/1/97/5763745/97\_1.pdf},
	issn = {0021-9223},
	keywords = {optimization, Bayesian optimization, global optimization, surrogate modeling, Gaussian process},
}

% Stochastic approximation algorithm (i.e., stochastic gradient descent) and how to analyze its radius of convergence for a fixed step-size -- you can decay its step size at a square-summable but not summable rate to guarantee convergence in the limit
@article{lai2003stochastic,
	author = {Lai, Tze Leung},
	title = {Stochastic approximation},
	year = {2003},
	journal = {The annals of Statistics},
	volume = {31},
	number = {2},
	pages = {391--406},
	publisher = {Institute of Mathematical Statistics},
    url = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-2/Stochastic-approximation-invited-paper/10.1214/aos/1051027873.full},
	keywords = {optimization, stochastic optimization, convex optimization},
}

% HyperNOMAD is the NOMAD team's open source Python software for performing hyperparameter optimization. Last I checked, it was not yet mature enough to use outside of their test problems, but I suspect improvements have been made since then
@article{lakhmiri2021hypernomad,
	author = {Lakhmiri, Dounia and Digabel, S{\'e}bastien Le and Tribes, Christophe},
	title = {{HyperNOMAD}: Hyperparameter Optimization of Deep Neural Networks Using Mesh Adaptive Direct Search},
	year = {2021},
	month = {9},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	volume = {47},
	number = {3},
	pages = {1--27},
	publisher = {ACM New York, NY, USA},
	doi = {10.1145/3450975},
	url = {https://dl.acm.org/doi/10.1145/3450975},
	issn = {0098-3500},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, mixed-variable optimization, autotuning, hyperparameter optimization, surrogate modeling},
}

% The APOSMM Python software is a framework for implementing multistart derivative-free optimization algorithms and running them in asynchronously
@article{larson2018asynchronously,
	author = {Larson, Jeffrey and Wild, Stefan M},
	title = {Asynchronously parallel optimization solver for finding multiple minima},
	year = {2018},
	month = {9},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {3},
	pages = {303--332},
	publisher = {Springer},
	doi = {10.1007/s12532-017-0131-4},
	url = {http://link.springer.com/10.1007/s12532-017-0131-4},
	issn = {1867-2949},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, global optimization},
}

% A thorough survey of techniques and algorithms in derivative-free optimization
@article{larson2019derivativefree,
	author = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
	title = {Derivative-free optimization methods},
	year = {2019},
	month = {5},
	journal = {Acta Numerica},
	volume = {28},
	pages = {287--404},
	publisher = {Cambridge University Press},
	doi = {10.1017/S0962492919000060},
	url = {https://www.cambridge.org/core/product/identifier/S0962492919000060/type/journal_article},
	issn = {0962-4929},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Jeff's GOOMBAH paper on exploiting composite structures in derivative-free optimization, meaning that a blackbox function is composed with an algebraic function and we want to optimize the result, while exploiting the fact that we know the equation of the algebraic function. Similar technique is used in ParMOO to exploit this same structure and others that are specific to the multiobjective case
@article{larson2024structureaware,
	author = {Larson, Jeffrey and Menickelly, Matt},
	title = {Structure-Aware Methods for Expensive Derivative-Free Nonsmooth Composite Optimization},
	year = {2024},
	month = {3},
	journal = {Mathematical Programming Computation},
	volume = {16},
	number = {1},
	pages = {1--36},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s12532-023-00245-5},
	url = {https://link.springer.com/10.1007/s12532-023-00245-5},
	issn = {1867-2949},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% A summary of classical solutions and challenges to solving sum-of-squares minimization of polynomials
@article{lasserre2001global,
	author = {Lasserre, Jean B},
	title = {Global optimization with polynomials and the problem of moments},
	year = {2001},
	month = {1},
	journal = {SIAM Journal on optimization},
	volume = {11},
	number = {3},
	pages = {796--817},
	publisher = {SIAM},
	doi = {10.1137/s1052623400366802},
	url = {http://epubs.siam.org/doi/10.1137/S1052623400366802},
	issn = {1052-6234},
	keywords = {optimization, convex optimization, global optimization},
}

% An adaptive scheme for selecting epsilon-constraint scalarizations when solving multiobjective optimization problems.
@article{laumanns2006efficient,
	author = {Laumanns, Marco and Thiele, Lothar and Zitzler, Eckart},
	title = {An efficient, adaptive parameter variation scheme for metaheuristics based on the epsilon-constraint method},
	year = {2006},
	month = {3},
	journal = {European Journal of Operational Research},
	volume = {169},
	number = {3},
	pages = {932--942},
	publisher = {Elsevier},
	doi = {10.1016/j.ejor.2004.08.029},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221704005715},
	issn = {0377-2217},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% NOMAD v3 is a widely-used open source numerical software package (in C++) for blackbox and derivative-free optimization via the MADS algorithms. Includes the official implementations of algorithms such as Ortho-MADS, PSD-MADS, and BiMADS. Support for parallel computing and surrogate modeling, and fairly extensible. Can be linked as a C++ library, or usage from command line interface. Used by a variety of industries and officially supported by Exxon Mobile. Has recently been replaced by the major refactor/rewrite in NOMAD v4. Still an example of widely-used open source numerical software, but the NOMAD v4 paper gives a look at how open source software practices have changed (improved) in the last 10 years
@article{ledigabel2011algorithm,
	author = {Le Digabel, S{\'e}bastien},
	title = {Algorithm 909: {NOMAD}: Nonlinear Optimization with the {MADS} Algorithm},
	year = {2011},
	month = {2},
	journal = {ACM Transactions on Mathematical Software},
	volume = {37},
	number = {4},
	numpages = {44},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1916461.1916468},
	url = {https://dl.acm.org/doi/10.1145/1916461.1916468},
	issn = {0098-3500},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, mixed-variable optimization, surrogate modeling},
}

% A taxonomy of constraint types encountered when solving blackbox / simulation optimization: quantifiable vs nonquantifiable, relaxable vs unrelaxable, a priori vs simulation-based, and known vs hidden.
@techreport{ledigabel2024taxonomy,
	author = {Le Digabel, S\'ebastien and Wild, Stefan M.},
	title = {A Taxonomy of Constraints in Black-Box Simulation-Based Optimization},
	year = {2024},
	month = {6},
	booktitle = {Optimization and Engineering},
	volume = {25},
	number = {2},
	pages = {1125--1143},
	institution = {Springer Science and Business Media LLC},
	doi = {10.1007/s11081-023-09839-3},
	url = {https://link.springer.com/10.1007/s11081-023-09839-3},
	issn = {1389-4420},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, constrained optimization},
}

% An algorithm that combines direct search / pattern search with an augmented Lagrangian penalty term in order to solve a constrained blackbox optimization problem
@article{lewis2002globally,
	author = {Lewis, Robert Michael and Torczon, Virginia},
	title = {A Globally Convergent Augmented {L}agrangian Pattern Search Algorithm for Optimization with General Constraints and Simple Bounds},
	year = {2002},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {12},
	number = {4},
	pages = {1075--1089},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/S1052623498339727},
	url = {http://epubs.siam.org/doi/10.1137/S1052623498339727},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search, global optimization, constrained optimization},
}

% The official publication of SMAC3 -- the latest version of SMAC from the automl group. SMAC was the first hyperparameter optimization and neural architecture search software to use random forest surrogates for modeling the hyperparameter configuration space. They also use a hierarchy of hyperparameters to handle "hidden parameters". They support multi and single-fidelity NAS applications. The open source software is written in Python and available from: github.com/automl/SMAC3
@article{lindauer2022smac3,
	author = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, André and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, René and Hutter, Frank},
	title = {{SMAC3}: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
	year = {2022},
	journal = {Journal of Machine Learning Research},
	volume = {23},
	number = {54},
	pages = {1--9},
	url = {http://jmlr.org/papers/v23/21-0888.html},
	keywords = {optimization, decision trees, Bayesian optimization, global optimization, autotuning, hyperparameter optimization, surrogate modeling},
}

% An early paper proposing the usage of surrogate models within multiobjective evolutionary algorithms in order to improve their performance on computationally expensive blackbox / simulation optimization problems, where the function evaluation budget may be limited
@inproceedings{liu2016surrogate,
	author = {Liu, Bo and Sun, Nan and Zhang, Qingfu and Grout, Vic and Gielen, Georges},
	title = {A surrogate model assisted evolutionary algorithm for computationally expensive design optimization problems with discrete variables},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 {IEEE} Congress on Evolutionary Computation ({CEC})},
	pages = {1650--1657},
	organization = {{IEEE}},
	location = {Vancouver, BC, Canada},
	doi = {10.1109/cec.2016.7743986},
	url = {http://ieeexplore.ieee.org/document/7743986},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, surrogate modeling, evolutionary algorithms, EA},
}

% Adaptive Kriging model-based sampling basically means using an interpolating Gaussian process's uncertainty information to select where to sample the next point during an adaptive sampling algorithm (for generating design-of-experiments or design space exploration).
@article{liu2017adaptive,
	author = {Liu, Haitao and Cai, Jianfei and Ong, Yew-Soon},
	title = {An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error},
	year = {2017},
	month = {11},
	journal = {Computers \& Chemical Engineering, Special Section - ESCAPE-26},
	volume = {106},
	pages = {171--182},
	publisher = {Elsevier BV},
	doi = {10.1016/j.compchemeng.2017.05.025},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009813541730234X},
	issn = {0098-1354},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, Gaussian process},
}

% The DARTS algorithm trains a probability distribution of possible weights, layer types, and connection topologies to sample. This creates a continuous relaxation of the neural architecture search problem, allowing it to be optimized using derivative-based techniques in fewer operations. This allows it to train faster, however, this method doesn't work that well in practice because even if it trains nice distributions, it generally doesn't actually sample well-performing networks with high probability.
@inproceedings{liu2019darts,
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	title = {{DARTS}: Differentiable Architecture Search},
	year = {2019},
	booktitle = {Proc. 7th International Conference on Learning Representations (ICLR '19)},
	url = {https://openreview.net/forum?id=S1eYHoC5FX},
	keywords = {optimization, autotuning, hyperparameter optimization},
}

% The DFMO algorithm is a multiobjective line search, which the authors recommend combining with MODIR to improve its convergence to the Pareto front after identifying the global Pareto front. The open source numerical software is implemented in Fortran and is currently bundled inside the MODIR software package on the authors' GitHub account
@article{liuzzi2016derivativefree,
	author = {Liuzzi, Giampaolo and Lucidi, Stefano and Rinaldi, Francesco},
	title = {A derivative-free approach to constrained multiobjective nonsmooth optimization},
	year = {2016},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {26},
	number = {4},
	pages = {2744--2774},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/15M1037810},
	url = {http://epubs.siam.org/doi/10.1137/15M1037810},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% The official GitHub account for the DFO-lib -- open source numerical software in Fortran for solving blackbox optimization and multiobjective optimization problems. The library used to be obtained via download from Liuzzi's personal website, where it was referred to as the DFO lib. In 2024 it appears to have been migrated to a GitHub account, with each individual piece of software in its own separate repository. Therefore, any reference to the DFO-lib must now be directed to the account as a whole, not an individual repository.
@misc{liuzzi2024dfolib,
	author = {Liuzzi, Giampaolo and others, },
	title = {{DFO-lib}},
	year = {2024},
	booktitle = {GitHub repository},
	number = {0.3.8},
	publisher = {GitHub},
	url = {https://github.com/DerivativeFreeLibrary},
	note = {Last accessed: Jul 2024},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% Multiobjective extension of the DIRECT algorithm for derivative-free blackbox optimization. This algorithm may suffer from some scalability issues, but is a good first step
@article{lovison2021extension,
	author = {Lovison, Alberto and Miettinen, Kaisa},
	title = {On the Extension of the {DIRECT} Algorithm to Multiple Objectives},
	year = {2021},
	month = {2},
	journal = {Journal of Global Optimization},
	volume = {79},
	number = {2},
	pages = {387--412},
	publisher = {Springer},
	doi = {10.1007/s10898-020-00942-8},
	url = {https://link.springer.com/10.1007/s10898-020-00942-8},
	issn = {0925-5001},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization},
}

% NSGA-Net is the NSGA-II/pymoo team's multiobjective genetic algorithm based NAS solver. The open source python software is available from: github.com/ianwhale/nsga-net
@article{lu2018nsganet,
	author = {Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang},
	title = {NSGA-NET: a multi-objective genetic algorithm for neural architecture search},
	year = {2018},
	journal = {GECCO-2019},
	keywords = {optimization, multiobjective optimization, mixed-variable optimization, autotuning, hyperparameter optimization, evolutionary algorithm, EA},
}

% Our paper proposing test functions for convex optimization problems with very specific properties, in order to gauge the effectiveness and robustness of various techniques subject to various forms of degeneracy
@inproceedings{lux2020analytic,
	author = {Lux, Thomas C. H. and Chang, Tyler H.},
	title = {Analytic test functions for generalizable evaluation of convex optimization techniques},
	year = {2020},
	month = {3},
	booktitle = {Proc. IEEE SoutheastCon 2020},
	numpages = {8},
	organization = {Institute of Electrical and Electronics Engineers},
	location = {Raleigh, NC, USA},
	doi = {10.1109/SoutheastCon44009.2020.9368254},
	url = {https://ieeexplore.ieee.org/document/9368254/},
	keywords = {optimization, convex optimization},
}

% Discussion of online learning in the context of multiobjective optimization
@inproceedings{mannor2014approachability,
	author = {Mannor, Shie and Perchet, Vianney and Stoltz, Gilles},
	title = {Approachability in unknown games: {O}nline learning meets multi-objective optimization},
	year = {2014},
	month = {13--15 June},
	booktitle = {Proc. 27th Conference on Learning Theory (PMLR)},
	series = {Proceedings of Machine Learning Research},
	volume = {35},
	pages = {339--355},
	organization = {PMLR},
	location = {Barcelona, Spain},
	url = {https://proceedings.mlr.press/v35/mannor14.html},
	keywords = {optimization, multiobjective optimization},
}

% Survey of common multiobjective optimization algorithms and scalarization techniques in the context of engineering design optimization
@article{marler2004survey,
	author = {Marler, Timothy R. and Arora, Jasbir S.},
	title = {Survey of multi-objective optimization methods for engineering},
	year = {2004},
	month = {4},
	journal = {Structural and Multidisciplinary Optimization},
	volume = {26},
	number = {6},
	pages = {369--395},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s00158-003-0368-6},
	url = {http://link.springer.com/10.1007/s00158-003-0368-6},
	issn = {1615-147X},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, scalarization},
}

% This is a modification to the NSGA-II multiobjective optimization algorithm where the authors introduce a pipeline of smaller "islands" of populations, which evolve independently. Cross migration between these islands keeps the populations from diverging too far apart and allowing for progress to be shared between all islands through eventual consistency. This modification allows NSGA-II to run fully asynchronously. This is the version used by Optuna
@inproceedings{martens2013asynchronous,
	author = {M{\"a}rtens, Marcus and Izzo, Dario},
	title = {The asynchronous island model and {NSGA-II}: study of a new migration operator and its performance},
	year = {2013},
	booktitle = {Proceedings of the 15th annual conference on Genetic and evolutionary computation},
	pages = {1173--1180},
	keywords = {optimization, multiobjective optimization, evolutionary algorithm, EA},
}

% pyMDO: an open source numerical Python framework for modeling and solving multidisciplinary engineering design optimization problems
@article{martins2009pymdo,
	author = {Martins, Joaquim R. R. A. and Marriage, Christopher and Tedford, Nathan},
	title = {{pyMDO}: An Object-Oriented Framework for Multidisciplinary Design Optimization},
	year = {2009},
	month = {8},
	journal = {ACM Transactions on Mathematical Software},
	volume = {36},
	number = {4},
	numpages = {20},
	publisher = {ACM},
	doi = {10.1145/1555386.1555389},
	url = {https://dl.acm.org/doi/10.1145/1555386.1555389},
	issn = {0098-3500},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% Algorithms and theorems on the difficulty of finding basic solutions for linear programming problems
@article{megiddo1991finding,
	author = {Megiddo, Nimrod},
	title = {On finding primal- and dual-optimal bases},
	year = {1991},
	month = {2},
	journal = {ORSA Journal on Computing},
	volume = {3},
	number = {1},
	pages = {63--65},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.3.1.63},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.3.1.63},
	issn = {0899-1499},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% DESDEO an open source Python framework for implementing interactive multiobjective optimization solvers
@article{misitano2021desdeo,
	author = {Misitano, Giovanni and Saini, Bhupinder S. and Afsar, Bekir and Shavazipour, Babooshka and Miettinen, Kaisa},
	title = {{DESDEO}: The Modular and Open Source Framework for Interactive Multiobjective Optimization},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {148277--148295},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2021.3123825},
	url = {https://ieeexplore.ieee.org/document/9591595},
	issn = {2169-3536},
	keywords = {optimization, multiobjective optimization},
}

% Introduction of performance profiles for benchmarking derivative-free and blackbox optimization algorithms and solvers. In particular, when comparing a set of optimization algorithms on a set of problems, we first calculate for each problem and solver the ratio of the value of some performance metric for that solver on that problem / the best observed value of that metric for all solvers. Then, we rank the solvers according to the fraction of problems where their performance is at least alpha. (Sort of an h-index like ranking).
@article{more2009benchmarking,
	author = {Mor\'{e}, Jorge J. and Wild, Stefan M.},
	title = {Benchmarking Derivative-Free Optimization Algorithms},
	year = {2009},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {20},
	number = {1},
	pages = {172--191},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/080724083},
	url = {http://epubs.siam.org/doi/10.1137/080724083},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, benchmarking},
}

% MORDRED: A 3D molecular descriptor calculator, which is widely used for embedding molecules into a continuous latent space (parameterized by their descriptors) which can be used to solve chemical property optimization problems. The MORDRED software is available open source in Python.
@article{moriwaki2018mordred,
	author = {Moriwaki, Hirotomo and Tia, Yu-Shi and Kawashita, Norihito and Takagi, Tatsuya},
	title = {Mordred: a molecular descriptor calculator},
	year = {2018},
	month = {12},
	journal = {Journal of Cheminformatics},
	volume = {10},
	number = {1},
	articleno = {4},
	numpages = {14},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1186/s13321-018-0258-y},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y},
	issn = {1758-2946},
	keywords = {optimization, simulation optimization, mixed-variable optimization},
}

% SOCEMO: A response surface modeling (RSM) based algorithm for solving multiobjective optimization problems. Uses a Latin hypercube design-of-experiments, RBF surrogate modeling, multiple scalarizations, and solves the scalarized subproblem via evolutionary algorithms to produce a batch of evaluations in each iteration of the algorithm
@article{muller2017socemo,
	author = {M{\"u}ller, Juliane},
	title = {{SOCEMO}: {S}urrogate optimization of computationally expensive multiobjective problems},
	year = {2017},
	month = {11},
	journal = {INFORMS Journal on Computing},
	volume = {29},
	number = {4},
	pages = {581--596},
	publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	doi = {10.1287/ijoc.2017.0749},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.2017.0749},
	issn = {1091-9856},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, scalarization, evolutionary algorithms, EA, RBFs},
}

% This is the last TAO (toolkit for advanced optimization) reference before this open source numerical simulation optimization software when merged with PETSc, into a single PETSc + TAO release
@techreport{munson2015tao,
	author = {Munson, Todd and Sarich, Jason and Wild, Stefan and Benson, Steven and McInnes, Lois Curfman},
	title = {{TAO} 3.5 Users Manual},
	year = {2015},
	number = {ANL/MCS-TM-322 version 3.5},
	institution = {Argonne National Laboratory},
	address = {Lemont, IL, USA},
	url = {https://www.mcs.anl.gov/petsc/petsc-3.5.4/docs/tao_manual.pdf},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, convex optimization},
}

% The classical textbook on response surface methodology and modeling practices. Contains useful information on the basic framework and applications of RSM. Also a useful reference for many of the options for specific techniques: Chapter 7 is a good reference for basic techniques in multiobjective RSM and Chapters 8-9 surveys the basic methods in design-of-experiments
@book{myers2016response,
	author = {Myers, Raymond H. and Montgomery, Douglas C. and Anderson-Cook, Christine M.},
	title = {Response Surface Methodology: Process and Design Optimization Using Designed Experiments},
	year = {2016},
	edition = {4},
	publisher = {John Wiley \& Sons, Inc.},
	address = {Hoboken, NJ, USA},
	isbn = {9781118916032},
    url = {https://books.google.com/books?hl=en&lr=&id=T-BbCwAAQBAJ&oi=fnd&pg=PR13&dq=Response+Surface+Methodology:+Process+and+Product+Optimization+Using+Designed+Experiments,+4th+Edition&ots=O3jdPna83T&sig=IimJlE46JBVkHOu7eik3RN9Z5GA#v=onepage&q=Response%20Surface%20Methodology%3A%20Process%20and%20Product%20Optimization%20Using%20Designed%20Experiments%2C%204th%20Edition&f=false},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, surrogate modeling},
}

% Original publication on Nesterov's momentum. I haven't read it (it is hard to find a copy and likely in Russian) but this is the preferred citation. The equation for Nesterov momentum in gradient descent is instead of using the update: x' = x - a*g(x), use x' = x - a*g(y) - b*v where y = x - b*v and v = b*v + a*g(x) -- in this equation, b*v is the momentum term which smooths out poor conditioning in the problem by encouraging the algorithm to continue in the direction it was headed instead of oscillating. Nexterov proves that this term also leads to better convergence rates. For best results, b is usually chosen to be a large value such as 0.9 or 0.99
@inproceedings{nesterov1983method,
	author = {Nesterov, Yurii},
	title = {A method for solving the convex programming problem with convergence rate O (1/k2)},
	year = {1983},
	booktitle = {Dokl akad nauk Sssr},
	volume = {269},
	numpages = {543},
	keywords = {optimization, stochastic optimization, convex optimization},
}

% An application of VTMOP for the multiobjective optimization (tuning) of the LCLS-II photoinjector (linear accelerator at SLAC). Had to use some hacks to get VTMOP to work, such as penalizing bad regions of the Pareto front, but ultimately performed better than NSGA-II
@article{neveu2023comparison,
	author = {Neveu, Nicole and Chang, Tyler H. and Franz, Paris and Hudson, Stephen and Larson, Jeffrey},
	title = {Comparison of multiobjective optimization methods for the {LCLS-II} photoinjector},
	year = {2023},
	month = {2},
	journal = {Computer Physics Communication},
	volume = {283},
	articleno = {108566},
	numpages = {10},
	publisher = {Elsevier BV},
	doi = {10.1016/j.cpc.2022.108566},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465522002855},
	issn = {0010-4655},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, evolutionary algorithm, EA},
}

% The classic textbook by Nocedal on fundamental techniques in nonlinear programming, such as local modeling and trust-region methods
@book{nocedal2006numerical,
	author = {Nocedal, Jorge and Wright, Stephen J.},
	title = {Numerical Optimization},
	year = {2006},
	booktitle = {Springer Series in Operations Research and Financial Engineering},
	series = {Springer Series in Operations Research and Financial Engineering},
	edition = {2},
	publisher = {Springer Verlag},
	address = {New York, NY, USA},
	doi = {10.1007/978-0-387-40065-5},
	url = {http://link.springer.com/10.1007/978-0-387-40065-5},
	isbn = {9780387303031},
	keywords = {optimization, convex optimization, constrained optimization, stochastic optimization},
}

% SCS is an open source numerical software for solving second-order cone problems from Steph Boyd's lab
@article{odonoghue2016conic,
	author = {O'Donoghue, Brendan and Chu, Eric and Parikh, Neal and Boyd, Stephen},
	title = {Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding},
	year = {2016},
	month = {June},
	journal = {Journal of Optimization Theory and Applications},
	volume = {169},
	number = {3},
	pages = {1042--1068},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10957-016-0892-3},
	url = {http://link.springer.com/10.1007/s10957-016-0892-3},
	issn = {0022-3239},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% PABO - a multiobjective Bayesian optimization software package that is specialized for NAS -- basically an inner network is used as a surrogate and an outer network is used to predict which designs to evaluate next -- I have serious reservations about this kind of approach, and it doesn't seem to work that well
@inproceedings{parsa2019pabo,
	author = {Parsa, Maryam and Ankit, Aayush and Ziabari, Amirkoushyar and Roy, Kaushik},
	title = {{PABO}: Pseudo Agent-Based Multi-Objective {B}ayesian Hyperparameter Optimization for Efficient Neural Accelerator Design},
	year = {2019},
	month = {11},
	booktitle = {Proc. 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
	pages = {1--8},
	organization = {IEEE/ACM},
	location = {Westin Westminster, CO, USA},
	doi = {10.1109/ICCAD45719.2019.8942046},
	url = {https://ieeexplore.ieee.org/document/8942046},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, global optimization, autotuning, hyperparameter optimization, surrogate modeling},
}

% H-PABO multiobjective Bayesian optimization framework, specialized for NAS, and improvement on PABO
@article{parsa2020bayesian,
	author = {Parsa, Maryam and Mitchell, John P. and Schuman, Catherine D. and Patton, Robert M. and Potok, Thomas E. and Roy, Kaushik},
	title = {Bayesian Multi-objective Hyperparameter Optimization for Accurate, Fast, and Efficient Neural Network Accelerator Design},
	year = {2020},
	month = {7},
	journal = {Frontiers in Neuroscience},
	volume = {14},
	numpages = {667},
	publisher = {Frontiers Media SA},
	doi = {10.3389/fnins.2020.00667},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00667/full},
	issn = {1662-453X},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, global optimization, autotuning, hyperparameter optimization, surrogate modeling},
}

% YAHPO gym is yet another hyperparameter optimization gym consisting of benchmark and test problems for testing neural architecture search and hyperparameter optimization algorithms. Most of the test problems are based on xgboost, knn, or svm models of real data. As of 2022, this one was not as mature as HPOBench and JAHS-Bench, so we didn't use it. They claim to now offer multiobjective optimization test problems as well. Open source Python software implementation available at: github.com/slds-lmu/yahpo_gym
@inproceedings{pfisterer2022yahpo,
	author = {Pfisterer, Florian and Schneider, Lennart and Moosbauer, Julia and Binder, Martin and Bischl, Bernd},
	editor = {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
	title = {{YAHPO} Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization},
	year = {2022},
	booktitle = {Proceedings of the First International Conference on Automated Machine Learning},
	series = {Proceedings of Machine Learning Research},
	volume = {188},
	pages = {3/1--39},
	organization = {PMLR},
	url = {https://proceedings.mlr.press/v188/pfisterer22a.html},
	keywords = {optimization, multiobjective optimization, autotuning, hyperparameter optimization, benchmarking},
}

% The COBYLA paper on Powell's constrainted optimization by linear approximation algorithm. COBYLA basically performs gradient descent on a constrainted blackbox optimization problem by fitting a linear model to the underlying function and following its gradient within a shrinking trust region. COBYLA is typically able to do this taking typically only one or two function evaluation per iteration since typically only one point is exiting the trust-region per iteration. (Occasionally, additional model improvement points must be sampled to maintain the interpolation set geometry). This makes COBYLA barely more expensive then true gradient descent. COBYQA has supplanted COBYLA since then (the Q standing for quadratic), but I personally prefer COBYLA still as a find the locally linear models to be more robust against noisy and nonsmooth data, still efficiently finding local minima even though COBYLA was not design for such problems. The original open source software was in impossibly complex old-style Fortran. A modern Fortran version has been provided in Pima by Zaikun Zhang, and a modern Python implementation is provided in PDFO by Ragonneau and Zhang.
@inproceedings{powell1994direct,
	author = {Powell, Michael J. D.},
	title = {A direct search optimization method that models the objective and constraint functions by linear interpolation},
	year = {1994},
	booktitle = {Gomez, S. and Hennart, J. P. (eds) Advances in Optimization and Numerical Analysis, vol 275},
	pages = {51--67},
	organization = {Springer},
	doi = {10.1007/978-94-015-8330-5_4},
	url = {http://link.springer.com/10.1007/978-94-015-8330-5_4},
	isbn = {9789048143580},
	keywords = {optimization, blackbox optimization, derivative-free optimization, DFO, constrained optimization},
}

% Experiences, challenges, and techniques for integrating the blackbox optimization solvers VTDIRECT95 and QNSTOP into the parallel service architecture SORCER
@inproceedings{raghunath2017global,
	author = {Raghunath, Chaitra and Chang, Tyler H. and Watson, Layne T. and Jrad, Mohamad and Kapania, Rakesh K. and Kolonay, Raymond M.},
	title = {Global deterministic and stochastic optimization in a service oriented architecture},
	year = {2017},
	booktitle = {Proc. 2017 Spring Simulation Conference (SpringSim 2017), the 25th High Performance Computing Symposium (HPC '17)},
	numpages = {7},
	organization = {SCS},
	location = {Virginia Beach, VA, USA},
	doi = {10.22360/springsim.2017.hpc.023},
	url = {http://dl.acm.org/citation.cfm?id=3108103},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization, stochastic optimization},
}

% PDFO: An open source modern Python implementation of Powell's derivative-free numerical optimization software suite, which is considered to be the standard (baseline) in derivative-free optimization solvers
@misc{ragonneau2021pdfo,
	author = {Ragonneau, Tom M. and Zhang, Zaikun},
	title = {{PDFO}: Cross-Platform Interfaces for {P}owell’s Derivative-Free Optimization Solvers},
	year = {2021},
	booktitle = {GitHub repository},
	number = {1.2},
	publisher = {GitHub},
	url = {https://github.com/pdfo/pdfo},
	note = {Last accessed: Apr 2025},
	keywords = {optimization, blackbox optimization, derivative-free optimization, DFO, constrained optimization},
}

% The calculus of simplex gradients: a detailed analysis of simplex gradients and their properties for approximating true gradients when solving derivative-free and blackbox optimization problems
@article{regis2015calculus,
	author = {Regis, Rommel G.},
	title = {The calculus of simplex gradients},
	year = {2015},
	month = {6},
	journal = {Optimization Letters},
	volume = {9},
	number = {5},
	pages = {845--865},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11590-014-0815-x},
	url = {http://link.springer.com/10.1007/s11590-014-0815-x},
	issn = {1862-4472},
	keywords = {optimization, blackbox optimization, derivative-free optimization, DFO},
}

% Using RBF surrogates in the context of multiobjective optimization. I believe he ultimately solved the surrogate problems with NSGA-II or some other heuristic
@article{regis2016multiobjective,
	author = {Regis, Rommel G.},
	title = {Multi-objective constrained black-box optimization using radial basis function surrogates},
	year = {2016},
	month = {9},
	journal = {Journal of Computational Science},
	volume = {16},
	pages = {140--155},
	publisher = {Elsevier BV},
	doi = {10.1016/j.jocs.2016.05.013},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1877750316300904},
	issn = {1877-7503},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, evolutionary algorithm, EA, RBFs},
}

% The original publication of PAWS: An adaptive weighted sum and trust-regions method for solving biobjective (multiobjective) optimization problems. The key here is that the trust-regions help the weighted sum scalarization reach into nonconvex regions of the Pareto front, even though that would not normally be possible
@inproceedings{ryu2009pareto,
	author = {Ryu, Jong-hyun and Kim, Sujin and Wan, Hong},
	title = {Pareto front approximation with adaptive weighted sum method in multiobjective simulation optimization},
	year = {2009},
	month = {12},
	booktitle = {Proc. 2009 Winter Simulation Conference (WSC '09)},
	pages = {623--633},
	organization = {IEEE},
	location = {Austin, TX, USA},
	doi = {10.1109/WSC.2009.5429562},
	url = {http://ieeexplore.ieee.org/document/5429562},
	keywords = {optimization, multiobjective optimization, simulation optimization, scalarization},
}

% The latest version of PAWS: An adaptive weighted sum and trust-regions method for solving biobjective (multiobjective) optimization problems. The key here is that the trust-regions help the weighted sum scalarization reach into nonconvex regions of the Pareto front, even though that would not normally be possible
@article{ryu2014derivativefree,
	author = {Ryu, Jong-Hyun and Kim, Sujin},
	title = {A derivative-free trust-region method for biobjective optimization},
	year = {2014},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {24},
	number = {1},
	pages = {334--362},
	publisher = {SIAM},
	doi = {10.1137/120864738},
	url = {http://epubs.siam.org/doi/10.1137/120864738},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% The SMT 2.0 paper, major improvements to the open source numerical software package (in Python) pySMT for solving multidisciplinary engineering design optimization (MDO) problems, while utilizing derivatives and providing numerical stability analysis for each surrogate model class. In SMT 2.0, support is added for hierarchical and mixed variables, and major improvements have been made to the structure, completeness, and features of the SMT library.
@article{saves2024smt,
	author = {Saves, Paul and Lafage, Rémi and Bartoli, Nathalie and Diouane, Youssef and Bussemaker, Jasper and Lefebvre, Thierry and Hwang, John T. and Morlier, Joseph and Martins, Joaquim R.R.A.},
	title = {SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
	year = {2024},
	month = {2},
	journal = {Advances in Engineering Software},
	volume = {188},
	numpages = {103571},
	publisher = {Elsevier BV},
	doi = {10.1016/j.advengsoft.2023.103571},
	url = {https://www.sciencedirect.com/science/article/pii/S096599782300162X},
	issn = {0965-9978},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, mixed-variable optimization, surrogate modeling, constrained optimization, RBFs, Gaussian process},
}

% Multiobjective molecule property optimization by optimizing processes in a continuous flow reactor (CFR) using the multiobjective evolutionary algorithm TS-EMO (thompson sampling evolutionary multiobjective optimization?)
@article{schweidtmann2018machine,
	author = {Schweidtmann, Artur M. and Clayton, Adam D. and Holmes, Nicholas and Bradford, Eric and Bourne, Richard A. and Lapkin, Alexei A.},
	title = {Machine learning meets continuous flow chemistry: Automated optimization towards the {Pareto} front of multiple objectives},
	year = {2018},
	month = {11},
	journal = {Chemical Engineering Journal},
	volume = {352},
	pages = {277--282},
	publisher = {Elsevier},
	doi = {10.1016/j.cej.2018.07.031},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1385894718312634},
	issn = {1385-8947},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, mixed-variable optimization, evolutionary algorithms, EA},
}

% A survey of hypervolume indicator usage in multiobjective evolutionary optimization, mainly in terms of algorithms that use hypervolume improvement and also as a performance indicator
@article{shang2020survey,
	author = {Shang, Ke and Ishibuchi, Hisao and He, Linjun and Pang, Lie Meng},
	title = {A survey on the hypervolume indicator in evolutionary multiobjective optimization},
	year = {2020},
	month = {2},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {25},
	number = {1},
	pages = {1--20},
	publisher = {IEEE},
	doi = {10.1109/TEVC.2020.3013290},
	url = {https://ieeexplore.ieee.org/document/9153850},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization, scalarization, evolutionary algorithms, EA},
}

% ASTRO-DF is a derivative-free stochastic optimization solver, targeted at noisy simulation optimization problems. Particularly those derived from Monte Carlo simulations. The algorithm is model-based within a trust region. The key contribution is adaptively determining how many Monte carlo samples to take in each iteration, in order to guarantee convergence. There are also ASTRO and ASTRO-C variants available. The open source Python software is available at github.com/sshashaa/astro-df
@inproceedings{shashaani2016astrodf,
	author = {Shashaani, Sara and Hunter, Susan R. and Pasupathy, Raghu},
	title = {{ASTRO-DF}: Adaptive sampling trust-region optimization algorithms, heuristics, and numerical experience},
	year = {2016},
	month = {12},
	booktitle = {2016 Winter Simulation Conference (WSC)},
	pages = {554--565},
	organization = {IEEE},
	location = {Washington, DC, USA},
	doi = {10.1109/WSC.2016.7822121},
	url = {http://ieeexplore.ieee.org/document/7822121/},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, stochastic optimization},
}

% A recent paper on the numerical performance of finite-difference-based methods for DFO. Personally, I don't think this is a viable approach given the performance of model-based methods. However, one of their experiments corroborates my experience that you can just run COBYLA on noisy and nonsmooth blackbox optimization problems, and even though it was not designed for those problems, it still does extremely well and regularly finds local minima (at least up to the noise level) -- from Nocedal's lab
@article{shi2023numerical,
	author = {Shi, Hao-Jun Michael and Xuan, Melody Qiming and Oztoprak, Figen and and, Jorge Nocedal},
	title = {On the numerical performance of finite-difference-based methods for derivative-free optimization},
	year = {2023},
	month = {3},
	journal = {Optimization Methods and Software},
	volume = {38},
	number = {2},
	pages = {289--311},
	publisher = {Taylor \& Francis},
	doi = {10.1080/10556788.2022.2121832},
	url = {https://doi.org/10.1080/10556788.2022.2121832},
	issn = {1055-6788},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO},
}

% The EDBO software: open source Python software for performing multiobjective bayesian optimization for chemical synthesis and molecular discovery. Links a multiobjective optimization solver with the MORDRED software for getting molecular descriptors and optimizes for the desired properties
@article{shields2021bayesian,
	author = {Shields, Benjamin J. and Stevens, Jason and Li, Jun and Parasram, Marvin and Damani, Farhan and Alvarado, Jesus I. M. and Janey, Jacob M. and Adams, Rryan P. and Doyle, Abigail G.},
	title = {Bayesian reaction optimization as a tool for chemical synthesis},
	year = {2021},
	month = {2},
	journal = {Nature},
	volume = {590},
	number = {7844},
	pages = {89--96},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41586-021-03213-y},
	url = {https://www.nature.com/articles/s41586-021-03213-y},
	issn = {0028-0836},
	git = {http://github.com/b-shields/edbo},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, surrogate modeling, mixed-variable optimization, Gaussian process},
}

% Steve Smale's list of unsolved open problems in mathematics, and mostly algorithms, one of which is reliably finding basic solutions to linear programming problems
@article{smale1998mathematical,
	author = {Smale, Steve},
	title = {Mathematical problems for the next century},
	year = {1998},
	month = {3},
	journal = {The Mathematical Intelligencer},
	volume = {20},
	number = {2},
	pages = {7--15},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/bf03025291},
	url = {http://link.springer.com/10.1007/BF03025291},
	issn = {0343-6993},
	keywords = {optimization, linear programming, LP, convex optimization, constrained optimization},
}

% One of the most popular textbooks in the field of multidisciplinary engineering design optimization. The introduction provides plenty of motivation for solving computational expensive multiobjective simulation / blackbox optimization problems
@book{sobieszczanskisobieski2015multidisciplinary,
	author = {Sobieszczanski-Sobieski, Jaroslaw and Morris, Alan and Van Tooren, Michel},
	title = {Multidisciplinary Design Optimization Supported by Knowledge Based Engineering},
	year = {2015},
	publisher = {John Wiley \& Sons, Ltd.},
	address = {Chichester, UK},
	isbn = {978-1-118-49212-3},
    doi = {10.1002/9781118897072},
	keywords = {optimization, multiobjective optimization, simulation optimization},
}

% OSQP is an open source numerical software for solving quadratic programming problems from Stephen Boyd's lab
@article{stellato2020osqp,
	author = {Stellato, Bartolomeo and Banjac, Goran and Goulart, Paul and Bemporad, Alberto and Boyd, Stephen},
	title = {{OSQP}: an operator splitting solver for quadratic programs},
	year = {2020},
	month = {12},
	journal = {Mathematical Programming Computation},
	volume = {12},
	number = {4},
	pages = {637--672},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s12532-020-00179-2},
	url = {http://link.springer.com/10.1007/s12532-020-00179-2},
	issn = {1867-2949},
	keywords = {optimization, quadratic programming, QP, convex optimization, constrained optimization},
}

% The original publication of the weighted Chebyshev scalarization scheme for multiobjective optimization
@article{steuer1983interactive,
	author = {Steuer, Ralph E and Choo, Eng-Ung},
	title = {An interactive weighted Tchebycheff procedure for multiple objective programming},
	year = {1983},
	month = {10},
	journal = {Mathematical programming},
	volume = {26},
	number = {3},
	pages = {326--344},
	publisher = {Springer},
	doi = {10.1007/BF02591870},
	url = {http://link.springer.com/10.1007/BF02591870},
	issn = {0025-5610},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% The textbook on evolutionary multiobjective optimization (MOO), including common algorithms, evaluation methodologies, fundamental techniques, and test problems
@book{sv2005evolutionary,
	editor = {Abraham, Ajith and Jain, Lakhmi and Goldberg, Robert},
	title = {Evolutionary Multiobjective Optimization: Theoretical Advances and Applications},
	year = {2005},
	booktitle = {Advanced Information and Knowledge Processing},
	series = {Advanced Information and Knowledge Processing Series},
	publisher = {Springer Verlag},
	address = {London, UK},
	doi = {10.1007/1-84628-137-7},
	url = {http://link.springer.com/10.1007/1-84628-137-7},
	isbn = {1852337877},
	keywords = {optimization, multiobjective optimization, evolutionary algorithms, EA},
}

% BoostDMS is numerical software library providing access to Custodio's direct search and pattern search software, including MultiGLODS and DMS, in Matlab with full parallel computing support
@article{tavares2022parallel,
	author = {Tavares, S. and Br\'as, C. P. and Cust\'odio, A. L. and Duarte, V. and Medeiros, P.},
	title = {Parallel Strategies for Direct Multisearch},
	year = {2022},
	month = {3},
	journal = {Numerical Algorithms},
	volume = {92},
	number = {3},
	pages = {1757--1788},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11075-022-01364-1},
	url = {https://link.springer.com/10.1007/s11075-022-01364-1},
	issn = {1017-1398},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, pattern search},
}

% A trust-region + RBF surrogate-based multiobjective optimization algorithm for solving heterogeneous multiobjective optimization problems (where one or more objectives is a computationally expensive blackbox, and one or more is not). The key is to just use the derivative of all the non blackbox objectives and use the model derivative for the blackbox terms. The algorithm itself follows something like Orbit
@article{thomann2019trustregion,
	author = {Thomann, Jana and Eichfelder, Gabriele},
	title = {A Trust-Region Algorithm for Heterogeneous Multiobjective Optimization},
	year = {2019},
	month = {1},
	journal = {{SIAM} Journal on Optimization},
	volume = {29},
	number = {2},
	pages = {1017--1047},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/18m1173277},
	url = {https://epubs.siam.org/doi/10.1137/18M1173277},
	issn = {1052-6234},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% An open source MATLAB platform for implementing and running wide-scale comparisons against other multiobjective evolutionary algorithms on standard multiobjective test problems
@article{tian2017platemo,
	author = {Tian, Ye and Cheng, Ran and Zhang, Xingyi and Jin, Yaochu},
	title = {{PlatEMO}: A {MATLAB} Platform for Evolutionary Multi-Objective Optimization [Educational Forum]},
	year = {2017},
	month = {11},
	journal = {IEEE Computational Intelligence Magazine},
	volume = {12},
	number = {4},
	pages = {73--87},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/MCI.2017.2742868},
	url = {http://ieeexplore.ieee.org/document/8065138},
	issn = {1556-603X},
	keywords = {optimization, multiobjective optimization, benchmarking, evolutionary algorithms, EA},
}

% Simulated annealing and reinforcement learning based FPGA placement. The RL contributions are tenuous at best, but still an example of potential RL impact in industry
@inproceedings{tian2022improving,
	author = {Tian, Chunsheng and Chen, Lei and Wang, Yuan and Wang, Shuo and Zhou, Jing and Zhang, Yaowei and Li, Guang},
	title = {Improving Simulated Annealing Algorithm for {FPGA} Placement Based on Reinforcement Learning},
	year = {2022},
	month = {6},
	booktitle = {2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
	volume = {10},
	number = {},
	pages = {1912--1919},
	organization = {IEEE},
	location = {Chongqing, China},
	doi = {10.1109/ITAIC54216.2022.9836761},
	url = {https://ieeexplore.ieee.org/document/9836761/},
	keywords = {optimization, reinforcment learning, RL, simulated annealing, SA},
}

% A survey of Pareto front visualization techniques in multiobjective optimization
@article{tuvsar2015visualization,
	author = {Tu\vsar, Tea and Filipi\vc, Bogdan},
	title = {Visualization of {P}areto Front Approximations in Evolutionary Multiobjective Optimization: A Critical Review and the Prosection Method},
	year = {2015},
	month = {4},
	journal = {IEEE Transactions on Evolutionary Computation},
	volume = {19},
	number = {2},
	pages = {225--245},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TEVC.2014.2313407},
	url = {https://ieeexplore.ieee.org/document/6777535},
	issn = {1089-778X},
	keywords = {optimization, multiobjective optimization},
}

% A robust RBF surrogate-based model, with adaptive scaling of the basis function radii to maintain numerical stability and a custom LHS sampling technique
@article{urquhart2020surrogatebased,
	author = {Urquhart, Magnus and Ljungskog, Emil and Sebben, Simone},
	title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
	year = {2020},
	month = {3},
	journal = {Applied Soft Computing},
	volume = {88},
	numpages = {106050},
	publisher = {Elsevier BV},
	doi = {10.1016/j.asoc.2019.106050},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494619308324},
	issn = {1568-4946},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% The (recently open source) numerical software library SLATEC from Sandia is something of a precursor to a modern library like scipy. SLATEC provides highly optimized, numerically stable, Fortran implementations for nearly every basic numerical algorithm that one would encounter in scientific computing
@article{vandevender1982slatec,
	author = {Vandevender, Walter H. and Haskell, Karen H.},
	title = {The {SLATEC} Mathematical Subroutine Library},
	year = {1982},
	month = {9},
	journal = {SIGNUM Newsletter},
	volume = {17},
	number = {3},
	pages = {16--21},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1057594.1057595},
	url = {https://dl.acm.org/doi/10.1145/1057594.1057595},
	issn = {0163-5778},
	keywords = {optimization, convex optimization, constrained optimization},
}

% SciPy official publication: Scipy is an open source numerical software package which is the standard for advanced numerical methods and scientific software packages in Python. Most of scipy are wrappers for much older Fortran or C++ code, that has been highly optimized.
@article{virtanen2020scipy,
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Jarrod Millman, K. and Mayorov, Nikolay and Nelson, Andrew R.~J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, CJ and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E.~A. and Harris, Charles R and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1.0},
	title = {{SciPy} 1.0: {F}undamental Algorithms for Scientific Computing in {P}ython},
	year = {2020},
	month = {3},
	journal = {Nature Methods},
	volume = {17},
	number = {3},
	pages = {261--272},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41592-019-0686-2},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	issn = {1548-7091},
	keywords = {optimization, linear programming, quadratic programming, LP, QP, convex optimization, constrained optimization, derivative-free optimization, DFO},
}

% The latest release of HOMPACK: An open source numerical software package written in Fortran 90 for solving nonlinear and polynomial systems of equations via homotopy methods.
@article{watson1997algorithm,
	author = {Watson, Layne T and Sosonkina, Maria and Melville, Robert C and Morgan, Alexander P and Walker, Homer F},
	title = {Algorithm 777: {HOMPACK90}: A suite of {Fortran} 90 codes for globally convergent homotopy algorithms},
	year = {1997},
	month = {12},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	volume = {23},
	number = {4},
	pages = {514--549},
	publisher = {ACM},
	doi = {10.1145/279232.279235},
	url = {https://dl.acm.org/doi/10.1145/279232.279235},
	issn = {0098-3500},
	keywords = {optimization, convex optimization, global optimization},
}

% How optimization is used to autotune the configuration of the BLAS subroutines and kernels for the ATLAS project -- since most numerical software relies on BLAS, it is often prudent to spend time optimizing BLAS configurations (such as matrix block sizes, etc.) to match machine specific values (such as cache sizes, etc.) when installing on a HPC that will have a high numerical (compute bound) workload. This can be done automatically via numerical optimization, so that users can just run the ATLAS setup scripts to configure BLAS automatically if they want an optimized installation
@article{whaley2001automated,
	author = {Whaley, R. Clint and Petitet, Antoine and Dongarra, Jack J.},
	title = {Automated empirical optimizations of software and the {ATLAS} project},
	year = {2001},
	month = {1},
	journal = {Parallel Computing},
	volume = {27},
	number = {1--2},
	pages = {3--35},
	publisher = {Elsevier BV},
	doi = {10.1016/s0167-8191(00)00087-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819100000879},
	issn = {0167-8191},
	keywords = {optimization, mixed-variable optimization, autotuning},
}

% The original publication of the reference point method for scalarizing multiobjective optimization problems (minimize the distance to a reference point)
@incollection{wierzbicki1999reference,
	author = {Wierzbicki, Andrzej P.},
	editor = {Gal, Tomas and Stewart, Theodor J. and Hanne, Thomas},
	title = {Reference Point Approaches},
	year = {1999},
	booktitle = {Multicriteria Decision Making: Advances in MCDM Models, Algorithms, Theory, and Applications},
	series = {International Series in Operations Research &amp; Management Science},
	pages = {237--275},
	publisher = {Springer US},
	address = {Boston, MA},
	doi = {10.1007/978-1-4615-5025-9_9},
	url = {http://link.springer.com/10.1007/978-1-4615-5025-9_9},
	isbn = {9781461372837},
	issn = {0884-8289},
	keywords = {optimization, multiobjective optimization, scalarization},
}

% ORBIT: the original algorithm for solving optimization problems via sequentially minimizing RBF interpolants with a linear tail inside a sequence of trust-regions. I can't remember if it's open source, but Stefan has a high quality numerical software in MATLAB
@article{wild2008orbit,
	author = {Wild, Stefan M. and Regis, Rommel G. and Shoemaker, Christine A.},
	title = {{ORBIT:} {O}ptimization by Radial Basis Function Interpolation in Trust-Regions},
	year = {2008},
	month = {1},
	journal = {SIAM Journal on Scientific Computing},
	volume = {30},
	number = {6},
	pages = {3197--3219},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/070691814},
	url = {http://epubs.siam.org/doi/10.1137/070691814},
	issn = {1064-8275},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% Analysis of the convergence rate of RBF-based surrogates with linear tail (fully linear model) inside a sequence of trust regions. This is the theory used in ORBIT
@article{wild2011global,
	author = {Wild, Stefan M. and Shoemaker, Christine A.},
	title = {Global Convergence of Radial Basis Function Trust Region Derivative-Free Algorithms},
	year = {2011},
	month = {7},
	journal = {SIAM Journal on Optimization},
	volume = {21},
	number = {3},
	pages = {761--781},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/09074927X},
	url = {http://epubs.siam.org/doi/10.1137/09074927X},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling, RBFs},
}

% The POUNDERS composite blackbox / simulation optimization algorithm, which is an open source numerical software package for exploiting the sum-of-squares structure in derivative-free least squares problems. Specifically, POUNDERS models the blackbox function / simulation's outputs using a fully linear model then uses the sum-of-squares structure to get a free Hessian approximation, and achieve second-order convergence for the price of first-order convergence. Although not included, open source numerical software implementations are now available in Python and Matlab through the PyOptUs GitHub group
@incollection{wild2017solving,
	author = {Wild, Stefan M.},
	editor = {Terlaky, Tamas and Anjos, Miguel F. and Ahmed, Shabbir},
	title = {Solving Derivative-Free Nonlinear Least Squares Problems with {POUNDERS}},
	year = {2017},
	month = {4},
	booktitle = {Advances and Trends in Optimization with Engineering Applications},
	pages = {529--540},
	publisher = {SIAM},
	doi = {10.1137/1.9781611974683.ch40},
	url = {http://www.mcs.anl.gov/papers/P5120-0414.pdf},
	isbn = {978-1-611974-67-6},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% A hypervolume-based approach to a multiobjective DIRECT algorithm for multiobjective blackbox / simulation optimization
@inproceedings{wong2016hypervolumebased,
	author = {Wong, Cheryl Sze Yin and Al-Dujaili, Abdullah and Sundaram, Suresh},
	title = {Hypervolume-Based {DIRECT} for Multi-Objective Optimisation},
	year = {2016},
	month = {7},
	booktitle = {Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
	pages = {1201--1208},
	organization = {ACM},
	location = {Denver, CO, USA},
	doi = {10.1145/2908961.2931702},
	url = {https://dl.acm.org/doi/10.1145/2908961.2931702},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, DIRECT, global optimization, scalarization},
}

% Official publication for ytopt: Argonne's HPC and scientific library autotuning software using Bayesian optimization to portably autotune numerical libraries for optimal performance on a given HPC as part of the exasale computing project. The software uses a random forest surrogate model and calculates their model-form uncertainties through resampling models. Then, they use random sampling of their acquisition function to perform Bayesian optimization at scale with fully distributed evaluation of the selected configurations. The results scale well on the HPCs Theta and Summit. The open source Python software is available at github.com/ytopt-team/ytopt
@article{wu2025ytopt,
	author = {Wu, Xingfu and Balaprakash, Prasanna and Kruse, Michael and Koo, Jaehoon and Videau, Brice and Hovland, Paul and Taylor, Valerie and Geltz, Brad and Jana, Siddhartha and Hall, Mary},
	title = {ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales},
	year = {2025},
	month = {1},
	journal = {Concurrency and Computation: Practice and Experience},
	volume = {37},
	number = {1},
	articleno = {e8322},
	publisher = {Wiley},
	doi = {10.1002/cpe.8322},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.8322},
	issn = {1532-0626},
	keywords = {optimization, decision trees, Bayesian optimization, global optimization, surrogate modeling, autotuning},
}

% A survey of interactive techniques and software in multiobjective optimization including the known challenges limitations
@article{xin2018interactive,
	author = {Xin, Bin and Chen, Lu and Chen, Jie and Ishibuchi, Hisao and Hirota, Kaoru and Liu, Bo},
	title = {Interactive Multiobjective Optimization: A Review of the State-of-the-Art},
	year = {2018},
	journal = {IEEE Access},
	volume = {6},
	pages = {41256--41279},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/ACCESS.2018.2856832},
	url = {https://ieeexplore.ieee.org/document/8412189},
	issn = {2169-3536},
	keywords = {optimization, multiobjective optimization},
}

% Multiobjective Bayesian optimization with differentiable hypervolume improvement-based acquisition
@article{yang2019multiobjective,
	author = {Yang, Kaifeng and Emmerich, Michael and Deutz, Andr{\'{e}} and B\"{a}ck, Thomas},
	title = {Multi-Objective {Bayesian} Global Optimization Using Expected {Hypervolume} Improvement Gradient},
	year = {2019},
	month = {2},
	journal = {Swarm and Evolutionary Computation},
	volume = {44},
	pages = {945--956},
	publisher = {Elsevier BV},
	doi = {10.1016/j.swevo.2018.10.007},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650217307861},
	issn = {2210-6502},
	keywords = {optimization, multiobjective optimization, Bayesian optimization, global optimization, surrogate modeling, scalarization, evolutionary algorithms, EA},
}

% Parallel multiobjective Bayesian optimization with differentiable hypervolume improvement-based acquisition
@inproceedings{yang2019multipoint,
	author = {Yang, Kaifeng and Palar, Pramudita Satria and Emmerich, Michael and Shimoyama, Koji and B\"{a}ck, Thomas},
	title = {A multi-point mechanism of expected hypervolume improvement for parallel multi-objective {Bayesian} global optimization},
	year = {2019},
	month = {7},
	booktitle = {Proc. Genetic and Evolutionary Computation Conference (GECCO19)},
	organization = {ACM},
	location = {Prague Czech Republic},
	doi = {10.1145/3321707.3321784},
	url = {https://dl.acm.org/doi/10.1145/3321707.3321784},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization, surrogate modeling, scalarization, evolutionary algorithms, EA},
}

% Solving DFT model calibrations for chemical design via active learning
@article{yuan2023active,
	author = {Yuan, Xiaoze and Zhou, Yuwei and Peng, Qing and Yang, Yong and Li, Yongwang and Wen, Xiaodong},
	title = {Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials},
	year = {2023},
	month = {1},
	journal = {Nature Computational Materials},
	volume = {9},
	number = {1},
	numpages = {12},
	publisher = {Nature Publishing Group UK London},
	doi = {10.1038/s41524-023-00967-z},
	url = {https://www.nature.com/articles/s41524-023-00967-z},
	issn = {2057-3960},
	keywords = {optimization, blackbox optimization, simulation optimization, Bayesian optimization, global optimization},
}

% PhD thesis on extracting high-dimensional (many objective) Pareto fronts including a thorough survey of such algorithms
@phdthesis{yukish2004algorithms,
	author = {Yukish, Michael},
	title = {Algorithms to identify {P}areto points in multi-dimensional data sets},
	year = {2004},
	school = {The Pennsylvania State University, Dept. of Mechanical Engineering},
	url = {https://etda.libraries.psu.edu/catalog/6336},
	keywords = {optimization, multiobjective optimization, high dimension},
}

% An algorithm for solving composite sum-of-squares blackbox / simulation optimization problems by modeling the blackbox function / simulation's outputs using a fully linear model then using the sum-of-squares structure to get a free Hessian approximation
@article{zhang2010derivativefree,
	author = {Zhang, Hongchao and Conn, Andrew R. and Scheinberg, Katya},
	title = {A Derivative-Free Algorithm for Least-Squares Minimization},
	year = {2010},
	month = {1},
	journal = {SIAM Journal on Optimization},
	volume = {20},
	number = {6},
	pages = {3555--3576},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/09075531X},
	url = {http://epubs.siam.org/doi/10.1137/09075531X},
	issn = {1052-6234},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Convergence analysis when solving composite sum-of-squares blackbox / simulation optimization problems by modeling the blackbox function / simulation's outputs using a fully linear model then using the sum-of-squares structure to get a free Hessian approximation. Basically, one can achieve second-order convergence for the price of first-order convergence
@article{zhang2012local,
	author = {Zhang, Hongchao and Conn, Andrew R.},
	title = {On the Local Convergence of a Derivative-Free Algorithm for Least-Squares Minimization},
	year = {2012},
	month = {3},
	journal = {Computational Optimization and Applications},
	volume = {51},
	number = {2},
	pages = {481--507},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s10589-010-9367-x},
	url = {http://link.springer.com/10.1007/s10589-010-9367-x},
	issn = {0926-6003},
	keywords = {optimization, blackbox optimization, simulation optimization, derivative-free optimization, DFO, surrogate modeling},
}

% Prima: open source reference implementation of all of Powell's numerical optimization solvers for blackbox / simulation optimization problems in modern Fortran. IMO, these should be considered the state-of-the-art and reference implementations for all blackbox optimization research
@misc{zhang2023prima,
	author = {Zhang, Zaikun},
	title = {{PRIMA}: Reference Implementation for {Powell}'s Methods with Modernization and Amelioration},
	year = {2023},
	howpublished = {github repository},
	doi = {10.5281/zenodo.8052654},
	url = {http://www.libprima.net},
	note = {Last accessed: Apr 2025},
	keywords = {optimization, blackbox optimization, derivative-free optimization, DFO, constrained optimization},
}

% An application for multiobjective optimization in the context of aircraft wing design. We are optimizing one objective that is the lift/drag ratio, and another that describes the controllability. Problem is solved using multiobjective particle swarm
@inproceedings{zhao2018multiobjective,
	author = {Zhao, Wei and Kapania, Rakesh K.},
	title = {Multiobjective Optimization of Composite Flying-wings with {SpaRibs} and Multiple Control Surfaces},
	year = {2018},
	month = {6},
	booktitle = {Proc. 2018 Multidisciplinary Analysis and Optimization Conference},
	numpages = {3424},
	organization = {AIAA},
	location = {Atlanta, GA, USA},
	doi = {10.2514/6.2018-3424},
	url = {https://arc.aiaa.org/doi/10.2514/6.2018-3424},
	keywords = {optimization, multiobjective optimization, blackbox optimization, simulation optimization},
}

% The original publication for L-BFGS-B software, which solves bound-constrained optimization problems using a limited-memory BFGS. This is the standard implementation that is used in all L-BFGS-B codes to date, such as scipy, all machine learning codes, and most engineering codes and nonlinear systems solvers. The code is open source high-quality numerical software, written in old-style Fortran
@article{zhu1997algorithm,
	author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
	title = {Algorithm 778: {L-BFGS-B}: {F}ortran Subroutines for Large-Scale Bound-Constrained Optimization},
	year = {1997},
	month = {12},
	journal = {ACM Transactions on Mathematical Software},
	volume = {23},
	number = {4},
	pages = {550--560},
	publisher = {ACM},
	doi = {10.1145/279232.279236},
	url = {https://dl.acm.org/doi/10.1145/279232.279236},
	issn = {0098-3500},
	keywords = {optimization, constrained optimization, convex optimization},
}

% SPEA2 strength Pareto evolutionary algorithm -- an old evolutionary algorithm that was once a competitor to NSGA-II (and with significant overlap in co-authorship), but is now largely obsolete.
@article{zitzler2001spea2,
	author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
	title = {{SPEA2}: Improving the strength {Pareto} evolutionary algorithm},
	year = {2001},
	journal = {TIK-report},
	volume = {103},
	publisher = {Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH Zurich), Institut f{\"u}r Technische},
	doi = {10.3929/ethz-a-004284029},
	git = {https://github.com/manuparra/spea2},
	keywords = {optimization, multiobjective optimization, evolutionary algorithms, EA},
}


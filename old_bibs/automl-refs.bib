@article{pugliese2021machinelearning,
  title    = {Machine learning-based approach: global trends, research directions, and regulatory standpoints},
  journal  = {Data Science and Management},
  volume   = {4},
  pages    = {19-29},
  year     = {2021},
  issn     = {2666-7649},
  doi      = {https://doi.org/10.1016/j.dsm.2021.12.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666764921000485},
  author   = {Raffaele Pugliese and Stefano Regondi and Riccardo Marini},
  keywords = {Machine learning, Artificial intelligence, Research trends, Healthcare, Data governance, Nanotechnology, Cybersecurity},
  abstract = {The field of machine learning (ML) is sufficiently young that it is still expanding at an accelerating pace, lying at the crossroads of computer science and statistics, and at the core of artificial intelligence (AI) and data science. Recent progress in ML has been driven both by the development of new learning algorithms theory, and by the ongoing explosion in the availability of vast amount of data (often referred to as "big data") and low-cost computation. The adoption of ML-based approaches can be found throughout science, technology and industry, leading to more evidence-based decision-making across many walks of life, including healthcare, biomedicine, manufacturing, education, financial modeling, data governance, policing, and marketing. Although the past decade has witnessed the increasing interest in these fields, we are just beginning to tap the potential of these ML algorithms for studying systems that improve with experience. In this paper, we present a comprehensive view on geo worldwide trends (taking into account China, the USA, Israel, Italy, the UK, and the Middle East) of ML-based approaches highlighting the rapid growth in the last 5 years attributable to the introduction of related national policies. Furthermore, based on the literature review, we also discuss the potential research directions in this field, summarizing some popular application areas of machine learning technology, such as healthcare, cyber-security systems, sustainable agriculture, data governance, and nanotechnology, and suggest that the "dissemination of research" in the ML scientific community has undergone the exceptional growth in the time range of 2018–2020, reaching a value of 16,339 publications. Finally, we report the challenges and the regulatory standpoints for managing ML technology. Overall, we hope that this work will help to explain the geo trends of ML approaches and their applicability in various real-world domains, as well as serve as a reference point for both academia and industry professionals, particularly from a technical, ethical and regulatory point of view.}
}

@inproceedings{hutter2011smac,
  title     = {Sequential Model-Based Optimization for General Algorithm Configuration},
  author    = {Frank Hutter and Holger H. Hoos and Kevin Leyton-Brown},
  booktitle = {Learning and Intelligent Optimization},
  year      = {2011},
  url       = {https://api.semanticscholar.org/CorpusID:6944647}
}

@article{vapnik1991principles,
  title   = {Principles of risk minimization for learning theory},
  author  = {Vapnik, Vladimir},
  journal = {Advances in neural information processing systems},
  volume  = {4},
  year    = {1991}
}

@article{jones1998efficient,
  title     = {Efficient global optimization of expensive black-box functions},
  author    = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal   = {Journal of Global optimization},
  volume    = {13},
  pages     = {455--492},
  year      = {1998},
  publisher = {Springer}
}

@article{elsken2019neural,
  title     = {Neural architecture search: A survey},
  author    = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal   = {The Journal of Machine Learning Research},
  volume    = {20},
  number    = {1},
  pages     = {1997--2017},
  year      = {2019},
  publisher = {JMLR.org}
}

@inproceedings{liu2018darts,
  title     = {{DARTS}: Differentiable Architecture Search},
  author    = {Hanxiao Liu and Karen Simonyan and Yiming Yang},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=S1eYHoC5FX}
}

@inproceedings{chu2021darts,
  title     = {{\{}DARTS{\}}-: Robustly Stepping out of Performance Collapse Without Indicators},
  author    = {Xiangxiang Chu and Xiaoxing Wang and Bo Zhang and Shun Lu and Xiaolin Wei and Junchi Yan},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=KLH36ELmwIB}
}

@article{yang2020hyperparameter,
  title     = {On hyperparameter optimization of machine learning algorithms: Theory and practice},
  author    = {Yang, Li and Shami, Abdallah},
  journal   = {Neurocomputing},
  volume    = {415},
  pages     = {295--316},
  year      = {2020},
  publisher = {Elsevier}
}

@inproceedings{pham2018efficient,
  title        = {Efficient neural architecture search via parameters sharing},
  author       = {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle    = {International conference on machine learning},
  pages        = {4095--4104},
  year         = {2018},
  organization = {PMLR}
}

@inproceedings{guo2020single,
  title        = {Single path one-shot neural architecture search with uniform sampling},
  author       = {Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle    = {Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages        = {544--560},
  year         = {2020},
  organization = {Springer}
}

@article{li2017hyperband,
  title     = {Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author    = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal   = {The journal of machine learning research},
  volume    = {18},
  number    = {1},
  pages     = {6765--6816},
  year      = {2017},
  publisher = {JMLR.org}
}

@inproceedings{jamieson2016non,
  title        = {Non-stochastic best arm identification and hyperparameter optimization},
  author       = {Jamieson, Kevin and Talwalkar, Ameet},
  booktitle    = {Artificial intelligence and statistics},
  pages        = {240--248},
  year         = {2016},
  organization = {PMLR}
}

@inproceedings{cox1992lcb,
  author    = {Cox, D.D. and John, S.},
  booktitle = {[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics},
  title     = {A statistical method for global optimization},
  year      = {1992},
  volume    = {},
  number    = {},
  pages     = {1241-1246 vol.2},
  abstract  = {An algorithm for finding global optima using statistical prediction is presented. Assuming a random function model, lower confidence bounds on predicted values are used for sequential selection of evaluation points and as a convergence criterion. Comparison with published results for several test functions indicates that the procedure is very efficient in finding the global optimum of a multimodal function, and in terminating with relatively few evaluations.<>},
  keywords  = {},
  doi       = {10.1109/ICSMC.1992.271617},
  issn      = {},
  month     = {Oct}
}

@article{kushner1964pi,
  author   = {Kushner, H. J.},
  title    = {{A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise}},
  journal  = {Journal of Basic Engineering},
  volume   = {86},
  number   = {1},
  pages    = {97-106},
  year     = {1964},
  month    = {03},
  abstract = {{A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.}},
  issn     = {0021-9223},
  doi      = {10.1115/1.3653121},
  url      = {https://doi.org/10.1115/1.3653121},
  eprint   = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/86/1/97/5763745/97\_1.pdf}
}



@article{hennig2012entropy,
  title   = {Entropy Search for Information-Efficient Global Optimization.},
  author  = {Hennig, Philipp and Schuler, Christian J},
  journal = {Journal of Machine Learning Research},
  volume  = {13},
  number  = {6},
  year    = {2012}
}

@article{hernandez2014predictive,
  title   = {Predictive entropy search for efficient global optimization of black-box functions},
  author  = {Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Hoffman, Matthew W and Ghahramani, Zoubin},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014}
}

@inproceedings{wang2017max,
  title        = {Max-value entropy search for efficient Bayesian optimization},
  author       = {Wang, Zi and Jegelka, Stefanie},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3627--3635},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{srinivas2010ucb,
  author    = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  title     = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
  year      = {2010},
  isbn      = {9781605589077},
  publisher = {Omnipress},
  address   = {Madison, WI, USA},
  abstract  = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages     = {1015–1022},
  numpages  = {8},
  location  = {Haifa, Israel},
  series    = {ICML'10}
}

@phdthesis{lakshminarayanan2016phd,
  title  = {Decision trees and forests: a probabilistic perspective},
  author = {Lakshminarayanan, Balaji},
  year   = {2016},
  school = {UCL (University College London)}
}

@inproceedings{lakshminarayanan2016mondrian,
  title        = {Mondrian forests for large-scale regression when uncertainty matters},
  author       = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
  booktitle    = {Artificial Intelligence and Statistics},
  pages        = {1478--1487},
  year         = {2016},
  organization = {PMLR}
}

@article{lakshminarayanan2014mondrian,
  title   = {Mondrian forests: Efficient online random forests},
  author  = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
  journal = {Advances in neural information processing systems},
  volume  = {27},
  year    = {2014}
}


@inproceedings{nasbench101,
  title     = {{NAS}-Bench-101: Towards Reproducible Neural Architecture Search},
  author    = {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {7105--7114},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/ying19a/ying19a.pdf},
  url       = {https://proceedings.mlr.press/v97/ying19a.html},
  abstract  = {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.}
}

@article{naschecklist_lindauer_2020,
  author     = {Lindauer, Marius and Hutter, Frank},
  title      = {Best Practices for Scientific Research on Neural Architecture Search},
  year       = {2020},
  issue_date = {January 2020},
  publisher  = {JMLR.org},
  volume     = {21},
  number     = {1},
  issn       = {1532-4435},
  abstract   = {Finding a well-performing architecture is often tedious for both deep learning practitioners and researchers, leading to tremendous interest in the automation of this task by means of neural architecture search (NAS). Although the community has made major strides in developing better NAS methods, the quality of scientific empirical evaluations in the young field of NAS is still lacking behind that of other areas of machine learning. To address this issue, we describe a set of possible issues and ways to avoid them, leading to the NAS best practices checklist available at http://automl.org/nas_checklist.pdf.},
  journal    = {J. Mach. Learn. Res.},
  month      = {jan},
  articleno  = {243},
  numpages   = {18},
  keywords   = {neural architecture search, empirical evaluation, scientific best practices}
}

@inproceedings{lakshminarayanan_deepensemble_2017,
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  title     = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6405–6416},
  numpages  = {12},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@article{hutter2014algorithm,
  title     = {Algorithm runtime prediction: Methods and evaluation},
  journal   = {Artificial Intelligence},
  volume    = {206},
  pages     = {79-111},
  year      = {2014},
  issn      = {0004-3702},
  doi       = {https://doi.org/10.1016/j.artint.2013.10.003},
  url       = {https://www.sciencedirect.com/science/article/pii/S0004370213001082},
  author    = {Frank Hutter and Lin Xu and Holger H. Hoos and Kevin Leyton-Brown},
  keywords  = {Supervised machine learning, Performance prediction, Empirical performance models, Response surface models, Highly parameterized algorithms, Propositional satisfiability, Mixed integer programming, Travelling salesperson problem},
  abstract  = {Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithmʼs runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and—perhaps most importantly—a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.},
  publisher = {Elsevier}
}

@article{geurts2006extremely,
  title     = {Extremely randomized trees},
  author    = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  journal   = {Machine learning},
  volume    = {63},
  pages     = {3--42},
  year      = {2006},
  publisher = {Springer}
}

@article{breiman1996bagging,
  title     = {Bagging predictors},
  author    = {Breiman, Leo},
  journal   = {Machine learning},
  volume    = {24},
  pages     = {123--140},
  year      = {1996},
  publisher = {Springer}
}

@book{james2013introduction,
  title     = {An introduction to statistical learning},
  author    = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and others},
  volume    = {112},
  year      = {2013},
  publisher = {Springer}
}

@article{breiman2001random,
  title     = {Random forests},
  author    = {Breiman, Leo},
  journal   = {Machine learning},
  volume    = {45},
  pages     = {5--32},
  year      = {2001},
  publisher = {Springer}
}

@inproceedings{domingos2000unified,
  title        = {A unified bias-variance decomposition},
  author       = {Domingos, Pedro},
  booktitle    = {Proceedings of 17th international conference on machine learning},
  pages        = {231--238},
  year         = {2000},
  organization = {Morgan Kaufmann Stanford}
}

@article{taylor2007scoringrule,
  author    = {Tilmann Gneiting and Adrian E Raftery},
  title     = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  journal   = {Journal of the American Statistical Association},
  volume    = {102},
  number    = {477},
  pages     = {359-378},
  year      = {2007},
  publisher = {Taylor & Francis},
  doi       = {10.1198/016214506000001437},
  url       = {https://doi.org/10.1198/016214506000001437},
  eprint    = {https://doi.org/10.1198/016214506000001437}
}

@article{ruhkopf2022masif,
  title   = {MASIF: Meta-learned Algorithm Selection using Implicit Fidelity Information},
  author  = {Ruhkopf, Tim and Mohan, Aditya and Deng, Difan and Tornede, Alexander and Hutter, Frank and Lindauer, Marius},
  journal = {Transactions on Machine Learning Research},
  year    = {2022}
}

@article{adriaensen2023efficient,
  title   = {Efficient bayesian learning curve extrapolation using prior-data fitted networks},
  author  = {Adriaensen, Steven and Rakotoarison, Herilalaina and M{\"u}ller, Samuel and Hutter, Frank},
  journal = {arXiv preprint arXiv:2310.20447},
  year    = {2023}
}

@article{hollmann2022tabpfn,
  title   = {Tabpfn: A transformer that solves small tabular classification problems in a second},
  author  = {Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal = {arXiv preprint arXiv:2207.01848},
  year    = {2022}
}

@article{el2017prediction,
  title   = {The Prediction Advantage: A Universally Meaningful Performance Measure for Classification and Regression},
  author  = {El-Yaniv, Ran and Geifman, Yonatan and Wiener, Yair},
  journal = {arXiv preprint arXiv:1705.08499},
  year    = {2017}
}

@article{viering2022shape,
  title     = {The shape of learning curves: a review},
  author    = {Viering, Tom and Loog, Marco},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2022},
  publisher = {IEEE}
}

@article{muller2021transformers,
  title   = {Transformers can do bayesian inference},
  author  = {M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal = {arXiv preprint arXiv:2112.10510},
  year    = {2021}
}

@inproceedings{langley00,
  author    = {P. Langley},
  title     = {Crafting Papers on Machine Learning},
  year      = {2000},
  pages     = {1207--1216},
  editor    = {Pat Langley},
  booktitle = {Proceedings of the 17th International Conference
               on Machine Learning (ICML 2000)},
  address   = {Stanford, CA},
  publisher = {Morgan Kaufmann}
}

@techreport{mitchell80,
  author      = {T. M. Mitchell},
  title       = {The Need for Biases in Learning Generalizations},
  institution = {Rutgers University, Computer Science Department},
  year        = {1980},
  address     = {New Brunswick, MA}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title  = {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year   = {1989}
}

@book{MachineLearningI,
  editor    = {R. S. Michalski and J. G. Carbonell and T.
               M. Mitchell},
  title     = {Machine Learning: An Artificial Intelligence
               Approach, Vol. I},
  publisher = {Tioga},
  year      = {1983},
  address   = {Palo Alto, CA}
}

@book{DudaHart2nd,
  author    = {R. O. Duda and P. E. Hart and D. G. Stork},
  title     = {Pattern Classification},
  publisher = {John Wiley and Sons},
  edition   = {2nd},
  year      = {2000}
}

@incollection{Newell81,
  author    = {A. Newell and P. S. Rosenbloom},
  title     = {Mechanisms of Skill Acquisition and the Law of
               Practice},
  booktitle = {Cognitive Skills and Their Acquisition},
  pages     = {1--51},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  year      = {1981},
  editor    = {J. R. Anderson},
  chapter   = {1},
  address   = {Hillsdale, NJ}
}


@article{Samuel59,
  author  = {A. L. Samuel},
  title   = {Some Studies in Machine Learning Using the Game of
             Checkers},
  journal = {IBM Journal of Research and Development},
  year    = {1959},
  volume  = {3},
  number  = {3},
  pages   = {211--229}
}

@inproceedings{10.5555/645940.671380,
  author    = {B. Gu et al.},
  title     = {Modelling Classification Performance for Large Data Sets},
  year      = {2001},
  isbn      = {3540422986},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  abstract  = {For many learning algorithms, their learning accuracy will increase as the size of training data increases, forming the well-known learning curve. Usually a learning curve can be fitted by interpolating or extrapolating some points on it with a specified model. The obtained learning curve can then be used to predict the maximum achievable learning accuracy or to estimate the amount of data needed to achieve an expected learning accuracy, both of which will be especially meaningful to data mining on large data sets. Although some models have been proposed to model learning curves, most of them do not test their applicability to large data sets. In this paper, we focus on this issue. We empirically compare six potentially useful models by fitting learning curves of two typical classification algorithms--C4.5 (decision tree) and LOG (logistic discrimination) on eight large UCI benchmark data sets. By using all available data for learning, we fit a full-length learning curve; by using a small portion of the data, we fit a part-length learning curve. The models are then compared in terms of two performances: (1) how well they fit a full-length learning curve, and (2) how well a fitted part-length learning curve can predict learning accuracy at the full length. Experimental results show that the power law (y = a - b * x-c) is the best among the six models in both the performances for the two algorithms and all the data sets. These results support the applicability of learning curves to data mining.},
  booktitle = {Proceedings of the Second International Conference on Advances in Web-Age Information Management},
  pages     = {317–328},
  numpages  = {12},
  series    = {WAIM '01}
}

@book{ghosh2006introduction,
  title     = {An introduction to Bayesian analysis: theory and methods},
  author    = {JK. Ghosh et al.},
  volume    = {},
  year      = {},
  publisher = {}
}

@software{jax2018github,
  author  = {J. Bradbury et al.},
  title   = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url     = {http://github.com/google/jax},
  version = {0.3.13},
  year    = {2018}
}

@techreport{gavin2019levenberg,
  title   = {The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
  author  = {HP. Gavin},
  year    = {2019},
  institution = {Duke University, Department of Civil and Environmental Engineering},
}

@inproceedings{wu2018understanding,
  title     = {Understanding Short-Horizon Bias in Stochastic Meta-Optimization},
  author    = {Y. Wu et al.},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  url       = {https://openreview.net/forum?id=H1MczcgR-}
}

@inproceedings{bansal2022jahsbench,
  title     = {JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search},
  author    = {A. Bansal et al.},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2022}
}

@inproceedings{bohdal2023pasha,
  title     = {PASHA: Efficient HPO and NAS with Progressive Resource Allocation},
  author    = {Bohdal, Ondrej and Balles, Lukas and Wistuba, Martin and Ermis, Beyza and Archambeau, Cedric and Zappella, Giovanni},
  booktitle = {ICLR},
  year      = {2023}
}

@inproceedings{pfisterer2022yahpo,
  title        = {Yahpo gym-an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization},
  author       = {F. Pfisterer et al.},
  booktitle    = {International Conference on Automated Machine Learning},
  pages        = {3--1},
  year         = {2022},
  organization = {PMLR}
}

@article{yan2021bench,
  title   = {Nas-bench-x11 and the power of learning curves},
  author  = {Y. Shen et al.},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {22534--22549},
  year    = {2021}
}

@article{mohr2023fast,
  title     = {Fast and informative model selection using learning curve cross-validation},
  author    = {F. Mohr et al.},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2023},
  publisher = {IEEE}
}

@inproceedings{mohr2022lcdb,
  title        = {LCDB 1.0: An extensive learning curves database for classification tasks},
  author       = {Mohr, Felix and Viering, Tom J and Loog, Marco and van Rijn, Jan N},
  booktitle    = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages        = {3--19},
  year         = {2022},
  organization = {Springer}
}

@article{hutter_learning_2021,
  title   = {Learning curve theory},
  author  = {M. Hutter},
  journal = {arXiv preprint arXiv:2102.04074},
  year    = {2021}
}


@article{li2020system,
  title   = {A system for massively parallel hyperparameter tuning},
  author  = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-Tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  journal = {Proceedings of Machine Learning and Systems},
  volume  = {2},
  pages   = {230--246},
  year    = {2020}
}

@inproceedings{domhan2015speeding,
  title     = {Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author    = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle = {Twenty-fourth international joint conference on artificial intelligence},
  year      = {2015}
}

@article{baker_accelerating_2017,
  title   = {Accelerating neural architecture search using performance prediction},
  author  = {B. Baker et al.},
  journal = {arXiv preprint arXiv:1705.10823},
  year    = {2017}
}

@inproceedings{klein2016learning,
  title     = {Learning curve prediction with Bayesian neural networks},
  author    = {Klein, Aaron and Falkner, Stefan and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle = {International conference on learning representations},
  year      = {2016}
}

@article{zimmer2021auto,
  title     = {Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl},
  author    = {L. Zimmer et al.},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {43},
  number    = {9},
  pages     = {3079--3090},
  year      = {2021},
  publisher = {IEEE}
}

@article{eggensperger_hpobench_2021,
  title   = {HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO},
  author  = {K. Eggensperger et al.},
  journal = {arXiv preprint arXiv:2109.06716},
  year    = {2021}
}

@article{white_how_2021,
  title   = {How powerful are performance predictors in neural architecture search?},
  author  = {C. White et al.},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {28454--28469},
  year    = {2021}
}

@inproceedings{wu2020practical,
  title        = {Practical multi-fidelity Bayesian optimization for hyperparameter tuning},
  author       = {J. Wu et al.},
  booktitle    = {Uncertainty in Artificial Intelligence},
  pages        = {788--798},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{klein2017fast,
  title        = {Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author       = {A. Klein et al.},
  booktitle    = {Artificial intelligence and statistics},
  pages        = {528--536},
  year         = {2017},
  organization = {PMLR}
}

@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {F. Pedregosa et al.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

% Audet's guide to assessing performance of multiobjective optimization algorithms
@article{audet2021performance,
  title     = {Performance indicators in multiobjective optimization},
  author    = {Audet, Charles and Bigeon, Jean and Cartier, Dominique and Le Digabel, S{\'e}bastien and Salomon, Ludovic},
  journal   = {European journal of operational research},
  volume    = {292},
  number    = {2},
  pages     = {397--422},
  year      = {2021},
  publisher = {Elsevier}
}

@article{yu2020hyper,
  title   = {Hyper-parameter optimization: A review of algorithms and applications},
  author  = {Yu, Tong and Zhu, Hong},
  journal = {arXiv preprint arXiv:2003.05689},
  year    = {2020}
}

@article{li2018hyperband,
  title   = {Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author  = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal = {Journal of Machine Learning Research},
  volume  = {18},
  number  = {185},
  pages   = {1--52},
  year    = {2018}
}

@inproceedings{falkner2018bohb,
  title        = {BOHB: Robust and efficient hyperparameter optimization at scale},
  author       = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle    = {International conference on machine learning},
  pages        = {1437--1446},
  year         = {2018},
  organization = {PMLR}
}

@inproceedings{awad2021dehb,
  author    = {N. Awad and N. Mallik and F. Hutter},
  title     = {{DEHB}: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization},
  pages     = {2147--2153},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {ijcai.org},
  editor    = {Z. Zhou},
  year      = {2021}
}

@article{mohr2023lccv,
  title     = {Fast and informative model selection using learning curve cross-validation},
  author    = {Mohr, Felix and van Rijn, Jan N},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2023},
  publisher = {IEEE}
}
@article{mohr2022lcsurvey,
  author     = {Felix Mohr and
                Jan N. van Rijn},
  title      = {Learning Curves for Decision Making in Supervised Machine Learning
                - {A} Survey},
  journal    = {CoRR},
  volume     = {abs/2201.12150},
  year       = {2022},
  url        = {https://arxiv.org/abs/2201.12150},
  eprinttype = {arXiv},
  eprint     = {2201.12150},
  timestamp  = {Wed, 02 Feb 2022 15:00:01 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2201-12150.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{eggensperger2021hpobench,
  title     = {{HPOB}ench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for {HPO}},
  author    = {Katharina Eggensperger and Philipp M{\"u}ller and Neeratyoy Mallik and Matthias Feurer and Rene Sass and Aaron Klein and Noor Awad and Marius Lindauer and Frank Hutter},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year      = {2021},
  url       = {https://openreview.net/forum?id=1k4rJYEwda-}
}

@article{klein2019tabular,
  author     = {Aaron Klein and
                Frank Hutter},
  title      = {Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization},
  journal    = {CoRR},
  volume     = {abs/1905.04970},
  year       = {2019},
  url        = {http://arxiv.org/abs/1905.04970},
  eprinttype = {arXiv},
  eprint     = {1905.04970},
  timestamp  = {Tue, 28 May 2019 12:48:08 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-04970.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

% Optuna
@inproceedings{akiba2019optuna,
  title     = {Optuna: A next-generation hyperparameter optimization framework},
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle = {Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages     = {2623--2631},
  year      = {2019}
}

@misc{optunadocs,
  author       = {{Optuna developers}},
  year         = {2018},
  title        = {Optuna API: optuna.samplers},
  howpublished = {\url{https://optuna.readthedocs.io/en/stable/reference/samplers/index.html}},
  note         = {Accessed: Jun 27, 2023}
}

@article{audet2009,
  author  = {Audet, Charles and Dennis, J. E.},
  title   = {A Progressive Barrier for Derivative-Free Nonlinear Programming},
  journal = {SIAM Journal on Optimization},
  volume  = {20},
  number  = {1},
  pages   = {445-472},
  year    = {2009},
  doi     = {10.1137/070692662}
}

% BoTorch framework
@article{balandat2020botorch,
  title   = {BoTorch: A framework for efficient Monte-Carlo Bayesian optimization},
  author  = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {21524--21538},
  year    = {2020}
}

% Original DeepHyper pub
@inproceedings{balaprakash2018deephyper,
  title        = {DeepHyper: Asynchronous hyperparameter search for deep neural networks},
  author       = {Balaprakash, Prasanna and Salim, Michael and Uram, Thomas D. and Vishwanath, Venkat and Wild, Stefan M.},
  booktitle    = {IEEE 25th international conference on high performance computing (HiPC)},
  pages        = {42--51},
  year         = {2018},
  organization = {IEEE}
}

% JAHS-Bench-201 paper
@inproceedings{bansal2022jahs,
  title     = {{JAHS-Bench-201}: A Foundation For Research On Joint Architecture And Hyperparameter Search},
  author    = {Archit Bansal and Danny Stoll and Maciej Janowski and Arber Zela and Frank Hutter},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2022}
}

% pymoo library -- state-of-the-art implementations of NSGA-II, NSGA-III, and other variants
@article{blank2020pymoo,
  author  = {J. {Blank} and K. {Deb}},
  journal = {IEEE Access},
  title   = {pymoo: Multi-Objective Optimization in Python},
  year    = {2020},
  volume  = {8},
  pages   = {89497-89509},
  doi     = {10.1109/ACCESS.2020.2990567}
}

% ParMOO batch-parallel model-based MOO solver
@article{chang2023parmoo,
  title   = {{ParMOO}: A Python library for parallel multiobjective simulation optimization},
  author  = {Chang, Tyler H. and Wild, Stefan M.},
  journal = {Journal of Open Source Software},
  volume  = {8},
  number  = {82},
  pages   = {4468},
  year    = {2023}
}

% Multiobjective framework design paper, with list of relevant solvers and strategies for obtaining customized performance on real problems
@article{chang2023designing,
  title   = {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
  author  = {Chang, Tyler H and Wild, Stefan M},
  journal = {arXiv preprint arXiv:2304.06881},
  year    = {2023}
}

@phdthesis{chang2020mathematical,
  title  = {Mathematical Software for Multiobjective Optimization Problems},
  author = {Chang, Tyler Hunter},
  year   = {2020},
  school = {Virginia Tech}
}

% Bayesian optimization scalarization functions
@inproceedings{chugh2020scalarizing,
  title        = {Scalarizing functions in Bayesian multiobjective optimization},
  author       = {Chugh, Tinkle},
  booktitle    = {2020 IEEE Congress on Evolutionary Computation (CEC)},
  pages        = {1--8},
  year         = {2020},
  organization = {IEEE}
}

% MOO augmented Lagrangian
@article{cocchiandlapucci2020,
  title     = {An augmented Lagrangian algorithm for multi-objective optimization},
  author    = {Cocchi, G and Lapucci, M},
  journal   = {Computational Optimization and Applications},
  volume    = {77},
  number    = {1},
  pages     = {29--56},
  year      = {2020},
  publisher = {Springer}
}

% Quadratic scalarization
@article{dandurand2016quadratic,
  title     = {Quadratic scalarization for decomposed multiobjective optimization},
  author    = {Dandurand, Brian and Wiecek, Margaret M.},
  journal   = {OR spectrum},
  volume    = {38},
  number    = {4},
  pages     = {1071--1096},
  year      = {2016},
  publisher = {Springer}
}

% NBI method for adaptive scalarization
@article{das1998,
  author  = {Das, Indraneel and Dennis, John E.},
  year    = {1998},
  title   = {Normal-boundary intersection: A new method for generating the {P}areto surface in nonlinear multicriteria optimization problems},
  journal = {SIAM Journal on Optimization},
  volume  = {8},
  number  = {3},
  pages   = {631--657}
}

% qEHVI paper using MC to approximate hypevolume improvement with automatic derivative propogation
@article{daulton2020differentiable,
  title   = {Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization},
  author  = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {9851--9864},
  year    = {2020}
}

% DTLZ problems
@incollection{deb2005scalable,
  title     = {Scalable test problems for evolutionary multiobjective optimization},
  author    = {Deb, Kalyanmoy and Thiele, Lothar and Laumanns, Marco and Zitzler, Eckart},
  booktitle = {Evolutionary Multiobjective Optimization, Theoretical Advances and Applications},
  chapter   = {6},
  editors   = {Abraham, Jain, and Goldberg},
  year      = {2005},
  address   = {London, UK},
  publisher = {Springer}
}

% Original NSGA-II paper
@article{deb2002fast,
  title     = {A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author    = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal   = {IEEE transactions on evolutionary computation},
  volume    = {6},
  number    = {2},
  pages     = {182--197},
  year      = {2002},
  publisher = {IEEE}
}

@inproceedings{martens2013asynchronous,
  title     = {The asynchronous island model and NSGA-II: study of a new migration operator and its performance},
  author    = {M{\"a}rtens, Marcus and Izzo, Dario},
  booktitle = {Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages     = {1173--1180},
  year      = {2013}
}

% NAS-Bench-201 introduces the cell-based architecture representation used by JAHS-Bench-201
@inproceedings{dong2020nas,
  title     = {{NAS-Bench-201}: Extending the Scope of Reproducible Neural Architecture Search},
  author    = {Xuanyi Dong and Yi Yang},
  booktitle = {International Conference on Learning Representations (ICLR 2020)},
  year      = {2020},
  url       = {https://openreview.net/forum?id=HJxyZkBKDr}
}

% Ehrgott's classic -- the "bible" of MOO
@book{ehrgott2005multicriteria,
  title     = {Multicriteria optimization},
  author    = {Ehrgott, Matthias},
  volume    = {491},
  year      = {2005},
  publisher = {Springer Science \& Business Media}
}

% BoTorch NAS
@inproceedings{eriksson2021latencyaware,
  title     = {Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization},
  author    = {David Eriksson and Pierce I-Jen Chuang and Samuel Daulton and Peng Xia and Akshat Shrivastava and Arun Babu and Shicong Zhao and Ahmed A Aly and Ganesh Venkatesh and Maximilian Balandat},
  booktitle = {8th ICML Workshop on Automated Machine Learning (AutoML) },
  year      = {2021},
  url       = {https://openreview.net/forum?id=0ciyfd4SvbI}
}

% Scalable high-dimensional BO in BoTorch via Trust-Regions (TURBO)
@article{eriksson2019scalable,
  title   = {Scalable global optimization via local bayesian optimization},
  author  = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

% NSGA-III reference point methods
@article{deb2013evolutionary,
  title     = {An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part {I}: solving problems with box constraints},
  author    = {Deb, Kalyanmoy and Jain, Himanshu},
  journal   = {IEEE transactions on evolutionary computation},
  volume    = {18},
  number    = {4},
  pages     = {577--601},
  year      = {2013},
  publisher = {IEEE}
}

% Adaptive weighting scheme for MOO
@article{deshpande2016multiobjective,
  title     = {Multiobjective optimization using an adaptive weighting scheme},
  author    = {Deshpande, Shubhangi and Watson, Layne T and Canfield, Robert A},
  journal   = {Optimization Methods and Software},
  volume    = {31},
  number    = {1},
  pages     = {110--133},
  year      = {2016},
  publisher = {Taylor \& Francis}
}

% GD and IGD are Pareto non-compliant, IGD+ is
@inproceedings{ishibuchi2015modified,
  title        = {Modified distance calculation in generational distance and inverted generational distance},
  author       = {Ishibuchi, Hisao and Masuda, Hiroyuki and Tanigaki, Yuki and Nojima, Yusuke},
  booktitle    = {Evolutionary Multi-Criterion Optimization: 8th International Conference, EMO 2015, Guimar{\~a}es, Portugal, March 29--April 1, 2015. Proceedings, Part II 8},
  pages        = {110--125},
  year         = {2015},
  organization = {Springer}
}

% HyperNOMAD
@article{lakhmiri2021hypernomad,
  title     = {HyperNOMAD: Hyperparameter Optimization of Deep Neural Networks Using Mesh Adaptive Direct Search},
  author    = {Lakhmiri, Dounia and Digabel, S{\'e}bastien Le and Tribes, Christophe},
  journal   = {ACM Transactions on Mathematical Software (TOMS)},
  volume    = {47},
  number    = {3},
  pages     = {1--27},
  year      = {2021},
  publisher = {ACM New York, NY, USA}
}

% NOMAD is standard blackbox optimization solver in the operations research industry, often taught as part of the curriculum in universities around the world. They have a parallel bi-objective solver based on nonsmooth scalarization functions.
@article{ledigabel2011algorithm,
  title     = {Algorithm 909: {NOMAD}: Nonlinear optimization with the {MADS} algorithm},
  author    = {Le Digabel, S{\'e}bastien},
  journal   = {ACM Transactions on Mathematical Software (TOMS)},
  volume    = {37},
  number    = {4},
  pages     = {1--15},
  year      = {2011},
  publisher = {ACM New York, NY, USA}
}

@techreport{ledigabel2015,
  author      = {S\'ebastien {Le Digabel} and Stefan M. Wild},
  title       = {A Taxonomy of Constraints in Simulation-Based Optimization},
  institution = {Argonne National Laboratory, Mathematics and Computer Science Division},
  type        = {Preprint},
  number      = {ANL/MCS-P5350-0515},
  anumber     = {arXiv:1505.07881},
  year        = {2015},
  gurl        = {https://scholar.google.com/scholar?cluster=8245889514910760898},
  url         = {http://www.mcs.anl.gov/papers/P5350-0515.pdf}
}


% DARTS gradient-based one-shot NAS
@inproceedings{liu2019darts,
  title     = {{DARTS}: Differentiable Architecture Search},
  author    = {Hanxiao Liu and Karen Simonyan and Yiming Yang},
  booktitle = {Proc. 7th International Conference on Learning Representations (ICLR '19)},
  year      = {2019},
  url       = {https://openreview.net/forum?id=S1eYHoC5FX}
}

% NSGA-Net NSGA-II/pymoo inspired multiobjective NAS
@article{lu2018nsga,
  title     = {NSGA-NET: a multi-objective genetic algorithm for neural architecture search},
  author    = {Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and  Banzhaf, Wolfgang},
  booktitle = {GECCO-2019},
  year      = {2018}
}

% multiobjective TPE
@inproceedings{ozaki2020multiobjective,
  author    = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Onishi, Masaki},
  title     = {Multiobjective Tree-Structured Parzen Estimator for Computationally Expensive Optimization Problems},
  year      = {2020},
  doi       = {10.1145/3377930.3389817},
  booktitle = {Proc. the 2020 Genetic and Evolutionary Computation Conference (GECCO '20)},
  pages     = {533–541},
  numpages  = {9},
  location  = {Canc\'{u}n, Mexico}
}

% another multiobjective TPE
@article{ozaki2022multiobjective,
  title   = {Multiobjective tree-structured Parzen estimator},
  author  = {Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Nomura, Masahiro and Onishi, Masaki},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {73},
  pages   = {1209--1250},
  year    = {2022}
}

% Powell's solvers (scipy default)
@misc{ragonneau2023pdfo,
  title        = {{PDFO}: a cross-platform package for {Powell}'s derivative-free optimization solvers},
  author       = {Ragonneau, T. M. and Zhang, Z.},
  howpublished = {arXiv:2302.13246},
  year         = 2023
}

% BoostDFO's multiobjective strategy, based on Chebyshev scalarizations
@article{tavares2023parallel,
  title     = {Parallel strategies for direct multisearch},
  author    = {Tavares, S and Br{\'a}s, CP and Cust{\'o}dio, AL and Duarte, V and Medeiros, P},
  journal   = {Numerical Algorithms},
  volume    = {92},
  number    = {3},
  pages     = {1757--1788},
  year      = {2023},
  publisher = {Springer}
}

% EA-esque neural ensemble search (NES) outperforms DARTS for diversity and robustness to dataset shift, even without taking a multiobjective approach
@article{zaidi2021neural,
  title   = {Neural ensemble search for uncertainty estimation and dataset shift},
  author  = {Zaidi, Sheheryar and Zela, Arber and Elsken, Thomas and Holmes, Chris C and Hutter, Frank and Teh, Yee},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {7898--7911},
  year    = {2021}
}

% ParEGO
@article{parego2006,
  author  = {Knowles, J.},
  journal = {IEEE Transactions on Evolutionary Computation},
  title   = {ParEGO: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  year    = {2006},
  volume  = {10},
  number  = {1},
  pages   = {50-66},
  doi     = {10.1109/TEVC.2005.851274}
}

% ParEGO update
@inproceedings{cristescu2015surrogate,
  title     = {Surrogate-based multiobjective optimization: ParEGO update and test},
  author    = {Cristescu, Cristina and Knowles, Joshua},
  booktitle = {Workshop on Computational Intelligence (UKCI)},
  volume    = {770},
  pages     = {46},
  year      = {2015}
}

@inproceedings{hutter2012parallel,
  title        = {Parallel algorithm configuration},
  author       = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle    = {Learning and Intelligent Optimization: 6th International Conference, LION 6, Paris, France, January 16-20, 2012, Revised Selected Papers},
  pages        = {55--70},
  year         = {2012},
  organization = {Springer}
}

% HPOBench benchmarks
@article{klein_tabular_2019,
  author     = {Aaron Klein and
                Frank Hutter},
  title      = {Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization},
  journal    = {CoRR},
  volume     = {abs/1905.04970},
  year       = {2019},
  url        = {http://arxiv.org/abs/1905.04970},
  eprinttype = {arXiv},
  eprint     = {1905.04970},
  timestamp  = {Tue, 28 May 2019 12:48:08 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-04970.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@techreport{xiao2017,
  author      = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title       = {{Fashion-MNIST}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  year        = {2017},
  doi         = {10.48550/arXiv.1708.07747},
  institution = {ArXiv Preprint}
}

% Combo benchmark
@article{xia_predicting_2018,
  title    = {Predicting tumor cell line response to drug pairs with deep learning},
  volume   = {19},
  issn     = {1471-2105},
  url      = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2509-3},
  doi      = {10.1186/s12859-018-2509-3},
  abstract = {Background: The National Cancer Institute drug pair screening effort against 60 well-characterized human tumor cell lines (NCI-60) presents an unprecedented resource for modeling combinational drug activity.
              Results: We present a computational model for predicting cell line response to a subset of drug pairs in the NCI-ALMANAC database. Based on residual neural networks for encoding features as well as predicting tumor growth, our model explains 94\% of the response variance. While our best result is achieved with a combination of molecular feature types (gene expression, microRNA and proteome), we show that most of the predictive power comes from drug descriptors. To further demonstrate value in detecting anticancer therapy, we rank the drug pairs for each cell line based on model predicted combination effect and recover 80\% of the top pairs with enhanced activity.
              Conclusions: We present promising results in applying deep learning to predicting combinational drug response. Our feature analysis indicates screening data involving more cell lines are needed for the models to make better use of molecular features.},
  language = {en},
  number   = {S18},
  urldate  = {2021-11-08},
  journal  = {BMC Bioinformatics},
  author   = {Xia, Fangfang and Shukla, Maulik and Brettin, Thomas and Garcia-Cardona, Cristina and Cohn, Judith and Allen, Jonathan E. and Maslov, Sergei and Holbeck, Susan L. and Doroshow, James H. and Evrard, Yvonne A. and Stahlberg, Eric A. and Stevens, Rick L.},
  month    = dec,
  year     = {2018},
  pages    = {486}
}

% Privacy in Machine Learning
@inproceedings{dwork2008differential,
  title        = {Differential privacy: A survey of results},
  author       = {Dwork, Cynthia},
  booktitle    = {International conference on theory and applications of models of computation},
  pages        = {1--19},
  year         = {2008},
  organization = {Springer}
}

% Fairness in Machine Learning
@article{mehrabi2021survey,
  title     = {A survey on bias and fairness in machine learning},
  author    = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal   = {ACM computing surveys (CSUR)},
  volume    = {54},
  number    = {6},
  pages     = {1--35},
  year      = {2021},
  publisher = {ACM New York, NY, USA}
}

% Calibration in Machine Learning
@article{song2021classifier,
  title   = {Classifier Calibration: A survey on how to assess and improve predicted class probabilities},
  author  = {Song, Hao and Perello-Nieto, Miquel and Santos-Rodriguez, Raul and Kull, Meelis and Flach, Peter and others},
  journal = {arXiv preprint arXiv:2112.10327},
  year    = {2021}
}

% Uncertainty Quantification in Machine Learning
@article{begoli2019need,
  title     = {The need for uncertainty quantification in machine-assisted medical decision making},
  author    = {Begoli, Edmon and Bhattacharya, Tanmoy and Kusnezov, Dimitri},
  journal   = {Nature Machine Intelligence},
  volume    = {1},
  number    = {1},
  pages     = {20--23},
  year      = {2019},
  publisher = {Nature Publishing Group UK London}
}

@article{gawlikowski2023survey,
  title     = {A survey of uncertainty in deep neural networks},
  author    = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal   = {Artificial Intelligence Review},
  pages     = {1--77},
  year      = {2023},
  publisher = {Springer}
}

% Explainable Machine Learning
@article{burkart2021survey,
  title   = {A survey on the explainability of supervised machine learning},
  author  = {Burkart, Nadia and Huber, Marco F},
  journal = {Journal of Artificial Intelligence Research},
  volume  = {70},
  pages   = {245--317},
  year    = {2021}
}

@article{tjoa2020survey,
  title     = {A survey on explainable artificial intelligence (xai): Toward medical xai},
  author    = {Tjoa, Erico and Guan, Cuntai},
  journal   = {IEEE transactions on neural networks and learning systems},
  volume    = {32},
  number    = {11},
  pages     = {4793--4813},
  year      = {2020},
  publisher = {IEEE}
}

% Adversarial Robustness Machine Learning
@article{muhammad2022survey,
  title     = {A survey on efficient methods for adversarial robustness},
  author    = {Muhammad, Awais and Bae, Sung-Ho},
  journal   = {IEEE Access},
  volume    = {10},
  pages     = {118815--118830},
  year      = {2022},
  publisher = {IEEE}
}

% Performance aware neural architecture search
@inproceedings{tan2019mnasnet,
  title     = {Mnasnet: Platform-aware neural architecture search for mobile},
  author    = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {2820--2828},
  year      = {2019}
}

@article{smac3_2022,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

@article{pinelis2019order,
  title   = {Order statistics on the spacings between order statistics for the uniform distribution},
  author  = {Pinelis, Iosif},
  journal = {arXiv preprint arXiv:1909.06406},
  year    = {2019}
}

@article{jordan2015machine,
  title     = {Machine learning: Trends, perspectives, and prospects},
  author    = {Jordan, Michael I and Mitchell, Tom M},
  journal   = {Science},
  volume    = {349},
  number    = {6245},
  pages     = {255--260},
  year      = {2015},
  publisher = {American Association for the Advancement of Science}
}

@article{sun_igd_evo_2019,
  author  = {Sun, Yanan and Yen, Gary G. and Yi, Zhang},
  journal = {IEEE Transactions on Evolutionary Computation},
  title   = {IGD Indicator-Based Evolutionary Algorithm for Many-Objective Optimization Problems},
  year    = {2019},
  volume  = {23},
  number  = {2},
  pages   = {173-187},
  doi     = {10.1109/TEVC.2018.2791283}
}

@article{liu2021winningautodl,
  title     = {Winning solutions and post-challenge analyses of the ChaLearn AutoDL challenge 2019},
  author    = {Liu, Zhengying and Pavao, Adrien and Xu, Zhen and Escalera, Sergio and Ferreira, Fabio and Guyon, Isabelle and Hong, Sirui and Hutter, Frank and Ji, Rongrong and Junior, Julio CS Jacques and others},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {43},
  number    = {9},
  pages     = {3108--3125},
  year      = {2021},
  publisher = {IEEE}
}

% Survey on Multi-Objective HPO
@misc{karl2022multiobjective,
  title         = {Multi-Objective Hyperparameter Optimization -- An Overview},
  author        = {Florian Karl and Tobias Pielok and Julia Moosbauer and Florian Pfisterer and Stefan Coors and Martin Binder and Lennart Schneider and Janek Thomas and Jakob Richter and Michel Lang and Eduardo C. Garrido-Merchán and Juergen Branke and Bernd Bischl},
  year          = {2022},
  eprint        = {2206.07438},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

% PINN benchmark

@misc{wight2020solving,
  title         = {Solving Allen-Cahn and Cahn-Hilliard Equations using the Adaptive Physics Informed Neural Networks},
  author        = {Colby L. Wight and Jia Zhao},
  year          = {2020},
  eprint        = {2007.04542},
  archiveprefix = {arXiv},
  primaryclass  = {math.NA}
}

@misc{wang2020understanding,
  title         = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
  author        = {Sifan Wang and Yujun Teng and Paris Perdikaris},
  year          = {2020},
  eprint        = {2001.04536},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{https://doi.org/10.13140/rg.2.2.20057.24169,
  doi       = {10.13140/RG.2.2.20057.24169},
  url       = {http://rgdoi.net/10.13140/RG.2.2.20057.24169},
  author    = {Bischof, Rafael and Kraus, Michael},
  language  = {en},
  title     = {Multi-Objective Loss Balancing for Physics-Informed Deep Learning},
  publisher = {Unpublished},
  year      = {2021}
}

% Quantile transformation
@article{amaratunga2001analysis,
  title     = {Analysis of data from viral DNA microchips},
  author    = {Amaratunga, Dhammika and Cabrera, Javier},
  journal   = {Journal of the American Statistical Association},
  volume    = {96},
  number    = {456},
  pages     = {1161--1170},
  year      = {2001},
  publisher = {Taylor \& Francis}
}

@article{bolstad2003comparison,
  title     = {A comparison of normalization methods for high density oligonucleotide array data based on variance and bias},
  author    = {Bolstad, Benjamin M and Irizarry, Rafael A and {\AA}strand, Magnus and Speed, Terence P.},
  journal   = {Bioinformatics},
  volume    = {19},
  number    = {2},
  pages     = {185--193},
  year      = {2003},
  publisher = {Oxford University Press}
}

@inproceedings{hutter2014fanova,
  author    = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  title     = {An Efficient Approach for Assessing Hyperparameter Importance},
  year      = {2014},
  publisher = {JMLR.org},
  abstract  = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that--even in very highdimensional cases--most performance variation is attributable to just a few hyperparameters.},
  booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
  pages     = {I–754–I–762},
  location  = {Beijing, China},
  series    = {ICML'14}
}

@article{wild2015dfo,
  doi       = {10.1088/0954-3899/42/3/034031},
  url       = {https://doi.org/10.1088%2F0954-3899%2F42%2F3%2F034031},
  year      = 2015,
  month     = {feb},
  publisher = {{IOP} Publishing},
  volume    = {42},
  number    = {3},
  pages     = {034031},
  author    = {Stefan M Wild and Jason Sarich and Nicolas Schunck},
  title     = {Derivative-free optimization for parameter estimation in computational nuclear physics},
  journal   = {Journal of Physics G: Nuclear and Particle Physics}
}

@article{hauschild2011introduction,
  title     = {An introduction and survey of estimation of distribution algorithms},
  author    = {Hauschild, Mark and Pelikan, Martin},
  journal   = {Swarm and evolutionary computation},
  volume    = {1},
  number    = {3},
  pages     = {111--128},
  year      = {2011},
  publisher = {Elsevier}
}

@article{olsson1975nelder,
  title     = {The Nelder-Mead simplex procedure for function minimization},
  author    = {Olsson, Donald M and Nelson, Lloyd S},
  journal   = {Technometrics},
  volume    = {17},
  number    = {1},
  pages     = {45--51},
  year      = {1975},
  publisher = {Taylor \& Francis}
}

@article{rutenbar1989simulated,
  title     = {Simulated annealing algorithms: An overview},
  author    = {Rutenbar, Rob A},
  journal   = {IEEE Circuits and Devices magazine},
  volume    = {5},
  number    = {1},
  pages     = {19--26},
  year      = {1989},
  publisher = {IEEE}
}

@article{poli2007particle,
  title     = {Particle swarm optimization},
  author    = {Poli, Riccardo and Kennedy, James and Blackwell, Tim},
  journal   = {Swarm intelligence},
  volume    = {1},
  number    = {1},
  pages     = {33--57},
  year      = {2007},
  publisher = {Springer}
}

@article{larson2019derivative,
  title     = {Derivative-free optimization methods},
  author    = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M},
  journal   = {Acta Numerica},
  volume    = {28},
  pages     = {287--404},
  year      = {2019},
  publisher = {Cambridge University Press}
}

@article{back1993overview,
  title     = {An overview of evolutionary algorithms for parameter optimization},
  author    = {B{\"a}ck, Thomas and Schwefel, Hans-Paul},
  journal   = {Evolutionary computation},
  volume    = {1},
  number    = {1},
  pages     = {1--23},
  year      = {1993},
  publisher = {mit Press}
}

@article{bartz2016survey,
  title   = {A survey of model-based methods for global optimization},
  author  = {Bartz-Beielstein, Thomas},
  journal = {Bioinspired Optimization Methods and Their Applications},
  pages   = {1--18},
  year    = {2016}
}


@article{bischl2017mlrmbo,
  title   = {mlrMBO: A modular framework for model-based optimization of expensive black-box functions},
  author  = {Bischl, Bernd and Richter, Jakob and Bossek, Jakob and Horn, Daniel and Thomas, Janek and Lang, Michel},
  journal = {arXiv preprint arXiv:1703.03373},
  year    = {2017}
}

@article{de2005tutorial,
  title     = {A tutorial on the cross-entropy method},
  author    = {De Boer, Pieter-Tjerk and Kroese, Dirk P and Mannor, Shie and Rubinstein, Reuven Y},
  journal   = {Annals of operations research},
  volume    = {134},
  number    = {1},
  pages     = {19--67},
  year      = {2005},
  publisher = {Springer}
}

@article{shariari2016outoftheloop,
  title      = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
  volume     = {104},
  issn       = {0018-9219, 1558-2256},
  shorttitle = {Taking the {Human} {Out} of the {Loop}},
  url        = {https://ieeexplore.ieee.org/document/7352306/},
  doi        = {10.1109/JPROC.2015.2494218},
  abstract   = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable conﬁguration parameters. These parameters are often speciﬁed and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in signiﬁcant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  language   = {en},
  number     = {1},
  urldate    = {2022-03-29},
  journal    = {Proceedings of the IEEE},
  author     = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  month      = jan,
  year       = {2016},
  pages      = {148--175}
}

@article{10.1145/3372390,
  author     = {Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.},
  title      = {The Landscape of Exascale Research: A Data-Driven Literature Analysis},
  year       = {2020},
  issue_date = {March 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {53},
  number     = {2},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3372390},
  doi        = {10.1145/3372390},
  abstract   = {The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing.},
  journal    = {ACM Comput. Surv.},
  month      = {mar},
  articleno  = {23},
  numpages   = {43},
  keywords   = {extreme-scale computing, data-driven analysis, Exascale computing, high-performance computing, literature review}
}

@inproceedings{bergstra2013making,
  title        = {Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author       = {Bergstra, James and Yamins, Daniel and Cox, David},
  booktitle    = {International conference on machine learning},
  pages        = {115--123},
  year         = {2013},
  organization = {PMLR}
}

@inproceedings{bergstra2011hpo,
  author    = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
  title     = {Algorithms for Hyper-Parameter Optimization},
  year      = {2011},
  isbn      = {9781618395993},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
  pages     = {2546–2554},
  numpages  = {9},
  location  = {Granada, Spain},
  series    = {NIPS'11}
}

@article{frazier_tutorial_2018,
  title    = {A {Tutorial} on {Bayesian} {Optimization}},
  url      = {http://arxiv.org/abs/1807.02811},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantiﬁes the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function deﬁned from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-ﬁdelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the ﬁeld. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justiﬁed by a formal decision-theoretic argument, standing in contrast to previous ad hoc modiﬁcations.},
  language = {en},
  urldate  = {2022-03-31},
  journal  = {arXiv:1807.02811 [cs, math, stat]},
  author   = {Frazier, Peter I.},
  month    = jul,
  year     = {2018},
  note     = {arXiv: 1807.02811},
  keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning}
}

@article{mockus1978application,
  title   = {The application of Bayesian methods for seeking the extremum},
  author  = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  journal = {Towards global optimization},
  volume  = {2},
  number  = {117-129},
  pages   = {2},
  year    = {1978}
}

@article{liu2019gpbigdata,
  title      = {When {Gaussian} {Process} {Meets} {Big} {Data}: {A} {Review} of {Scalable} {GPs}},
  shorttitle = {When {Gaussian} {Process} {Meets} {Big} {Data}},
  url        = {http://arxiv.org/abs/1807.01065},
  abstract   = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process (GP) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable GPs involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit speciﬁc structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues regarding the implementation of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
  language   = {en},
  urldate    = {2022-03-31},
  journal    = {arXiv:1807.01065 [cs, stat]},
  author     = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  month      = apr,
  year       = {2019},
  note       = {arXiv: 1807.01065},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{snoek2015scalable,
  title        = {Scalable bayesian optimization using deep neural networks},
  author       = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle    = {International conference on machine learning},
  pages        = {2171--2180},
  year         = {2015},
  organization = {PMLR}
}

@incollection{ginsbourger2010kriging,
  title     = {Kriging is well-suited to parallelize optimization},
  author    = {Ginsbourger, David and Riche, Rodolphe Le and Carraro, Laurent},
  booktitle = {Computational intelligence in expensive optimization problems},
  pages     = {131--162},
  year      = {2010},
  publisher = {Springer}
}

@inproceedings{gonzalez2016batch,
  title        = {Batch Bayesian optimization via local penalization},
  author       = {Gonz{\'a}lez, Javier and Dai, Zhenwen and Hennig, Philipp and Lawrence, Neil},
  booktitle    = {Artificial intelligence and statistics},
  pages        = {648--657},
  year         = {2016},
  organization = {PMLR}
}

@article{snoek2012practical,
  title   = {Practical bayesian optimization of machine learning algorithms},
  author  = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal = {Advances in neural information processing systems},
  volume  = {25},
  year    = {2012}
}

@inproceedings{hernandez2017parallel,
  title        = {Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space},
  author       = {Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Requeima, James and Pyzer-Knapp, Edward O and Aspuru-Guzik, Al{\'a}n},
  booktitle    = {International conference on machine learning},
  pages        = {1470--1479},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{garcia2019fully,
  title     = {Fully Distributed Bayesian Optimization with Stochastic Policies},
  author    = {Garcia-Barcos, Javier and Martinez-Cantin, Ruben},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {2357--2363},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/327},
  url       = {https://doi.org/10.24963/ijcai.2019/327}
}

@inproceedings{alvi2019asyncbo,
  title     = {Asynchronous Batch {B}ayesian Optimisation with Improved Local Penalisation},
  author    = {Alvi, Ahsan and Ru, Binxin and Calliess, Jan-Peter and Roberts, Stephen and Osborne, Michael A.},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {253--262},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/alvi19a/alvi19a.pdf},
  url       = {https://proceedings.mlr.press/v97/alvi19a.html},
  abstract  = {Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisation on K Workers (PLAyBOOK), for asynchronous parallel BO. We demonstrate empirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world problem. We undertake a comparison between synchronous and asynchronous BO, and show that asynchronous BO often outperforms synchronous batch BO in both wall-clock time and sample efficiency.}
}

@article{jones2001taxonomy,
  title     = {A taxonomy of global optimization methods based on response surfaces},
  author    = {Jones, Donald R},
  journal   = {Journal of global optimization},
  volume    = {21},
  pages     = {345--383},
  year      = {2001},
  publisher = {Springer}
}

@article{wenzel2020hyperparameter,
  title   = {Hyperparameter ensembles for robustness and uncertainty quantification},
  author  = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {6514--6527},
  year    = {2020}
}

@article{liaw2018tune,
  title   = {Tune: A Research Platform for Distributed Model Selection and Training},
  author  = {Liaw, Richard and Liang, Eric and Nishihara, Robert
             and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
  journal = {arXiv preprint arXiv:1807.05118},
  year    = {2018}
}

@misc{optuna_software,
  doi       = {10.48550/ARXIV.1907.10902},
  url       = {https://arxiv.org/abs/1907.10902},
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{tagasovska2018single,
  title   = {Single-model uncertainties for deep learning},
  author  = {Tagasovska, Natasa and Lopez-Paz, David},
  journal = {arXiv preprint arXiv:1811.00908},
  year    = {2018}
}

@book{rudary2009predictive,
  title     = {On predictive linear {G}aussian models},
  author    = {Rudary, Matthew R},
  year      = {2009},
  pages     = {385-386},
  publisher = {University of Michigan}
}

@article{der2009aleatory,
  title     = {Aleatory or epistemic? Does it matter?},
  author    = {Der Kiureghian, Armen and Ditlevsen, Ove},
  journal   = {Structural Safety},
  volume    = {31},
  number    = {2},
  pages     = {105--112},
  year      = {2009},
  publisher = {Elsevier}
}

@article{van2021feature,
  title   = {On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty},
  author  = {van Amersfoort, Joost and Smith, Lewis and Jesson, Andrew and Key, Oscar and Gal, Yarin},
  journal = {arXiv preprint arXiv:2102.11409},
  year    = {2021}
}

@article{fort2019deep,
  title   = {Deep ensembles: A loss landscape perspective},
  author  = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal = {arXiv preprint arXiv:1912.02757},
  year    = {2019}
}


@article{russell2021multivariate,
  title     = {Multivariate uncertainty in deep learning},
  author    = {Russell, Rebecca L and Reale, Christopher},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2021},
  publisher = {IEEE}
}


@article{zaidi2020neural,
  title   = {Neural Ensemble Search for Uncertainty Estimation and Dataset Shift},
  author  = {Zaidi, Sheheryar and Zela, Arber and Elsken, Thomas and Holmes, Chris and Hutter, Frank and Teh, Yee Whye},
  journal = {arXiv preprint arXiv:2006.08573},
  year    = {2020}
}


@article{bishop1994mixture,
  title     = {Mixture density networks},
  author    = {Bishop, Christopher M},
  year      = {1994},
  publisher = {Aston University}
}


@inproceedings{blundell2015weight,
  title        = {Weight uncertainty in neural network},
  author       = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1613--1622},
  year         = {2015},
  organization = {PMLR}
}



@inproceedings{10.1145/3295500.3356202,
  author    = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Wild, Stefan and Vishwanath, Venkatram and Xia, Fangfang and Brettin, Tom and Stevens, Rick},
  title     = {Scalable Reinforcement-Learning-Based Neural Architecture Search for Cancer Deep Learning Research},
  year      = {2019},
  isbn      = {9781450362290},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3295500.3356202},
  doi       = {10.1145/3295500.3356202},
  abstract  = {Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {37},
  numpages  = {33},
  keywords  = {deep learning, neural architecture search, reinforcement learning, cancer},
  location  = {Denver, Colorado},
  series    = {SC '19}
}

@inproceedings{10.5555/3433701.3433711,
  author    = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
  title     = {Recurrent Neural Network Architecture Search for Geophysical Emulation},
  year      = {2020},
  isbn      = {9781728199986},
  publisher = {IEEE Press},
  abstract  = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. Constructing neural networks for forecasting such data is non-trivial, however, and often requires trial and error. To address these limitations, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks (POD-LSTMs). We develop a scalable neural architecture search for generating stacked LSTMs to forecast temperature in the NOAA Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies POD-LSTMs that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {8},
  numpages  = {14},
  keywords  = {emulation, geophysics, recurrent neural networks},
  location  = {Atlanta, Georgia},
  series    = {SC '20}
}

@article{izmailov2018averaging,
  title   = {Averaging weights leads to wider optima and better generalization},
  author  = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal = {arXiv preprint arXiv:1803.05407},
  year    = {2018}
}

@article{lakshminarayanan2016simple,
  title   = {Simple and scalable predictive uncertainty estimation using deep ensembles},
  author  = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal = {arXiv preprint arXiv:1612.01474},
  year    = {2016}
}

@inproceedings{9378060,
  author    = {S. {Jiang} and P. {Balaprakash}},
  booktitle = {2020 IEEE International Conference on Big Data (Big Data)},
  title     = {Graph Neural Network Architecture Search for Molecular Property Prediction},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1346-1353},
  doi       = {10.1109/BigData50022.2020.9378060}
}


@misc{wu2018moleculenet,
  title         = {{MoleculeNet}: A Benchmark for Molecular Machine Learning},
  author        = {Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
  year          = {2018},
  eprint        = {1703.00564},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{maulik2021reduced,
  title     = {Reduced-order modeling of advection-dominated systems with recurrent neural networks and convolutional autoencoders},
  author    = {Maulik, Romit and Lusch, Bethany and Balaprakash, Prasanna},
  journal   = {Physics of Fluids},
  volume    = {33},
  number    = {3},
  pages     = {037106},
  year      = {2021},
  publisher = {American Institute of Phyiscs}
}

@article{Real_Aggarwal_Huang_Le_2019,
  title        = {Regularized Evolution for Image Classifier Architecture Search},
  volume       = {33},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4405},
  doi          = {10.1609/aaai.v33i01.33014780},
  abstractnote = {&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;},
  number       = {01},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  year         = {2019},
  month        = {Jul.},
  pages        = {4780-4789}
}


@article{HE2021106622,
  title    = {{AutoML}: A survey of the state-of-the-art},
  journal  = {Knowledge-Based Systems},
  volume   = {212},
  pages    = {106622},
  year     = {2021},
  issn     = {0950-7051},
  doi      = {https://doi.org/10.1016/j.knosys.2020.106622},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705120307516},
  author   = {Xin He and Kaiyong Zhao and Xiaowen Chu},
  keywords = {Deep learning, Automated machine learning (autoML), Neural architecture search (NAS), Hyperparameter optimization (HPO)},
  abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods – covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.}
}

@article{10.1162/106365602320169811,
  author     = {Stanley, Kenneth O. and Miikkulainen, Risto},
  title      = {Evolving Neural Networks through Augmenting Topologies},
  year       = {2002},
  issue_date = {Summer 2002},
  publisher  = {MIT Press},
  address    = {Cambridge, MA, USA},
  volume     = {10},
  number     = {2},
  issn       = {1063-6560},
  url        = {https://doi.org/10.1162/106365602320169811},
  doi        = {10.1162/106365602320169811},
  journal    = {Evol. Comput.},
  month      = jun,
  pages      = {99–127},
  numpages   = {29},
  keywords   = {genetic algorithms, network topologies, neuroevolution, competing conventions, speciation, neural networks}
}

@article{ashukha2020pitfalls,
  title   = {Pitfalls of in-domain uncertainty estimation and ensembling in deep learning},
  author  = {Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},
  journal = {arXiv preprint arXiv:2002.06470},
  year    = {2020}
}

@article{ovadia2019can,
  title   = {Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
  author  = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal = {arXiv preprint arXiv:1906.02530},
  year    = {2019}
}

@article{wilson2020bayesian,
  title   = {Bayesian deep learning and a probabilistic perspective of generalization},
  author  = {Wilson, Andrew Gordon and Izmailov, Pavel},
  journal = {arXiv preprint arXiv:2002.08791},
  year    = {2020}
}

@article{srivastava2014dropout,
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The Journal of Machine Learning research},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  year      = {2014},
  publisher = {JMLR. org}
}


@article{hoffman2013stochastic,
  title   = {Stochastic variational inference.},
  author  = {Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal = {Journal of Machine Learning Research},
  volume  = {14},
  number  = {5},
  year    = {2013}
}

@book{neal2012bayesian,
  title     = {Bayesian learning for neural networks},
  author    = {Neal, Radford M},
  volume    = {118},
  year      = {2012},
  publisher = {Springer Science \& Business Media}
}

@article{DBLP:journals/corr/abs-1802-03268,
  author        = {Hieu Pham and
                   Melody Y. Guan and
                   Barret Zoph and
                   Quoc V. Le and
                   Jeff Dean},
  title         = {Efficient Neural Architecture Search via Parameter Sharing},
  journal       = {CoRR},
  volume        = {abs/1802.03268},
  year          = {2018},
  url           = {http://arxiv.org/abs/1802.03268},
  archiveprefix = {arXiv},
  eprint        = {1802.03268},
  timestamp     = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1802-03268.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{58871,
  author  = {Hansen, L.K. and Salamon, P.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Neural network ensembles},
  year    = {1990},
  volume  = {12},
  number  = {10},
  pages   = {993--1001},
  doi     = {10.1109/34.58871}
}

@inproceedings{10.5555/648054.743935,
  author    = {Dietterich, Thomas G.},
  title     = {Ensemble Methods in Machine Learning},
  year      = {2000},
  isbn      = {3540677046},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  abstract  = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
  booktitle = {Proceedings of the First International Workshop on Multiple Classifier Systems},
  pages     = {1–-15},
  numpages  = {15},
  series    = {MCS '00}
}

@inproceedings{10.5555/3091696.3091730,
  author    = {Kohavi, Ron and Wolpert, David},
  title     = {Bias plus Variance Decomposition for Zero-One Loss Functions},
  year      = {1996},
  isbn      = {1558604197},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA},
  booktitle = {Proceedings of the Thirteenth International Conference on International Conference on Machine Learning},
  pages     = {275–-283},
  numpages  = {9},
  location  = {Bari, Italy},
  series    = {ICML'96}
}

@inproceedings{gal2016dropout,
  title        = {Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning},
  author       = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1050--1059},
  year         = {2016},
  organization = {PMLR}
}

@article{VERIFICATIONOFFORECASTSEXPRESSEDINTERMSOFPROBABILITY,
  author    = {GLENN W.  BRIER},
  title     = {Verification  of Forecasts Expressed in Terms of Probability},
  journal   = {Monthly Weather Review},
  year      = {1950},
  publisher = {American Meteorological Society},
  address   = {Boston MA, USA},
  volume    = {78},
  number    = {1},
  doi       = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  pages     = {1 - 3},
  url       = {https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml}
}

@article{gneiting2007strictly,
  title     = {Strictly proper scoring rules, prediction, and estimation},
  author    = {Gneiting, Tilmann and Raftery, Adrian E},
  journal   = {Journal of the American statistical Association},
  volume    = {102},
  number    = {477},
  pages     = {359--378},
  year      = {2007},
  publisher = {Taylor \& Francis}
}

@inproceedings{374138,
  author    = {Nix, D.A. and Weigend, A.S.},
  booktitle = {Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},
  title     = {Estimating the mean and variance of the target probability distribution},
  year      = {1994},
  volume    = {1},
  number    = {},
  pages     = {55-60 vol.1},
  doi       = {10.1109/ICNN.1994.374138}
}

@article{hutter_efficient_nodate,
  title    = {An Efficient Approach for Assessing Hyperparameter Importance},
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efﬁcient methods that can be used to gain such insight, leveraging random forest models ﬁt on the data already gathered by Bayesian optimization. We ﬁrst introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional {ANOVA} framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very highdimensional cases—most performance variation is attributable to just a few hyperparameters.},
  pages    = {9},
  author   = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  langid   = {english},
  file     = {Hutter et al. - An Efficient Approach for Assessing Hyperparameter.pdf:/Users/romainegele/Zotero/storage/4T5QQMTF/Hutter et al. - An Efficient Approach for Assessing Hyperparameter.pdf:application/pdf}
}

@article{kendall2017uncertainties,
  title   = {What uncertainties do we need in {B}ayesian deep learning for computer vision?},
  author  = {Kendall, Alex and Gal, Yarin},
  journal = {arXiv preprint arXiv:1703.04977},
  year    = {2017}
}
@article{hullermeier2021aleatoric,
  title     = {Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods},
  author    = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal   = {Machine Learning},
  volume    = {110},
  number    = {3},
  pages     = {457--506},
  year      = {2021},
  publisher = {Springer}
}



@article{escalante_particle_nodate,
  title    = {Particle Swarm Model Selection},
  abstract = {This paper proposes the application of particle swarm optimization ({PSO}) to the problem of full model selection, {FMS}, for classiﬁcation tasks. {FMS} is deﬁned as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classiﬁcation error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. {FMS} can be applied to any classiﬁcation domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with {FMS}. We adopt {PSO} for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows {PSO} to avoid overﬁtting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with {PSO}, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge.},
  pages    = {36},
  author   = {Escalante, Hugo Jair and Montes, Manuel and Sucar, Luis Enrique and Mx, Inaoep and Mx, Inaoep},
  langid   = {english},
  file     = {Escalante et al. - Particle Swarm Model Selection.pdf:/Users/romainegele/Zotero/storage/LZZ2WPWS/HH4H4JEI.pdf:application/pdf}
}
@article{mclachlan2019finite,
  title     = {Finite mixture models},
  author    = {McLachlan, Geoffrey J and Lee, Sharon X and Rathnayake, Suren I},
  journal   = {Annual Review of Statistics and Its Application},
  volume    = {6},
  pages     = {355--378},
  year      = {2019},
  publisher = {Annual Reviews}
}

@article{karnin_almost_nodate,
  title    = {Almost Optimal Exploration in Multi-Armed Bandits},
  abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameterfree algorithms for identifying the best arm, in two diﬀerent settings: given a target conﬁdence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doublylogarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
  pages    = {9},
  author   = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
  langid   = {english},
  file     = {Karnin et al. - Almost Optimal Exploration in Multi-Armed Bandits.pdf:/Users/romainegele/Zotero/storage/6NNZ6HXR/Karnin et al. - Almost Optimal Exploration in Multi-Armed Bandits.pdf:application/pdf}
}

@article{swersky_freeze-thaw_2014,
  title        = {Freeze-Thaw {Bayesian} Optimization},
  url          = {http://arxiv.org/abs/1406.3896},
  abstract     = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
  journaltitle = {{arXiv}:1406.3896 [cs, stat]},
  author       = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  urldate      = {2020-07-06},
  date         = {2014-06-15},
  eprinttype   = {arxiv},
  eprint       = {1406.3896},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/A88TTWXH/Swersky et al. - 2014 - Freeze-Thaw Bayesian Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/C7XXXR3F/1406.html:text/html}
}

@article{yu_hyper-parameter_2020,
  title        = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
  url          = {http://arxiv.org/abs/2003.05689},
  shorttitle   = {Hyper-Parameter Optimization},
  abstract     = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization ({HPO}) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on {HPO}. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for {HPO}, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when {HPO} is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  journaltitle = {{arXiv}:2003.05689 [cs, stat]},
  author       = {Yu, Tong and Zhu, Hong},
  urldate      = {2020-07-28},
  date         = {2020-03-12},
  eprinttype   = {arxiv},
  eprint       = {2003.05689},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/WF4XSB7E/Yu and Zhu - 2020 - Hyper-Parameter Optimization A Review of Algorith.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/3ASVNAG5/2003.html:text/html}
}

@article{snoek_practical_nodate,
  title    = {Practical {Bayesian} Optimization of Machine Learning Algorithms},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process ({GP}). We show that certain choices for the nature of the {GP}, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured {SVMs} and convolutional neural networks.},
  pages    = {9},
  author   = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  langid   = {english},
  file     = {Snoek et al. - Practical Bayesian Optimization of Machine Learnin.pdf:/Users/romainegele/Zotero/storage/FEXFSSWR/Snoek et al. - Practical Bayesian Optimization of Machine Learnin.pdf:application/pdf}
}

@article{zoph_learning_2017,
  title        = {Learning Transferable Architectures for Scalable Image Recognition},
  url          = {http://arxiv.org/abs/1707.07012},
  abstract     = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "{NASNet} search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the {CIFAR}-10 dataset and then apply this cell to the {ImageNet} dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "{NASNet} architecture". We also introduce a new regularization technique called {ScheduledDropPath} that significantly improves generalization in the {NASNet} models. On {CIFAR}-10 itself, {NASNet} achieves 2.4\% error rate, which is state-of-the-art. On {ImageNet}, {NASNet} achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on {ImageNet}. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer {FLOPS} - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of {NASNets} exceed those of the state-of-the-art human-designed models. For instance, a small version of {NASNet} also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by {NASNet} used with the Faster-{RCNN} framework surpass state-of-the-art by 4.0\% achieving 43.1\% {mAP} on the {COCO} dataset.},
  journaltitle = {{arXiv}:1707.07012 [cs, stat]},
  author       = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  urldate      = {2018-08-20},
  date         = {2017-07-21},
  eprinttype   = {arxiv},
  eprint       = {1707.07012},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv\:1707.07012 PDF:/Users/romainegele/Zotero/storage/BSIICKHP/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable I.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/I9Q8HPCV/1707.html:text/html}
}

@article{zoph_neural_2016,
  title        = {Neural Architecture Search with Reinforcement Learning},
  url          = {http://arxiv.org/abs/1611.01578},
  abstract     = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this {RNN} with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the {CIFAR}-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our {CIFAR}-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used {LSTM} cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on {PTB} and achieves a state-of-the-art perplexity of 1.214.},
  journaltitle = {{arXiv}:1611.01578 [cs]},
  author       = {Zoph, Barret and Le, Quoc V.},
  urldate      = {2018-08-20},
  date         = {2016-11-04},
  eprinttype   = {arxiv},
  eprint       = {1611.01578},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1611.01578 PDF:/Users/romainegele/Zotero/storage/H2NPDM2L/Zoph and Le - 2016 - Neural Architecture Search with Reinforcement Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/RZMHVFRK/1611.html:text/html}
}

@article{jin_auto-keras_2018,
  title        = {{Auto-Keras}: Efficient Neural Architecture Search with Network Morphism},
  url          = {http://arxiv.org/abs/1806.10282},
  shorttitle   = {Auto-Keras},
  abstract     = {Neural architecture search ({NAS}) has been proposed to automatically tune deep neural networks, but existing search algorithms usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for {NAS} by enabling a more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search by introducing a neural network kernel and a tree-structured acquisition function optimization algorithm, which more efficiently explores the search space. Intensive experiments have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source {AutoML} system on our method, namely Auto-Keras. The system runs in parallel on {CPU} and {GPU}, with an adaptive search strategy for different {GPU} memory limits.},
  journaltitle = {{arXiv}:1806.10282 [cs, stat]},
  author       = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  urldate      = {2018-11-07},
  date         = {2018-06-26},
  eprinttype   = {arxiv},
  eprint       = {1806.10282},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1806.10282 PDF:/Users/romainegele/Zotero/storage/K8Y3XBT5/Jin et al. - 2018 - Auto-Keras Efficient Neural Architecture Search w.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/FGQEMA94/1806.html:text/html}
}

@article{real_regularized_2018,
  title        = {Regularized Evolution for Image Classifier Architecture Search},
  url          = {http://arxiv.org/abs/1802.01548},
  abstract     = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---{AmoebaNet}-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, {AmoebaNet}-A has comparable accuracy to current state-of-the-art {ImageNet} models discovered with more complex architecture-search methods. Scaled to larger size, {AmoebaNet}-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 {ImageNet} accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  journaltitle = {{arXiv}:1802.01548 [cs]},
  author       = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  urldate      = {2019-08-20},
  date         = {2018-02-05},
  year         = {2018},
  eprinttype   = {arxiv},
  eprint       = {1802.01548},
  keywords     = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.6, I.5.1, I.5.2},
  file         = {arXiv\:1802.01548 PDF:/Users/romainegele/Zotero/storage/YHAZD9PA/Real et al. - 2018 - Regularized Evolution for Image Classifier Archite.pdf:application/pdf;arXiv\:1802.01548 PDF:/Users/romainegele/Zotero/storage/GV8QGC3Z/Real et al. - 2018 - Regularized Evolution for Image Classifier Archite.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/D2JC4UYQ/1802.html:text/html;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/BSI84K8Q/1802.html:text/html}
}

@article{yang_nas_2020,
  title        = {{NAS} evaluation is frustratingly hard},
  url          = {http://arxiv.org/abs/1912.12522},
  abstract     = {Neural Architecture Search ({NAS}) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of \$8\$ {NAS} methods on \$5\$ datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many {NAS} techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used {DARTS} search space in order to understand the contribution of each component in the {NAS} pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macro-structure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between \$8\$ and \$20\$ cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current {NAS} pitfalls. The code used is available at https://github.com/antoyang/{NAS}-Benchmark.},
  journaltitle = {{arXiv}:1912.12522 [cs, stat]},
  author       = {Yang, Antoine and Esperança, Pedro M. and Carlucci, Fabio M.},
  urldate      = {2020-05-18},
  date         = {2020-02-13},
  eprinttype   = {arxiv},
  eprint       = {1912.12522},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/HBU4LLCF/Yang et al. - 2020 - NAS evaluation is frustratingly hard.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/UTSZTLC9/1912.html:text/html}
}

@article{li2020geometrydarts,
  title        = {Geometry-Aware Gradient Algorithms for Neural Architecture Search},
  url          = {http://arxiv.org/abs/2004.07802},
  abstract     = {Many recent state-of-the-art methods for neural architecture search ({NAS}) relax the {NAS} problem into a joint continuous optimization over architecture parameters and their shared-weights, enabling the application of standard gradient-based optimizers. However, this training process remains poorly understood, as evidenced by the multitude of gradient-based heuristics that have been recently proposed. Invoking the theory of mirror descent, we present a unifying framework for designing and analyzing gradient-based {NAS} methods that exploit the underlying problem structure to quickly find high-performance architectures. Our geometry-aware framework leads to simple yet novel algorithms that (1) enjoy faster convergence guarantees than existing gradient-based methods and (2) achieve state-of-the-art accuracy on the latest {NAS} benchmarks in computer vision. Notably, we exceed the best published results for both {CIFAR} and {ImageNet} on both the {DARTS} search space and {NAS}-Bench-201; on the latter benchmark we achieve close to oracle-optimal performance on {CIFAR}-10 and {CIFAR}-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous parameterizations of discrete {NAS} search spaces.},
  journaltitle = {{arXiv}:2004.07802 [cs, math, stat]},
  author       = {Li, Liam and Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
  urldate      = {2020-05-26},
  date         = {2020-04-16},
  eprinttype   = {arxiv},
  eprint       = {2004.07802},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Optimization and Control},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/F6P8LRX2/Li et al. - 2020 - Geometry-Aware Gradient Algorithms for Neural Arch.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/QWYAUIKB/2004.html:text/html}
}

@article{zela2020darts,
  title        = {Understanding and Robustifying Differentiable Architecture Search},
  url          = {http://arxiv.org/abs/1909.09656},
  abstract     = {Differentiable Architecture Search ({DARTS}) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, {DARTS} does not work robustly for new problems: we identify a wide range of search spaces for which {DARTS} yields degenerate architectures with very poor test performance. We study this failure mode and show that, while {DARTS} successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify {DARTS} to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of {DARTS} that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.},
  journaltitle = {{arXiv}:1909.09656 [cs, stat]},
  author       = {Zela, Arber and Elsken, Thomas and Saikia, Tonmoy and Marrakchi, Yassine and Brox, Thomas and Hutter, Frank},
  urldate      = {2020-05-30},
  date         = {2020-01-28},
  eprinttype   = {arxiv},
  eprint       = {1909.09656},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/QN69MWVJ/Zela et al. - 2020 - Understanding and Robustifying Differentiable Arch.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/ZP22T5MY/1909.html:text/html}
}

@article{chen_fitting_2020,
  title        = {Fitting the Search Space of Weight-sharing {NAS} with Graph Convolutional Networks},
  url          = {http://arxiv.org/abs/2004.08423},
  abstract     = {Neural architecture search has attracted wide attentions in both academia and industry. To accelerate it, researchers proposed weight-sharing methods which first train a super-network to reuse computation among different operators, from which exponentially many sub-networks can be sampled and efficiently evaluated. These methods enjoy great advantages in terms of computational costs, but the sampled sub-networks are not guaranteed to be estimated precisely unless an individual training process is taken. This paper owes such inaccuracy to the inevitable mismatch between assembled network layers, so that there is a random error term added to each estimation. We alleviate this issue by training a graph convolutional network to fit the performance of sampled sub-networks so that the impact of random errors becomes minimal. With this strategy, we achieve a higher rank correlation coefficient in the selected set of candidates, which consequently leads to better performance of the final architecture. In addition, our approach also enjoys the flexibility of being used under different hardware constraints, since the graph convolutional network has provided an efficient lookup table of the performance of architectures in the entire search space.},
  journaltitle = {{arXiv}:2004.08423 [cs, stat]},
  author       = {Chen, Xin and Xie, Lingxi and Wu, Jun and Wei, Longhui and Xu, Yuhui and Tian, Qi},
  urldate      = {2020-05-30},
  date         = {2020-04-17},
  eprinttype   = {arxiv},
  eprint       = {2004.08423},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/Y4V8LWLM/Chen et al. - 2020 - Fitting the Search Space of Weight-sharing NAS wit.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/UI9WE5P6/2004.html:text/html}
}

@article{zela_nas-bench-1shot1_2020,
  title    = {{NAS}-Bench-1SHOT1: Benchmarking and Dissecting One-Shot Neural Architecture Search},
  abstract = {One-shot neural architecture search ({NAS}) has played a crucial role in making {NAS} methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientiﬁc study of these components, we introduce a general framework for one-shot {NAS} that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that draws on the recent large-scale tabular benchmark {NAS}-Bench-101 for cheap anytime evaluations of one-shot {NAS} methods. To showcase the framework, we compare several state-of-the-art one-shot {NAS} methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for {NAS}-Bench-101.},
  pages    = {20},
  author   = {Zela, Arber and Siems, Julien and Hutter, Frank},
  date     = {2020},
  langid   = {english},
  file     = {Zela et al. - 2020 - NAS-BENCH-1SHOT1 BENCHMARKING AND DISSECTING ONE-.pdf:/Users/romainegele/Zotero/storage/7RTDVLIT/Zela et al. - 2020 - NAS-BENCH-1SHOT1 BENCHMARKING AND DISSECTING ONE-.pdf:application/pdf}
}

@article{ying_nas-bench-101_2019,
  title        = {{NAS}-Bench-101: Towards Reproducible Neural Architecture Search},
  url          = {http://arxiv.org/abs/1902.09635},
  shorttitle   = {{NAS}-Bench-101},
  abstract     = {Recent advances in neural architecture search ({NAS}) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing {NAS}-Bench-101, the first public architecture dataset for {NAS} research. To build {NAS}-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on {CIFAR}-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.},
  journaltitle = {{arXiv}:1902.09635 [cs, stat]},
  author       = {Ying, Chris and Klein, Aaron and Real, Esteban and Christiansen, Eric and Murphy, Kevin and Hutter, Frank},
  urldate      = {2020-06-01},
  date         = {2019-05-14},
  eprinttype   = {arxiv},
  eprint       = {1902.09635},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/LND3I76E/Ying et al. - 2019 - NAS-Bench-101 Towards Reproducible Neural Archite.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/FVB5UAAH/1902.html:text/html}
}

@article{dong_nas-bench-201_2020,
  title    = {{NAS}-{BENCH}-201: {E}xtending The Scope of Re-Producible Neural Architecture Search},
  abstract = {Neural architecture search ({NAS}) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the ﬁeld of {NAS}. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various {NAS} algorithms. {NAS}-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to {NAS}-Bench-101: {NAS}-Bench201 with a different search space, results on multiple datasets, and more diagnostic information. {NAS}-Bench-201 has a ﬁxed search space and provides a uniﬁed benchmark for almost any up-to-date {NAS} algorithms. The design of our search space is inspired from the one used in the most popular cell-based searching algorithms, where a cell is represented as a directed acyclic graph. Each edge here is associated with an operation selected from a predeﬁned operation set. For it to be applicable for all {NAS} algorithms, the search space deﬁned in {NAS}-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 neural cell candidates in total. The training log using the same setup and the performance for each architecture candidate are provided for three datasets. This allows researchers to avoid unnecessary repetitive training for selected architecture and focus solely on the search algorithm itself. The training time saved for every architecture also largely improves the efﬁciency of most {NAS} algorithms and brings a more computational cost friendly {NAS} community for a broader range of researchers. We provide additional diagnostic information such as ﬁne-grained loss and accuracy, which can give inspirations to new designs of {NAS} algorithms. In further support of the proposed {NAS}-Bench201, we have analyzed it from many aspects and benchmarked 10 recent {NAS} algorithms, which verify its applicability.},
  pages    = {16},
  author   = {Dong, Xuanyi and Yang, Yi},
  date     = {2020},
  langid   = {english},
  file     = {Dong and Yang - 2020 - NAS-BENCH-201 EXTENDING THE SCOPE OF RE- PRODUCIB.pdf:/Users/romainegele/Zotero/storage/4ND5WE74/Dong and Yang - 2020 - NAS-BENCH-201 EXTENDING THE SCOPE OF RE- PRODUCIB.pdf:application/pdf}
}

@article{liu_darts_2019,
  title        = {{DARTS}: Differentiable Architecture Search},
  url          = {http://arxiv.org/abs/1806.09055},
  shorttitle   = {{DARTS}},
  abstract     = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on {CIFAR}-10, {ImageNet}, Penn Treebank and {WikiText}-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  journaltitle = {{arXiv}:1806.09055 [cs, stat]},
  author       = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  urldate      = {2020-06-02},
  date         = {2019-04-23},
  eprinttype   = {arxiv},
  eprint       = {1806.09055},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/MHWB8BRV/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/VBAH5JNU/1806.html:text/html}
}

@article{liu_evolving_2020,
  title        = {Evolving Normalization-Activation Layers},
  url          = {http://arxiv.org/abs/2004.02967},
  abstract     = {Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of {EvoNorms}, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that {EvoNorms} not only work well on a variety of image classification models including {ResNets}, {MobileNets} and {EfficientNets} but also transfer well to Mask R-{CNN}, {SpineNet} for instance segmentation and {BigGAN} for image synthesis, significantly outperforming {BatchNorm} and {GroupNorm} based layers in many cases.},
  journaltitle = {{arXiv}:2004.02967 [cs, stat]},
  author       = {Liu, Hanxiao and Brock, Andrew and Simonyan, Karen and Le, Quoc V.},
  urldate      = {2020-06-03},
  date         = {2020-04-28},
  eprinttype   = {arxiv},
  eprint       = {2004.02967},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/EQZKY778/Liu et al. - 2020 - Evolving Normalization-Activation Layers.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/JWIPZQ2K/2004.html:text/html}
}

@article{zheng_rethinking_2020,
  title        = {Rethinking Performance Estimation in Neural Architecture Search},
  url          = {http://arxiv.org/abs/2005.09917},
  abstract     = {Neural architecture search ({NAS}) remains a challenging problem, which is attributed to the indispensable and time-consuming component of performance estimation ({PE}). In this paper, we provide a novel yet systematic rethinking of {PE} in a resource constrained regime, termed budgeted {PE} ({BPE}), which precisely and effectively estimates the performance of an architecture sampled from an architecture space. Since searching an optimal {BPE} is extremely time-consuming as it requires to train a large number of networks for evaluation, we propose a Minimum Importance Pruning ({MIP}) approach. Given a dataset and a {BPE} search space, {MIP} estimates the importance of hyper-parameters using random forest and subsequently prunes the minimum one from the next iteration. In this way, {MIP} effectively prunes less important hyper-parameters to allocate more computational resource on more important ones, thus achieving an effective exploration. By combining {BPE} with various search algorithms including reinforcement learning, evolution algorithm, random search, and differentiable architecture search, we achieve 1, 000x of {NAS} speed up with a negligible performance drop comparing to the {SOTA}},
  journaltitle = {{arXiv}:2005.09917 [cs]},
  author       = {Zheng, Xiawu and Ji, Rongrong and Wang, Qiang and Ye, Qixiang and Li, Zhenguo and Tian, Yonghong and Tian, Qi},
  urldate      = {2020-06-03},
  date         = {2020-05-20},
  eprinttype   = {arxiv},
  eprint       = {2005.09917},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/9JWYXI45/U2RKZ5IE.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/C42AJG47/2005.html:text/html}
}

@article{wang_particle_2020,
  title        = {Particle Swarm Optimisation for Evolving Deep Neural Networks for Image Classification by Evolving and Stacking Transferable Blocks},
  url          = {http://arxiv.org/abs/1907.12659},
  abstract     = {Deep Convolutional Neural Networks ({CNNs}) have been widely used in image classification tasks, but the process of designing {CNN} architectures is very complex, so Neural Architecture Search ({NAS}), automatically searching for optimal {CNN} architectures, has attracted more and more research interests. However, the computational cost of {NAS} is often too high to apply {NAS} on real-life applications. In this paper, an efficient particle swarm optimisation method named {EPSOCNN} is proposed to evolve {CNN} architectures inspired by the idea of transfer learning. {EPSOCNN} successfully reduces the computation cost by minimising the search space to a single block and utilising a small subset of the training set to evaluate {CNNs} during evolutionary process. Meanwhile, {EPSOCNN} also keeps very competitive classification accuracy by stacking the evolved block multiple times to fit the whole dataset. The proposed {EPSOCNN} algorithm is evaluated on {CIFAR}-10 dataset and compared with 13 peer competitors comprised of deep {CNNs} crafted by hand, learned by reinforcement learning methods and evolved by evolutionary computation approaches, which shows very promising results by outperforming all of the peer competitors with regard to the classification accuracy, number of parameters and the computational cost.},
  journaltitle = {{arXiv}:1907.12659 [cs]},
  author       = {Wang, Bin and Xue, Bing and Zhang, Mengjie},
  urldate      = {2020-06-24},
  date         = {2020-03-21},
  eprinttype   = {arxiv},
  eprint       = {1907.12659},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/BQ96I8HM/Wang et al. - 2020 - Particle Swarm Optimisation for Evolving Deep Neur.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/9KD2Y3GK/1907.html:text/html}
}

@article{zela2018towards,
  title   = {Towards automated deep learning: Efficient joint neural architecture and hyperparameter search},
  author  = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
  journal = {arXiv preprint arXiv:1807.06906},
  year    = {2018}
}


@article{byla_deepswarm_2019,
  title        = {{DeepSwarm}: Optimising Convolutional Neural Networks using Swarm Intelligence},
  url          = {http://arxiv.org/abs/1905.07350},
  shorttitle   = {{DeepSwarm}},
  abstract     = {In this paper we propose {DeepSwarm}, a novel neural architecture search ({NAS}) method based on Swarm Intelligence principles. At its core {DeepSwarm} uses Ant Colony Optimization ({ACO}) to generate ant population which uses the pheromone information to collectively search for the best neural architecture. Furthermore, by using local and global pheromone update rules our method ensures the balance between exploitation and exploration. On top of this, to make our method more efficient we combine progressive neural architecture search with weight reusability. Furthermore, due to the nature of {ACO} our method can incorporate heuristic information which can further speed up the search process. After systematic and extensive evaluation, we discover that on three different datasets ({MNIST}, Fashion-{MNIST}, and {CIFAR}-10) when compared to existing systems our proposed method demonstrates competitive performance. Finally, we open source {DeepSwarm} as a {NAS} library and hope it can be used by more deep learning researchers and practitioners.},
  journaltitle = {{arXiv}:1905.07350 [cs, stat]},
  author       = {Byla, Edvinas and Pang, Wei},
  urldate      = {2020-07-02},
  date         = {2019-05-17},
  eprinttype   = {arxiv},
  eprint       = {1905.07350},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, I.2.6},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/N8TYAPSS/Byla and Pang - 2019 - DeepSwarm Optimising Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/MUVYY69B/1905.html:text/html}
}

@article{geada_bonsai-net_2020,
  title        = {Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners},
  url          = {http://arxiv.org/abs/2006.09264},
  shorttitle   = {Bonsai-Net},
  abstract     = {One-shot Neural Architecture Search ({NAS}) aims to minimize the computational expense of discovering state-of-the-art models. However, in the past year attention has been drawn to the comparable performance of naive random search across the same search spaces used by leading {NAS} algorithms. To address this, we explore the effects of drastically relaxing the {NAS} search space, and we present Bonsai-Net, an efficient one-shot {NAS} method to explore our relaxed search space. Bonsai-Net is built around a modified differential pruner and can consistently discover state-of-the-art architectures that are significantly better than random search with fewer parameters than other state-of-the-art methods. Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.},
  journaltitle = {{arXiv}:2006.09264 [cs, stat]},
  author       = {Geada, Rob and Prangle, Dennis and {McGough}, Andrew Stephen},
  urldate      = {2020-07-13},
  date         = {2020-06-12},
  eprinttype   = {arxiv},
  eprint       = {2006.09264},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/QFW3XV9F/Geada et al. - 2020 - Bonsai-Net One-Shot Neural Architecture Search vi.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/2RXMEUHZ/2006.html:text/html}
}

@article{chu_fair_2020,
  title        = {Fair {DARTS}: Eliminating Unfair Advantages in Differentiable Architecture Search},
  url          = {http://arxiv.org/abs/1911.12126},
  shorttitle   = {Fair {DARTS}},
  abstract     = {Differentiable Architecture Search ({DARTS}) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair {DARTS} where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on {CIFAR}-10 and {ImageNet}. Our code is available on https://github.com/xiaomi-automl/fairdarts .},
  journaltitle = {{arXiv}:1911.12126 [cs, stat]},
  author       = {Chu, Xiangxiang and Zhou, Tianbao and Zhang, Bo and Li, Jixiang},
  urldate      = {2020-07-14},
  date         = {2020-03-10},
  eprinttype   = {arxiv},
  eprint       = {1911.12126},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/U4YWNHWR/Chu et al. - 2020 - Fair DARTS Eliminating Unfair Advantages in Diffe.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/NF796DKF/1911.html:text/html}
}

@article{ren_comprehensive_2020,
  title        = {A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions},
  url          = {http://arxiv.org/abs/2006.02903},
  shorttitle   = {A Comprehensive Survey of Neural Architecture Search},
  abstract     = {Deep learning has made major breakthroughs and progress in many fields. This is due to the powerful automatic representation capabilities of deep learning. It has been proved that the design of the network architecture is crucial to the feature representation of data and the final performance. In order to obtain a good feature representation of data, the researchers designed various complex network architectures. However, the design of the network architecture relies heavily on the researchers' prior knowledge and experience. Therefore, a natural idea is to reduce human intervention as much as possible and let the algorithm automatically design the architecture of the network. Thus going further to the strong intelligence. In recent years, a large number of related algorithms for {\textbackslash}textit\{Neural Architecture Search\} ({NAS}) have emerged. They have made various improvements to the {NAS} algorithm, and the related research work is complicated and rich. In order to reduce the difficulty for beginners to conduct {NAS}-related research, a comprehensive and systematic survey on the {NAS} is essential. Previously related surveys began to classify existing work mainly from the basic components of {NAS}: search space, search strategy and evaluation strategy. This classification method is more intuitive, but it is difficult for readers to grasp the challenges and the landmark work in the middle. Therefore, in this survey, we provide a new perspective: starting with an overview of the characteristics of the earliest {NAS} algorithms, summarizing the problems in these early {NAS} algorithms, and then giving solutions for subsequent related research work. In addition, we conducted a detailed and comprehensive analysis, comparison and summary of these works. Finally, we give possible future research directions.},
  journaltitle = {{arXiv}:2006.02903 [cs, stat]},
  author       = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  urldate      = {2020-07-20},
  date         = {2020-06-01},
  eprinttype   = {arxiv},
  eprint       = {2006.02903},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/2LRRU9JS/Ren et al. - 2020 - A Comprehensive Survey of Neural Architecture Sear.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/CK98TXT8/2006.html:text/html}
}

@article{maulik_recurrent_2020,
  title        = {Recurrent Neural Network Architecture Search for Geophysical Emulation},
  url          = {http://arxiv.org/abs/2004.10928},
  abstract     = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. However, constructing neural networks for forecasting such data is nontrivial and often requires trial and error. To that end, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks ({POD}-{LSTMs}). We develop a scalable neural architecture search for generating stacked {LSTMs} to forecast temperature in the {NOAA} Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies {POD}-{LSTMs} that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
  journaltitle = {{arXiv}:2004.10928 [physics]},
  author       = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
  urldate      = {2020-07-20},
  date         = {2020-04-22},
  eprinttype   = {arxiv},
  eprint       = {2004.10928},
  keywords     = {Physics - Computational Physics},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/A8W5XL5Y/Maulik et al. - 2020 - Recurrent Neural Network Architecture Search for G.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/3XR8R2GH/2004.html:text/html}
}

@article{mellor_neural_2020,
  title        = {Neural Architecture Search without Training},
  url          = {http://arxiv.org/abs/2006.04647},
  abstract     = {The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search ({NAS}) techniques to automate this design. However, {NAS} algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the {NAS}-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single {GPU}. Code to reproduce our experiments is available at https://github.com/{BayesWatch}/nas-without-training.},
  journaltitle = {{arXiv}:2006.04647 [cs, stat]},
  author       = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
  urldate      = {2020-07-22},
  date         = {2020-06-08},
  eprinttype   = {arxiv},
  eprint       = {2006.04647},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/FA3HKPSX/Mellor et al. - 2020 - Neural Architecture Search without Training.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/BERZACYM/2006.html:text/html}
}

@article{li_stacnas_2019,
  title        = {{StacNAS}: Towards Stable and Consistent Differentiable Neural Architecture Search},
  url          = {http://arxiv.org/abs/1909.11926},
  shorttitle   = {{StacNAS}},
  abstract     = {Differentiable Neural Architecture Search algorithms such as {DARTS} have attracted much attention due to the low search cost and competitive accuracy. However, it has been observed that {DARTS} can be unstable, especially when applied to new problems. One cause of the instability is the difficulty of two-level optimization. In addition, we identify two other causes: (1) Multicollinearity of correlated/similar operations leads to unpredictable change of the architecture parameters during search; (2) The optimization complexity gap between the proxy search stage and the final training leads to suboptimal architectures. Based on these findings, we propose a two-stage grouped variable pruning algorithm using one-level optimization. In the first stage, the best group is activated, and in the second stage, the best operation in the activated group is selected. Extensive experiments verify the superiority of the proposed method both for accuracy and for stability. For the {DARTS} search space, the proposed strategy obtains state-of-the-art accuracies on {CIFAR}-10, {CIFAR}-100 and {ImageNet}. Code is available at https://github.com/susan0199/stacnas.},
  journaltitle = {{arXiv}:1909.11926 [cs, stat]},
  author       = {Li, Guilin and Zhang, Xing and Wang, Zitong and Li, Zhenguo and Zhang, Tong},
  urldate      = {2020-08-05},
  date         = {2019-12-10},
  eprinttype   = {arxiv},
  eprint       = {1909.11926},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/84ASYP29/Li et al. - 2019 - StacNAS Towards Stable and Consistent Differentia.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/TKVPGIIU/1909.html:text/html}
}

@article{pham_efficient_2018,
  title        = {Efficient Neural Architecture Search via Parameter Sharing},
  url          = {http://arxiv.org/abs/1802.03268},
  abstract     = {We propose Efficient Neural Architecture Search ({ENAS}), a fast and inexpensive approach for automatic model design. In {ENAS}, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, {ENAS} is fast: it delivers strong empirical performances using much fewer {GPU}-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, {ENAS} discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the {CIFAR}-10 dataset, {ENAS} designs novel architectures that achieve a test error of 2.89\%, which is on par with {NASNet} (Zoph et al., 2018), whose test error is 2.65\%.},
  journaltitle = {{arXiv}:1802.03268 [cs, stat]},
  author       = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  urldate      = {2020-08-25},
  date         = {2018-02-11},
  eprinttype   = {arxiv},
  eprint       = {1802.03268},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/3C6RNXEA/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/742YH2EK/1802.html:text/html}
}

@article{nguyen_optimal_2020,
  title        = {Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search},
  url          = {http://arxiv.org/abs/2006.07593},
  abstract     = {Neural architecture search ({NAS}) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport ({OT}) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the {OT} is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein ({TW}), which is a negative definite variant of {OT}, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential {NAS} settings. Furthermore, we derive a novel parallel {NAS}, using quality k-determinantal point process on the {GP} posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our {TW}-based approaches outperform other baselines in both sequential and parallel {NAS}.},
  journaltitle = {{arXiv}:2006.07593 [cs, stat]},
  author       = {Nguyen, Vu and Le, Tam and Yamada, Makoto and Osborne, Michael A.},
  urldate      = {2020-10-07},
  date         = {2020-06-13},
  eprinttype   = {arxiv},
  eprint       = {2006.07593},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/H7QN9ZST/Nguyen et al. - 2020 - Optimal Transport Kernels for Sequential and Paral.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/PYVQG8G9/2006.html:text/html}
}

@article{awad_differential_2020,
  title        = {Differential Evolution for Neural Architecture Search},
  url          = {http://arxiv.org/abs/2012.06400},
  abstract     = {Neural architecture search ({NAS}) methods rely on a search strategy for deciding which architectures to evaluate next and a performance estimation strategy for assessing their performance (e.g., using full evaluations, multi-fidelity evaluations, or the one-shot model). In this paper, we focus on the search strategy. We introduce the simple yet powerful evolutionary algorithm of differential evolution to the {NAS} community. Using the simplest performance evaluation strategy of full evaluations, we comprehensively compare this search strategy to regularized evolution and Bayesian optimization and demonstrate that it yields improved and more robust results for 13 tabular {NAS} benchmarks based on {NAS}-Bench-101, {NAS}-Bench-1Shot1, {NAS}-Bench-201 and {NAS}-{HPO} bench.},
  journaltitle = {{arXiv}:2012.06400 [cs]},
  author       = {Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
  urldate      = {2021-01-12},
  date         = {2020-12-11},
  eprinttype   = {arxiv},
  eprint       = {2012.06400},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/QI8FTUCW/Awad et al. - 2020 - Differential Evolution for Neural Architecture Sea.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/CHELWTU4/2012.html:text/html}
}

@article{williams_function_1991,
  title        = {Function Optimization using Connectionist Reinforcement Learning Algorithms},
  volume       = {3},
  issn         = {0954-0091, 1360-0494},
  url          = {https://www.tandfonline.com/doi/full/10.1080/09540099108946587},
  doi          = {10.1080/09540099108946587},
  abstract     = {We explore the design decisions involved in reducing neural network architecture search ({NAS}) to a reinforcement learning ({RL}) problem. We compare several reductions on the {NAS}-Bench-101 dataset, while holding the {RL} algorithm and search space constant. Based on our ﬁndings, we discuss how {NAS} differs from typical {RL} settings, and suggest guidelines for applying {RL} to {NAS} problems.},
  pages        = {241--268},
  number       = {3},
  journaltitle = {Connection Science},
  shortjournal = {Connection Science},
  author       = {Williams, Ronald J. and Peng, Jing},
  urldate      = {2021-03-13},
  date         = {1991-01},
  langid       = {english},
  file         = {Williams and Peng - 1991 - Function Optimization using Connectionist Reinforc.pdf:/Users/romainegele/Zotero/storage/ENU8ZXIM/Williams and Peng - 1991 - Function Optimization using Connectionist Reinforc.pdf:application/pdf}
}

@article{erickson_autogluon-tabular_2020,
  title        = {{AutoGluon}-Tabular: Robust and Accurate {AutoML} for Structured Data},
  url          = {http://arxiv.org/abs/2003.06505},
  shorttitle   = {{AutoGluon}-Tabular},
  abstract     = {We introduce {AutoGluon}-Tabular, an open-source {AutoML} framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a {CSV} file. Unlike existing {AutoML} frameworks that primarily focus on model/hyperparameter selection, {AutoGluon}-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial {AutoML} platforms including {TPOT}, H2O, {AutoWEKA}, auto-sklearn, {AutoGluon}, and Google {AutoML} Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the {OpenML} {AutoML} Benchmark reveal that {AutoGluon} is faster, more robust, and much more accurate. We find that {AutoGluon} often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, {AutoGluon} beat 99\% of the participating data scientists after merely 4h of training on the raw data.},
  journaltitle = {{arXiv}:2003.06505 [cs, stat]},
  author       = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  urldate      = {2020-07-20},
  date         = {2020-03-13},
  eprinttype   = {arxiv},
  eprint       = {2003.06505},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/AMUEVT6Q/Erickson et al. - 2020 - AutoGluon-Tabular Robust and Accurate AutoML for .pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/UHBRXXB8/2003.html:text/html}
}

@article{djallel_bouneffouf_survey_2020,
  title    = {Survey on Automated End-to-End Data Science?},
  url      = {http://rgdoi.net/10.13140/RG.2.2.14681.34405},
  doi      = {10.13140/RG.2.2.14681.34405},
  abstract = {Data science is labor-intensive and human experts are scarce but heavily involved in every aspect of it. This makes data science time consuming and restricted to experts with the resulting quality heavily dependent on their experience and skills. To make data science more accessible and scalable, we need its democratization. Automated Data Science ({AutoDS}) is aimed towards that goal and is emerging as an important research and business topic. We introduce and deﬁne the {AutoDS} challenge, followed by a proposal of a general {AutoDS} framework that covers existing approaches but also provides guidance for the development of new methods. We categorize and review the existing literature from multiple aspects of the problem setup and employed techniques. Then we provide several views on how {AI} could succeed in automating endto-end {AutoDS}. We hope this survey can serve as insightful guideline for the {AutoDS} ﬁeld and provide inspiration for future research.},
  author   = {Djallel Bouneffouf and Aggarwal, Charu and Samulowitz, Horst and Buesser, Beat and Hoang, Thanh and Udayan Khurana and Sijia Liu and Tejaswini Pedapati and Parikshit Ram and Ambrish Rawat and Wistuba, Martin and Gray, Alexander},
  urldate  = {2020-07-31},
  date     = {2020},
  langid   = {english},
  note     = {Publisher: Unpublished},
  file     = {Djallel Bouneffouf et al. - 2020 - Survey on Automated End-to-End Data Science.pdf:/Users/romainegele/Zotero/storage/QUZ4MRDT/Djallel Bouneffouf et al. - 2020 - Survey on Automated End-to-End Data Science.pdf:application/pdf}
}

@article{gijsbers_open_nodate,
  title    = {An Open Source {AutoML} Benchmark},
  abstract = {In recent years, an active ﬁeld of research has developed around automated machine learning ({AutoML}). Unfortunately, comparing diﬀerent {AutoML} systems is hard and often done incorrectly. We introduce an open, ongoing, and extensible benchmark framework which follows best practices and avoids common mistakes. The framework is open-source, uses public datasets and has a website with up-to-date results. We use the framework to conduct a thorough comparison of 4 {AutoML} systems across 39 datasets and analyze the results.},
  pages    = {8},
  author   = {Gijsbers, Pieter and {LeDell}, Erin and Thomas, Janek and Poirier, Sebastien and Bischl, Bernd and Vanschoren, Joaquin},
  langid   = {english},
  file     = {Gijsbers et al. - An Open Source AutoML Benchmark.pdf:/Users/romainegele/Zotero/storage/MFTTE4R6/Gijsbers et al. - An Open Source AutoML Benchmark.pdf:application/pdf}
}

@article{zimmer_auto-pytorch_2020,
  title        = {Auto-{PyTorch} Tabular: Multi-Fidelity {MetaLearning} for Efficient and Robust {AutoDL}},
  url          = {http://arxiv.org/abs/2006.13799},
  shorttitle   = {Auto-{PyTorch} Tabular},
  abstract     = {While early {AutoML} frameworks focused on optimizing traditional {ML} pipelines and their hyperparameters, a recent trend in {AutoML} is to focus on neural architecture search. In this paper, we introduce Auto-{PyTorch}, which brings the best of these two worlds together by jointly and robustly optimizing the architecture of networks and the training hyperparameters to enable fully automated deep learning ({AutoDL}). Auto-{PyTorch} achieves state-of-the-art performance on several tabular benchmarks by combining multi-fidelity optimization with portfolio construction for warmstarting and ensembling of deep neural networks ({DNNs}) and common baselines for tabular data. To thoroughly study our assumptions on how to design such an {AutoDL} system, we additionally introduce a new benchmark on learning curves for {DNNs}, dubbed {LCBench}, and run extensive ablation studies of the full Auto-{PyTorch} on typical {AutoML} benchmarks, eventually showing that Auto-{PyTorch} performs better than several state-of-the-art competitors on average.},
  journaltitle = {{arXiv}:2006.13799 [cs, stat]},
  author       = {Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  urldate      = {2020-08-11},
  date         = {2020-06-24},
  eprinttype   = {arxiv},
  eprint       = {2006.13799},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/PQEXBWM2/Zimmer et al. - 2020 - Auto-PyTorch Tabular Multi-Fidelity MetaLearning .pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/LN2PYJFF/2006.html:text/html}
}


@inproceedings{thornton2013auto,
  title     = {Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
  author    = {Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages     = {847--855},
  year      = {2013}
}

@article{geman_neural_nodate,
  title  = {Neural Networks and the Bias Variance Dilemma},
  pages  = {58},
  author = {Geman, Stuart and Bienenstock, Elie and Doursat, {RenC}},
  langid = {english},
  file   = {Geman et al. - Neural Networks and the BiadVariance Dilemma.pdf:/Users/romainegele/Zotero/storage/Q9JUJTT2/Geman et al. - Neural Networks and the BiadVariance Dilemma.pdf:application/pdf}
}

@article{sun_survey_2019,
  title        = {A Survey of Optimization Methods from a Machine Learning Perspective},
  url          = {http://arxiv.org/abs/1906.06821},
  abstract     = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various ﬁelds. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great signiﬁcance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we ﬁrst describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning ﬁelds. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
  journaltitle = {{arXiv}:1906.06821 [cs, math, stat]},
  author       = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  urldate      = {2021-01-14},
  date         = {2019-10-23},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1906.06821},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
  file         = {Sun et al. - 2019 - A Survey of Optimization Methods from a Machine Le.pdf:/Users/romainegele/Zotero/storage/UFWWY69L/Sun et al. - 2019 - A Survey of Optimization Methods from a Machine Le.pdf:application/pdf}
}

@article{virmaux_lipschitz_nodate,
  title    = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
  abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is {NP}-hard and state-ofart methods may signiﬁcantly overestimate it. Then, we both extend and improve previous estimation methods by providing {AutoLip}, the ﬁrst generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efﬁcient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named {SeqLip} that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on {SeqLip} in order to tackle very large networks. Our experiments show that {SeqLip} can signiﬁcantly improve on the existing upper bounds. Finally, we provide an implementation of {AutoLip} in the {PyTorch} environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
  pages    = {10},
  author   = {Virmaux, Aladin and Scaman, Kevin},
  langid   = {english},
  file     = {Virmaux and Scaman - Lipschitz regularity of deep neural networks anal.pdf:/Users/romainegele/Zotero/storage/VF2QHC3I/Virmaux and Scaman - Lipschitz regularity of deep neural networks anal.pdf:application/pdf}
}

@article{zhu_unpaired_2018,
  title        = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  url          = {http://arxiv.org/abs/1703.10593},
  abstract     = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  journaltitle = {{arXiv}:1703.10593 [cs]},
  author       = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  urldate      = {2020-07-13},
  date         = {2018-11-15},
  eprinttype   = {arxiv},
  eprint       = {1703.10593},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/934JHMDA/Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/Y8HTRCRM/1703.html:text/html}
}

@article{park_specaugment_2019,
  title        = {{SpecAugment} on Large Scale Datasets},
  url          = {http://arxiv.org/abs/1912.05533},
  abstract     = {Recently, {SpecAugment}, an augmentation scheme for automatic speech recognition that acts directly on the spectrogram of input utterances, has shown to be highly effective in enhancing the performance of end-to-end networks on public datasets. In this paper, we demonstrate its effectiveness on tasks with large scale datasets by investigating its application to the Google Multidomain Dataset (Narayanan et al., 2018). We achieve improvement across all test domains by mixing raw training data augmented with {SpecAugment} and noise-perturbed training data when training the acoustic model. We also introduce a modification of {SpecAugment} that adapts the time mask size and/or multiplicity depending on the length of the utterance, which can potentially benefit large scale tasks. By using adaptive masking, we are able to further improve the performance of the Listen, Attend and Spell model on {LibriSpeech} to 2.2\% {WER} on test-clean and 5.2\% {WER} on test-other.},
  journaltitle = {{arXiv}:1912.05533 [cs, eess]},
  author       = {Park, Daniel S. and Zhang, Yu and Chiu, Chung-Cheng and Chen, Youzheng and Li, Bo and Chan, William and Le, Quoc V. and Wu, Yonghui},
  urldate      = {2020-07-13},
  date         = {2019-12-11},
  eprinttype   = {arxiv},
  eprint       = {1912.05533},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/92GIUV95/Park et al. - 2019 - SpecAugment on Large Scale Datasets.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/FY2PCFBE/1912.html:text/html}
}

@article{cubuk_autoaugment_2019,
  title        = {{AutoAugment}: Learning Augmentation Policies from Data},
  url          = {http://arxiv.org/abs/1805.09501},
  shorttitle   = {{AutoAugment}},
  abstract     = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called {AutoAugment} to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on {CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet} (without additional data). On {ImageNet}, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On {CIFAR}-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on {ImageNet} transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-{IIT} Pets, {FGVC} Aircraft, and Stanford Cars.},
  journaltitle = {{arXiv}:1805.09501 [cs, stat]},
  author       = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  urldate      = {2020-07-13},
  date         = {2019-04-11},
  eprinttype   = {arxiv},
  eprint       = {1805.09501},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/L7AF2TZS/Cubuk et al. - 2019 - AutoAugment Learning Augmentation Policies from D.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/G4IPA6WV/1805.html:text/html}
}

@article{lim_fast_2019,
  title        = {Fast {AutoAugment}},
  url          = {http://arxiv.org/abs/1905.00397},
  abstract     = {Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, {AutoAugment} has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of {GPU} hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast {AutoAugment} that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to {AutoAugment}, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including {CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet}.},
  journaltitle = {{arXiv}:1905.00397 [cs, stat]},
  author       = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  urldate      = {2020-07-13},
  date         = {2019-05-25},
  eprinttype   = {arxiv},
  eprint       = {1905.00397},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/39KNR973/Lim et al. - 2019 - Fast AutoAugment.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/EGU6V78E/1905.html:text/html}
}

@article{brochu_tutorial_2010,
  title        = {A Tutorial on {Bayesian} Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  url          = {http://arxiv.org/abs/1012.2599},
  abstract     = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
  journaltitle = {{arXiv}:1012.2599 [cs]},
  author       = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  urldate      = {2020-07-08},
  date         = {2010-12-12},
  eprinttype   = {arxiv},
  eprint       = {1012.2599},
  keywords     = {Computer Science - Machine Learning, I.2.6, G.1.6, G.3},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/EU9CUIIJ/FN7SB2EH.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/9BWIG83J/1012.html:text/html}
}

@article{guyon_introduction_nodate,
  title    = {An Introduction to Variable and Feature Selection},
  abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better deﬁnition of the objective function, feature construction, feature ranking, multivariate feature selection, efﬁcient search methods, and feature validity assessment methods.},
  pages    = {26},
  author   = {Guyon, Isabelle and Elisseeff, Andre},
  langid   = {english},
  file     = {Guyon and Elisseeff - An Introduction to Variable and Feature Selection.pdf:/Users/romainegele/Zotero/storage/WJRJ9K4U/Guyon and Elisseeff - An Introduction to Variable and Feature Selection.pdf:application/pdf}
}

@article{coleman_dawnbench_nodate,
  title    = {{DAWNBench}: An End-to-End Deep Learning Benchmark and Competition},
  abstract = {Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce {DAWNBench}, a benchmark and competition focused on end-to-end training time to achieve a state-of-the-art accuracy level, as well as inference time with that accuracy. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-{GPU} training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe {DAWNBench} will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.},
  pages    = {10},
  author   = {Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and Ré, Chris and Zaharia, Matei},
  langid   = {english},
  file     = {Coleman et al. - DAWNBench An End-to-End Deep Learning Benchmark a.pdf:/Users/romainegele/Zotero/storage/URXEQDAG/Coleman et al. - DAWNBench An End-to-End Deep Learning Benchmark a.pdf:application/pdf}
}

@article{goyal_accurate_2018,
  title        = {Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
  url          = {http://arxiv.org/abs/1706.02677},
  shorttitle   = {Accurate, Large Minibatch {SGD}},
  abstract     = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous {SGD} offers a potential solution to this problem by dividing {SGD} minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the {SGD} minibatch size. In this paper, we empirically show that on the {ImageNet} dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains {ResNet}-50 with a minibatch size of 8192 on 256 {GPUs} in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 {GPUs}. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  journaltitle = {{arXiv}:1706.02677 [cs]},
  author       = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  urldate      = {2020-05-21},
  date         = {2018-04-30},
  eprinttype   = {arxiv},
  eprint       = {1706.02677},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/P9UGZIXT/Goyal et al. - 2018 - Accurate, Large Minibatch SGD Training ImageNet i.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/4S6NGTNU/1706.html:text/html}
}

@article{sergeev_horovod_2018,
  title        = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  url          = {http://arxiv.org/abs/1802.05799},
  shorttitle   = {Horovod},
  abstract     = {Training modern deep learning models requires large amounts of computation, often provided by {GPUs}. Scaling computation from one {GPU} to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-{GPU} communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-{GPU} communication. Depending on the training library's {API}, the modification required may be either significant or minimal. Existing methods for enabling multi-{GPU} training under the {TensorFlow} library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-{GPU} training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-{GPU} communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in {TensorFlow}. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
  journaltitle = {{arXiv}:1802.05799 [cs, stat]},
  author       = {Sergeev, Alexander and Del Balso, Mike},
  urldate      = {2020-06-24},
  date         = {2018-02-20},
  eprinttype   = {arxiv},
  eprint       = {1802.05799},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/T8GHZIKB/Sergeev and Del Balso - 2018 - Horovod fast and easy distributed deep learning i.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/B9CBRGF3/1802.html:text/html}
}

@article{devarakonda_adabatch_2018,
  title        = {{AdaBatch}: Adaptive Batch Sizes for Training Deep Neural Networks},
  url          = {http://arxiv.org/abs/1712.02029},
  shorttitle   = {{AdaBatch}},
  abstract     = {Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard {AlexNet}, {ResNet}, and {VGG} networks operating on the popular {CIFAR}-10, {CIFAR}-100, and {ImageNet} datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 {NVIDIA} Tesla P100 {GPUs} while changing accuracy by less than 1\% relative to training with fixed batch sizes.},
  journaltitle = {{arXiv}:1712.02029 [cs, stat]},
  author       = {Devarakonda, Aditya and Naumov, Maxim and Garland, Michael},
  urldate      = {2020-06-30},
  date         = {2018-02-13},
  eprinttype   = {arxiv},
  eprint       = {1712.02029},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.6, 68T05,, I.5.0},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/4VIMGBFS/VCZ64JJR.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/VWV8EALZ/1712.html:text/html}
}

@article{maleki_scaling_2020,
  title        = {Scaling Distributed Training with Adaptive Summation},
  url          = {http://arxiv.org/abs/2006.02924},
  abstract     = {Stochastic gradient descent ({SGD}) is an inherently sequential training algorithm--computing the gradient at batch \$i\$ depends on the model parameters learned from batch \$i-1\$. Prior approaches that break this dependence do not honor them (e.g., sum the gradients for each batch, which is not what sequential {SGD} would do) and thus potentially suffer from poor convergence. This paper introduces a novel method to combine gradients called Adasum (for adaptive sum) that converges faster than prior work. Adasum is easy to implement, almost as efficient as simply summing gradients, and is integrated into the open-source toolkit Horovod. This paper first provides a formal justification for Adasum and then empirically demonstrates Adasum is more accurate than prior gradient accumulation methods. It then introduces a series of case-studies to show Adasum works with multiple frameworks, ({TensorFlow} and {PyTorch}), scales multiple optimizers (Momentum-{SGD}, Adam, and {LAMB}) to larger batch-sizes while still giving good downstream accuracy. Finally, it proves that Adasum converges. To summarize, Adasum scales Momentum-{SGD} on the {MLPerf} Resnet50 benchmark to 64K examples before communication (no {MLPerf} v0.5 entry converged with more than 16K), the Adam optimizer to 64K examples before communication on {BERT}-{LARGE} (prior work showed Adam stopped scaling at 16K), and the {LAMB} optimizer to 128K before communication on {BERT}-{LARGE} (prior work used 64K), all while maintaining downstream accuracy metrics. Finally, if a user does not need to scale, we show {LAMB} with Adasum on {BERT}-{LARGE} converges in 30\% fewer steps than the baseline.},
  journaltitle = {{arXiv}:2006.02924 [cs]},
  author       = {Maleki, Saeed and Musuvathi, Madan and Mytkowicz, Todd and Saarikivi, Olli and Xu, Tianju and Eksarevskiy, Vadim and Ekanayake, Jaliya and Barsoum, Emad},
  urldate      = {2020-07-01},
  date         = {2020-06-04},
  eprinttype   = {arxiv},
  eprint       = {2006.02924},
  keywords     = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/Z6AIUGBK/Maleki et al. - 2020 - Scaling Distributed Training with Adaptive Summati.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/V4D3XG3E/2006.html:text/html}
}

@article{laanait_exascale_2019,
  title        = {Exascale Deep Learning for Scientific Inverse Problems},
  url          = {http://arxiv.org/abs/1909.11150},
  abstract     = {We introduce novel communication strategies in synchronous distributed Deep Learning consisting of decentralized gradient reduction orchestration and computational graph-aware grouping of gradient tensors. These new techniques produce an optimal overlap between computation and communication and result in near-linear scaling (0.93) of distributed training up to 27,600 {NVIDIA} V100 {GPUs} on the Summit Supercomputer. We demonstrate our gradient reduction techniques in the context of training a Fully Convolutional Neural Network to approximate the solution of a longstanding scientific inverse problem in materials imaging. The efficient distributed training on a dataset size of 0.5 {PB}, produces a model capable of an atomically-accurate reconstruction of materials, and in the process reaching a peak performance of 2.15(4) {EFLOPS}\$\_\{16\}\$.},
  journaltitle = {{arXiv}:1909.11150 [cond-mat, physics:physics, stat]},
  author       = {Laanait, Nouamane and Romero, Joshua and Yin, Junqi and Young, M. Todd and Treichler, Sean and Starchenko, Vitalii and Borisevich, Albina and Sergeev, Alex and Matheson, Michael},
  urldate      = {2020-07-01},
  date         = {2019-09-24},
  eprinttype   = {arxiv},
  eprint       = {1909.11150},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Condensed Matter - Materials Science, Physics - Computational Physics},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/9ETFVLHY/Laanait et al. - 2019 - Exascale Deep Learning for Scientific Inverse Prob.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/9WQM36D8/1909.html:text/html}
}

@article{duvenaud_convolutional_2015,
  title        = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
  url          = {http://arxiv.org/abs/1509.09292},
  abstract     = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular ﬁngerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  journaltitle = {{arXiv}:1509.09292 [cs, stat]},
  author       = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
  urldate      = {2021-03-19},
  date         = {2015-11-03},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1509.09292},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  file         = {Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:/Users/romainegele/Zotero/storage/57NBVDGY/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf}
}

@article{wu_moleculenet_2018,
  title        = {{MoleculeNet}: A Benchmark for Molecular Machine Learning},
  url          = {http://arxiv.org/abs/1703.00564},
  shorttitle   = {{MoleculeNet}},
  abstract     = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces {MoleculeNet}, a large scale benchmark for molecular machine learning. {MoleculeNet} curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the {DeepChem} open source library). {MoleculeNet} benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
  journaltitle = {{arXiv}:1703.00564 [physics, stat]},
  author       = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
  urldate      = {2021-03-19},
  date         = {2018-10-25},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1703.00564},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Chemical Physics},
  file         = {Wu et al. - 2018 - MoleculeNet A Benchmark for Molecular Machine Lea.pdf:/Users/romainegele/Zotero/storage/8V3KU899/Wu et al. - 2018 - MoleculeNet A Benchmark for Molecular Machine Lea.pdf:application/pdf}
}

@article{jiang_graph_2020,
  title        = {Graph Neural Network Architecture Search for Molecular Property Prediction},
  url          = {http://arxiv.org/abs/2008.12187},
  abstract     = {Predicting the properties of a molecule from its structure is a challenging task. Recently, deep learning methods have improved the state of the art for this task because of their ability to learn useful features from the given data. By treating molecule structure as graphs, where atoms and bonds are modeled as nodes and edges, graph neural networks ({GNNs}) have been widely used to predict molecular properties. However, the design and development of {GNNs} for a given dataset rely on labor-intensive design and tuning of the network architectures. Neural architecture search ({NAS}) is a promising approach to discover high-performing neural network architectures automatically. To that end, we develop an {NAS} approach to automate the design and development of {GNNs} for molecular property prediction. Speciﬁcally, we focus on automated development of message-passing neural networks ({MPNNs}) to predict the molecular properties of small molecules in quantum mechanics and physical chemistry datasets from the {MoleculeNet} benchmark. We demonstrate the superiority of the automatically discovered {MPNNs} by comparing them with manually designed {GNNs} from the {MoleculeNet} benchmark. We study the relative importance of the choices in the {MPNN} search space, demonstrating that customizing the architecture is critical to enhancing performance in molecular property prediction and that the proposed approach can perform customization automatically with minimal manual effort.},
  journaltitle = {{arXiv}:2008.12187 [cs, q-bio, stat]},
  author       = {Jiang, Shengli and Balaprakash, Prasanna},
  urldate      = {2020-11-03},
  date         = {2020-08-27},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2008.12187},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Biomolecules},
  file         = {Jiang and Balaprakash - 2020 - Graph Neural Network Architecture Search for Molec.pdf:/Users/romainegele/Zotero/storage/23HLI99F/Jiang and Balaprakash - 2020 - Graph Neural Network Architecture Search for Molec.pdf:application/pdf}
}

@article{salim_balsam_2019,
  title        = {Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive {HPC} Workflows},
  url          = {http://arxiv.org/abs/1909.08704},
  shorttitle   = {Balsam},
  abstract     = {We introduce the Balsam service to manage high-throughput task scheduling and execution on supercomputing systems. Balsam allows users to populate a task database with a variety of tasks ranging from simple independent tasks to dynamic multi-task workflows. With abstractions for the local resource scheduler and {MPI} environment, Balsam dynamically packages tasks into ensemble jobs and manages their scheduling lifecycle. The ensembles execute in a pilot "launcher" which (i) ensures concurrent, load-balanced execution of arbitrary serial and parallel programs with heterogeneous processor requirements, (ii) requires no modification of user applications, (iii) is tolerant of task-level faults and provides several options for error recovery, (iv) stores provenance data (e.g task history, error logs) in the database, (v) supports dynamic workflows, in which tasks are created or killed at runtime. Here, we present the design and Python implementation of the Balsam service and launcher. The efficacy of this system is illustrated using two case studies: hyperparameter optimization of deep neural networks, and high-throughput single-point quantum chemistry calculations. We find that the unique combination of flexible job-packing and automated scheduling with dynamic (pilot-managed) execution facilitates excellent resource utilization. The scripting overheads typically needed to manage resources and launch workflows on supercomputers are substantially reduced, accelerating workflow development and execution.},
  journaltitle = {{arXiv}:1909.08704 [cs]},
  author       = {Salim, Michael A. and Uram, Thomas D. and Childers, J. Taylor and Balaprakash, Prasanna and Vishwanath, Venkatram and Papka, Michael E.},
  urldate      = {2020-08-13},
  date         = {2019-09-18},
  eprinttype   = {arxiv},
  eprint       = {1909.08704},
  keywords     = {Computer Science - Distributed, Parallel, and Cluster Computing},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/AZ7HLSLW/Salim et al. - 2019 - Balsam Automated Scheduling and Execution of Dyna.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/FKDFQI9R/1909.html:text/html}
}

@article{dong_neural_2019,
  title        = {Neural Logic Machines},
  url          = {http://arxiv.org/abs/1904.11694},
  abstract     = {We propose the Neural Logic Machine ({NLM}), a neural-symbolic architecture for both inductive learning and logic reasoning. {NLMs} exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), {NLMs} can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, {NLMs} achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.},
  journaltitle = {{arXiv}:1904.11694 [cs, stat]},
  author       = {Dong, Honghua and Mao, Jiayuan and Lin, Tian and Wang, Chong and Li, Lihong and Zhou, Denny},
  urldate      = {2019-05-13},
  date         = {2019-04-26},
  eprinttype   = {arxiv},
  eprint       = {1904.11694},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1904.11694 PDF:/Users/romainegele/Zotero/storage/KZLR86Y8/Dong et al. - 2019 - Neural Logic Machines.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/5GNL7UVU/1904.html:text/html}
}

@article{yu_dag-gnn_2019,
  title        = {{DAG}-{GNN}: {DAG} Structure Learning with Graph Neural Networks},
  url          = {http://arxiv.org/abs/1904.10098},
  shorttitle   = {{DAG}-{GNN}},
  abstract     = {Learning a faithful directed acyclic graph ({DAG}) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model ({SEM}) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the {DAG}. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin {DAG}-{GNN}. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at {\textbackslash}url\{https://github.com/fishmoon1234/{DAG}-{GNN}\}.},
  journaltitle = {{arXiv}:1904.10098 [cs, stat]},
  author       = {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
  urldate      = {2020-06-04},
  date         = {2019-04-22},
  eprinttype   = {arxiv},
  eprint       = {1904.10098},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/BQ8SVKCJ/Yu et al. - 2019 - DAG-GNN DAG Structure Learning with Graph Neural .pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/BDRIJ76D/1904.html:text/html}
}

@article{cappart_combining_2020,
  title        = {Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization},
  url          = {http://arxiv.org/abs/2006.01610},
  abstract     = {Combinatorial optimization has found applications in numerous fields, from aerospace to transportation planning and economics. The goal is to find an optimal solution among a finite set of possibilities. The well-known challenge one faces with combinatorial optimization is the state-space explosion problem: the number of possibilities grows exponentially with the problem size, which makes solving intractable for large problems. In the last years, deep reinforcement learning ({DRL}) has shown its promise for designing good heuristics dedicated to solve {NP}-hard combinatorial optimization problems. However, current approaches have two shortcomings: (1) they mainly focus on the standard travelling salesman problem and they cannot be easily extended to other problems, and (2) they only provide an approximate solution with no systematic ways to improve it or to prove optimality. In another context, constraint programming ({CP}) is a generic tool to solve combinatorial optimization problems. Based on a complete search procedure, it will always find the optimal solution if we allow an execution time large enough. A critical design choice, that makes {CP} non-trivial to use in practice, is the branching decision, directing how the search space is explored. In this work, we propose a general and hybrid approach, based on {DRL} and {CP}, for solving combinatorial optimization problems. The core of our approach is based on a dynamic programming formulation, that acts as a bridge between both techniques. We experimentally show that our solver is efficient to solve two challenging problems: the traveling salesman problem with time windows, and the 4-moments portfolio optimization problem. Results obtained show that the framework introduced outperforms the stand-alone {RL} and {CP} solutions, while being competitive with industrial solvers.},
  journaltitle = {{arXiv}:2006.01610 [cs]},
  author       = {Cappart, Quentin and Moisan, Thierry and Rousseau, Louis-Martin and Prémont-Schwarz, Isabeau and Cire, Andre},
  urldate      = {2020-06-05},
  date         = {2020-06-02},
  eprinttype   = {arxiv},
  eprint       = {2006.01610},
  keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/ZNSYIJJX/Cappart et al. - 2020 - Combining Reinforcement Learning and Constraint Pr.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/M3R7W8D7/2006.html:text/html}
}

@article{veness_gated_2020,
  title        = {Gated Linear Networks},
  url          = {http://arxiv.org/abs/1910.01526},
  abstract     = {This paper presents a new family of backpropagation-free neural architectures, Gated Linear Networks ({GLNs}). What distinguishes {GLNs} from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. We show that this architecture gives rise to universal learning capabilities in the limit, with effective model capacity increasing as a function of network size in a manner comparable with deep {ReLU} networks. Furthermore, we demonstrate that the {GLN} learning mechanism possesses extraordinary resilience to catastrophic forgetting, performing comparably to a {MLP} with dropout and Elastic Weight Consolidation on standard benchmarks. These desirable theoretical and empirical properties position {GLNs} as a complementary technique to contemporary offline deep learning methods.},
  journaltitle = {{arXiv}:1910.01526 [cs, math, stat]},
  author       = {Veness, Joel and Lattimore, Tor and Budden, David and Bhoopchand, Avishkar and Mattern, Christopher and Grabska-Barwinska, Agnieszka and Sezener, Eren and Wang, Jianan and Toth, Peter and Schmitt, Simon and Hutter, Marcus},
  urldate      = {2020-07-29},
  date         = {2020-06-11},
  eprinttype   = {arxiv},
  eprint       = {1910.01526},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/KX6IR562/Veness et al. - 2020 - Gated Linear Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/QCS2H7E6/1910.html:text/html}
}

@article{krishnan_meta_2020,
  title        = {Meta Continual Learning via Dynamic Programming},
  url          = {http://arxiv.org/abs/2008.02219},
  abstract     = {Meta-continual learning algorithms seek to rapidly train a model when faced with similar tasks sampled sequentially from a task distribution. Although impressive strides have been made in this area, there is no theoretical framework that enables systematic analysis of key learning challenges, such as generalization and catastrophic forgetting. We introduce a new theoretical framework for meta-continual learning using dynamic programming, analyze generalization and catastrophic forgetting, and establish conditions of optimality. We show that existing meta-continual learning methods can be derived from the proposed dynamic programming framework. Moreover, we develop a new dynamic-programming-based meta-continual approach that adopts stochastic-gradient-driven alternating optimization method. We show that, on meta-continual learning benchmark data sets, our theoretically grounded meta-continual learning approach is better than or comparable to the purely empirical strategies adopted by the existing state-of-the-art methods.},
  journaltitle = {{arXiv}:2008.02219 [cs, eess, stat]},
  author       = {Krishnan, R. and Balaprakash, Prasanna},
  urldate      = {2020-08-11},
  date         = {2020-08-05},
  eprinttype   = {arxiv},
  eprint       = {2008.02219},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/NNXSCCW8/Krishnan and Balaprakash - 2020 - Meta Continual Learning via Dynamic Programming.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/BL5EXTYD/2008.html:text/html}
}

@article{lindauer_boah_2019,
  title        = {{BOAH}: A Tool Suite for Multi-Fidelity Bayesian Optimization \& Analysis of Hyperparameters},
  url          = {http://arxiv.org/abs/1908.06756},
  shorttitle   = {{BOAH}},
  abstract     = {Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and {HyperBand}, and a comprehensive analysis of the optimization process and its outcomes.},
  journaltitle = {{arXiv}:1908.06756 [cs, stat]},
  author       = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, André and Marben, Joshua and Müller, Philipp and Hutter, Frank},
  urldate      = {2020-09-15},
  date         = {2019-08-16},
  eprinttype   = {arxiv},
  eprint       = {1908.06756},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/IVDA56ZA/Lindauer et al. - 2019 - BOAH A Tool Suite for Multi-Fidelity Bayesian Opt.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/M59Y62NU/1908.html:text/html}
}

@incollection{poole_exponential_2016,
  title     = {Exponential expressivity in deep neural networks through transient chaos},
  url       = {http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos.pdf},
  pages     = {3360--3368},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  author    = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  editor    = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  urldate   = {2020-10-05},
  date      = {2016},
  file      = {NIPS Full Text PDF:/Users/romainegele/Zotero/storage/PLF88VM9/Poole et al. - 2016 - Exponential expressivity in deep neural networks t.pdf:application/pdf;NIPS Snapshot:/Users/romainegele/Zotero/storage/BIDIWDKQ/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos.html:text/html}
}

@article{liu_energy-based_2020,
  title        = {Energy-based Out-of-distribution Detection},
  url          = {http://arxiv.org/abs/2010.03759},
  abstract     = {Determining whether inputs are out-of-distribution ({OOD}) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for {OOD} data. We propose a unified framework for {OOD} detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for {OOD} detection. On a {CIFAR}-10 pre-trained {WideResNet}, using the energy score reduces the average {FPR} (at {TPR} 95\%) by 18.03\% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.},
  journaltitle = {{arXiv}:2010.03759 [cs]},
  author       = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
  urldate      = {2020-10-10},
  date         = {2020-10-08},
  eprinttype   = {arxiv},
  eprint       = {2010.03759},
  keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/9CKGY39J/Liu et al. - 2020 - Energy-based Out-of-distribution Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/XMFCB97N/2010.html:text/html}
}

@article{carriere_perslay_2020,
  title        = {{PersLay}: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures},
  url          = {http://arxiv.org/abs/1904.09378},
  shorttitle   = {{PersLay}},
  abstract     = {Persistence diagrams, the most common descriptors of Topological Data Analysis, encode topological properties of data and have already proved pivotal in many diﬀerent applications of data science. However, since the metric space of persistence diagrams is not Hilbert, they end up being diﬃcult inputs for most Machine Learning techniques. To address this concern, several vectorization methods have been put forward that embed persistence diagrams into either ﬁnite-dimensional Euclidean space or implicit inﬁnite dimensional Hilbert space with kernels.},
  journaltitle = {{arXiv}:1904.09378 [cs, math, stat]},
  author       = {Carrière, Mathieu and Chazal, Frédéric and Ike, Yuichi and Lacombe, Théo and Royer, Martin and Umeda, Yuhei},
  urldate      = {2020-11-03},
  date         = {2020-03-08},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1904.09378},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Algebraic Topology, Computer Science - Computational Geometry},
  file         = {Carrière et al. - 2020 - PersLay A Neural Network Layer for Persistence Di.pdf:/Users/romainegele/Zotero/storage/FJJCU37J/Carrière et al. - 2020 - PersLay A Neural Network Layer for Persistence Di.pdf:application/pdf}
}

@article{carriere_note_2020,
  title        = {A note on stochastic subgradient descent for persistence-based functionals: convergence and practical aspects},
  url          = {http://arxiv.org/abs/2010.08356},
  shorttitle   = {A note on stochastic subgradient descent for persistence-based functionals},
  abstract     = {Solving optimization tasks based on functions and losses with a topological ﬂavor is a very active and growing ﬁeld of research in Topological Data Analysis, with plenty of applications in non-convex optimization, statistics and machine learning. All of these methods rely on the fact that most of the topological constructions are actually stratiﬁable and diﬀerentiable almost everywhere. However, the corresponding gradient and associated code is always anchored to a speciﬁc application and/or topological construction, and do not come with theoretical guarantees.},
  journaltitle = {{arXiv}:2010.08356 [cs, math]},
  author       = {Carrière, Mathieu and Chazal, Frédéric and Glisse, Marc and Ike, Yuichi and Kannan, Hariprasad},
  urldate      = {2020-11-03},
  date         = {2020-10-16},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2010.08356},
  keywords     = {Mathematics - Optimization and Control, Mathematics - Algebraic Topology, Computer Science - Computational Geometry},
  file         = {Carrière et al. - 2020 - A note on stochastic subgradient descent for persi.pdf:/Users/romainegele/Zotero/storage/9DFHQ5IB/Carrière et al. - 2020 - A note on stochastic subgradient descent for persi.pdf:application/pdf}
}

@article{peer_conflicting_2020,
  title        = {Conflicting Bundles: Adapting Architectures Towards the Improved Training of Deep Neural Networks},
  url          = {http://arxiv.org/abs/2011.02956},
  shorttitle   = {Conflicting Bundles},
  abstract     = {Designing neural network architectures is a challenging task and knowing which specific layers of a model must be adapted to improve the performance is almost a mystery. In this paper, we introduce a novel theory and metric to identify layers that decrease the test accuracy of the trained models, this identification is done as early as at the beginning of training. In the worst-case, such a layer could lead to a network that can not be trained at all. More precisely, we identified those layers that worsen the performance because they produce conflicting training bundles as we show in our novel theoretical analysis, complemented by our extensive empirical studies. Based on these findings, a novel algorithm is introduced to remove performance decreasing layers automatically. Architectures found by this algorithm achieve a competitive accuracy when compared against the state-of-the-art architectures. While keeping such high accuracy, our approach drastically reduces memory consumption and inference time for different computer vision tasks.},
  journaltitle = {{arXiv}:2011.02956 [cs]},
  author       = {Peer, David and Stabinger, Sebastian and Rodriguez-Sanchez, Antonio},
  urldate      = {2020-11-09},
  date         = {2020-11-05},
  eprinttype   = {arxiv},
  eprint       = {2011.02956},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/3FPQDSDQ/Peer et al. - 2020 - Conflicting Bundles Adapting Architectures Toward.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/A8N9XKRZ/2011.html:text/html}
}

@article{damour_underspecification_2020,
  title        = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  url          = {http://arxiv.org/abs/2011.03395},
  abstract     = {{ML} models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An {ML} pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern {ML} pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical {ML} pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  journaltitle = {{arXiv}:2011.03395 [cs, stat]},
  author       = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and {McLean}, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  urldate      = {2020-11-09},
  date         = {2020-11-06},
  eprinttype   = {arxiv},
  eprint       = {2011.03395},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/VMJZ7Z2V/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/78REFYFX/2011.html:text/html}
}

@article{white_study_2020,
  title        = {A Study on Encodings for Neural Architecture Search},
  url          = {http://arxiv.org/abs/2007.04965},
  abstract     = {Neural architecture search ({NAS}) has been extensively studied in the past few years. A popular approach is to represent each neural architecture in the search space as a directed acyclic graph ({DAG}), and then search over all {DAGs} by encoding the adjacency matrix and list of operations as a set of hyperparameters. Recent work has demonstrated that even small changes to the way each architecture is encoded can have a significant effect on the performance of {NAS} algorithms. In this work, we present the first formal study on the effect of architecture encodings for {NAS}, including a theoretical grounding and an empirical study. First we formally define architecture encodings and give a theoretical characterization on the scalability of the encodings we study Then we identify the main encoding-dependent subroutines which {NAS} algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that {NAS} encodings are an important design decision which can have a significant impact on overall performance. Our code is available at https://github.com/naszilla/nas-encodings.},
  journaltitle = {{arXiv}:2007.04965 [cs, stat]},
  author       = {White, Colin and Neiswanger, Willie and Nolen, Sam and Savani, Yash},
  urldate      = {2020-11-19},
  date         = {2020-07-09},
  eprinttype   = {arxiv},
  eprint       = {2007.04965},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/PBM9D4TB/White et al. - 2020 - A Study on Encodings for Neural Architecture Searc.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/2B5GKZPB/2007.html:text/html}
}

@article{li_adapting_nodate,
  title    = {Adapting Neural Architectures Between Domains},
  abstract = {Neural architecture search ({NAS}) has demonstrated impressive performance in automatically designing high-performance neural networks. The power of deep neural networks is to be unleashed for analyzing a large volume of data (e.g. {ImageNet}), but the architecture search is often executed on another smaller dataset (e.g. {CIFAR}-10) to ﬁnish it in a feasible time. However, it is hard to guarantee that the optimal architecture derived on the proxy task could maintain its advantages on another more challenging dataset. This paper aims to improve the generalization of neural architectures via domain adaptation. We analyze the generalization bounds of the derived architecture and suggest its close relations with the validation error and the data distribution distance on both domains. These theoretical analyses lead to {AdaptNAS}, a novel and principled approach to adapt neural architectures between domains in {NAS}. Our experimental evaluation shows that only a small part of {ImageNet} will be sufﬁcient for {AdaptNAS} to extend its architecture success to the entire {ImageNet} and outperform state-of-the-art comparison algorithms.},
  pages    = {10},
  author   = {Li, Yanxi and Yang, Zhaohui and Wang, Yunhe and Xu, Chang},
  langid   = {english},
  file     = {Li et al. - Adapting Neural Architectures Between Domains.pdf:/Users/romainegele/Zotero/storage/JSJC3TNF/Li et al. - Adapting Neural Architectures Between Domains.pdf:application/pdf}
}

@article{celli_no-regret_nodate,
  title    = {No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium},
  abstract = {The existence of simple, uncoupled no-regret dynamics that converge to correlated equilibria in normal-form games is a celebrated result in the theory of multi-agent systems. Speciﬁcally, it has been known for more than 20 years that when all players seek to minimize their internal regret in a repeated normal-form game, the empirical frequency of play converges to a normal-form correlated equilibrium. Extensive-form (that is, tree-form) games generalize normal-form games by modeling both sequential and simultaneous moves, as well as private information. Because of the sequential nature and presence of partial information in the game, extensive-form correlation has signiﬁcantly different properties than the normalform counterpart, many of which are still open research directions. Extensive-form correlated equilibrium ({EFCE}) has been proposed as the natural extensive-form counterpart to normal-form correlated equilibrium. However, it was currently unknown whether {EFCE} emerges as the result of uncoupled agent dynamics. In this paper, we give the ﬁrst uncoupled no-regret dynamics that converge to the set of {EFCEs} in n-player general-sum extensive-form games with perfect recall. First, we introduce a notion of trigger regret in extensive-form games, which extends that of internal regret in normal-form games. When each player has low trigger regret, the empirical frequency of play is close to an {EFCE}. Then, we give an efﬁcient no-trigger-regret algorithm. Our algorithm decomposes trigger regret into local subproblems at each decision point for the player, and constructs a global strategy of the player from the local solutions at each decision point.},
  pages    = {11},
  author   = {Celli, Andrea and Farina, Gabriele and Marchesi, Alberto and Gatti, Nicola},
  langid   = {english},
  file     = {Celli et al. - No-Regret Learning Dynamics for Extensive-Form Cor.pdf:/Users/romainegele/Zotero/storage/P7MSJZ67/Celli et al. - No-Regret Learning Dynamics for Extensive-Form Cor.pdf:application/pdf}
}

@article{valko_stochastic_nodate,
  title    = {Stochastic Simultaneous Optimistic Optimization},
  abstract = {We study the problem of global maximization of a function f given a ﬁnite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, {StoSOO}, follows an optimistic strategy to iteratively construct upper conﬁdence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A ﬁnite-time analysis of {StoSOO} shows that it performs almost as well as the best speciﬁcally-tuned algorithms even though the local smoothness of the function is not known.},
  pages    = {9},
  author   = {Valko, Michal and Carpentier, Alexandra and Munos, Rémi},
  langid   = {english},
  file     = {Valko et al. - Stochastic Simultaneous Optimistic Optimization.pdf:/Users/romainegele/Zotero/storage/QFR4TRIL/Valko et al. - Stochastic Simultaneous Optimistic Optimization.pdf:application/pdf}
}

@inproceedings{attali_weak_2007,
  location   = {Beijing, China},
  title      = {Weak witnesses for {Delaunay} triangulations of submanifolds},
  isbn       = {978-1-59593-666-0},
  url        = {http://portal.acm.org/citation.cfm?doid=1236246.1236267},
  doi        = {10.1145/1236246.1236267},
  abstract   = {The main result of this paper is an extension of de Silva’s Weak Delaunay Theorem to smoothly embedded curves and surfaces in Euclidean space. Assuming a sufﬁciently ﬁne sampling, we prove that i + 1 points in the sample span an i-simplex in the restricted Delaunay triangulation iff every subset of the i + 1 points has a weak witness.},
  eventtitle = {the 2007 {ACM} symposium},
  pages      = {143},
  booktitle  = {Proceedings of the 2007 {ACM} symposium on Solid and physical modeling  - {SPM} '07},
  publisher  = {{ACM} Press},
  author     = {Attali, Dominique and Edelsbrunner, Herbert and Mileyko, Yuriy},
  urldate    = {2020-09-28},
  date       = {2007},
  langid     = {english},
  file       = {Attali et al. - 2007 - Weak witnesses for Delaunay triangulations of subm.pdf:/Users/romainegele/Zotero/storage/GJAK27VE/TE27QJPJ.pdf:application/pdf}
}

@article{dasgupta_random_nodate,
  title    = {Random projection trees for vector quantization},
  abstract = {A simple and computationally efﬁcient scheme for tree-structured vector quantization is presented. Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie.},
  pages    = {14},
  author   = {Dasgupta, Sanjoy and Freund, Yoav},
  langid   = {english},
  file     = {Dasgupta and Freund - Random projection trees for vector quantization.pdf:/Users/romainegele/Zotero/storage/54XG85P9/Dasgupta and Freund - Random projection trees for vector quantization.pdf:application/pdf}
}

@article{hofer_deep_2018,
  title        = {Deep Learning with Topological Signatures},
  url          = {http://arxiv.org/abs/1707.04041},
  abstract     = {Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classiﬁcation experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.},
  journaltitle = {{arXiv}:1707.04041 [cs, math]},
  author       = {Hofer, Christoph and Kwitt, Roland and Niethammer, Marc and Uhl, Andreas},
  urldate      = {2020-10-05},
  date         = {2018-02-16},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1707.04041},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Mathematics - Algebraic Topology},
  file         = {Hofer et al. - 2018 - Deep Learning with Topological Signatures.pdf:/Users/romainegele/Zotero/storage/7FL2Z9PJ/RW5V92EE.pdf:application/pdf}
}

@article{hofer_learning_nodate,
  title    = {Learning Representations of Persistence Barcodes},
  abstract = {We consider the problem of supervised learning with summary representations of topological features in data. In particular, we focus on persistent homology, the prevalent tool used in topological data analysis. As the summary representations, referred to as barcodes or persistence diagrams, come in the unusual format of multi sets, equipped with computationally expensive metrics, they can not readily be processed with conventional learning techniques. While diﬀerent approaches to address this problem have been proposed, either in the context of kernel-based learning, or via carefully designed vectorization techniques, it remains an open problem how to leverage advances in representation learning via deep neural networks. Appropriately handling topological summaries as input to neural networks would address the disadvantage of previous strategies which handle this type of data in a task-agnostic manner. In particular, we propose an approach that is designed to learn a task-speciﬁc representation of barcodes. In other words, we aim to learn a representation that adapts to the learning problem while, at the same time, preserving theoretical properties (such as stability). This is done by projecting barcodes into a ﬁnite dimensional vector space using a collection of parametrized functionals, so called structure elements, for which we provide a generic construction scheme. A theoretical analysis of this approach reveals suﬃcient conditions to preserve stability, and also shows that diﬀerent choices of structure elements lead to great diﬀerences with respect to their suitability for numerical optimization. When implemented as a neural network input layer, our approach demonstrates compelling performance on various types of problems, including graph classiﬁcation and eigenvalue prediction, the classiﬁcation of 2D/3D object shapes and recognizing activities from {EEG} signals.},
  pages    = {45},
  author   = {Hofer, Christoph D and Kwitt, Roland and Niethammer, Marc},
  langid   = {english},
  file     = {Hofer et al. - Learning Representations of Persistence Barcodes.pdf:/Users/romainegele/Zotero/storage/EA2X4JNB/Hofer et al. - Learning Representations of Persistence Barcodes.pdf:application/pdf}
}

@article{perea_sliding_2013,
  title        = {Sliding Windows and Persistence: An Application of Topological Methods to Signal Analysis},
  url          = {http://arxiv.org/abs/1307.6188},
  shorttitle   = {Sliding Windows and Persistence},
  abstract     = {We develop in this paper a theoretical framework for the topological study of time series data. Broadly speaking, we describe geometrical and topological properties of sliding window (or time-delay) embeddings, as seen through the lens of persistent homology. In particular, we show that maximum persistence at the point-cloud level can be used to quantify periodicity at the signal level, prove structural and convergence theorems for the resulting persistence diagrams, and derive estimates for their dependency on window size and embedding dimension. We apply this methodology to quantifying periodicity in synthetic data sets, and compare the results with those obtained using state-of-the-art methods in gene expression analysis. We call this new method {SW}1PerS which stands for Sliding Windows and 1-dimensional Persistence Scoring.},
  journaltitle = {{arXiv}:1307.6188 [math, stat]},
  author       = {Perea, Jose and Harer, John},
  urldate      = {2020-10-07},
  date         = {2013-11-25},
  eprinttype   = {arxiv},
  eprint       = {1307.6188},
  keywords     = {Mathematics - Algebraic Topology, Mathematics - Statistics Theory},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/I627CQYI/Perea and Harer - 2013 - Sliding Windows and Persistence An Application of.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/RUC8KD3T/1307.html:text/html}
}

@article{noauthor_phylogenetic_nodate,
  title = {Phylogenetic trees metric.pdf},
  file  = {Full Text PDF:/Users/romainegele/Zotero/storage/ZC3ZYA4S/3M7SXCC2.pdf:application/pdf}
}

@article{khrulkov_geometry_nodate,
  title    = {Geometry Score: A Method For Comparing Generative Adversarial Networks},
  abstract = {One of the biggest challenges in the research of generative adversarial networks ({GANs}) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a {GAN} by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real–life models and datasets and demonstrate that our method provides new insights into properties of {GANs}.},
  pages    = {9},
  author   = {Khrulkov, Valentin and Oseledets, Ivan},
  langid   = {english},
  file     = {Khrulkov and Oseledets - Geometry Score A Method For Comparing Generative .pdf:/Users/romainegele/Zotero/storage/DJTHK4TQ/Khrulkov and Oseledets - Geometry Score A Method For Comparing Generative .pdf:application/pdf}
}

@article{turner_persistent_2014,
  title        = {Persistent Homology Transform for Modeling Shapes and Surfaces},
  url          = {http://arxiv.org/abs/1310.1030},
  abstract     = {In this paper we introduce a statistic, the persistent homology transform ({PHT}), to model surfaces in \${\textbackslash}mathbb\{R\}{\textasciicircum}3\$ and shapes in \${\textbackslash}mathbb\{R\}{\textasciicircum}2\$. This statistic is a collection of persistence diagrams - multiscale topological summaries used extensively in topological data analysis. We use the {PHT} to represent shapes and execute operations such as computing distances between shapes or classifying shapes. We prove the map from the space of simplicial complexes in \${\textbackslash}mathbb\{R\}{\textasciicircum}3\$ into the space spanned by this statistic is injective. This implies that the statistic is a sufficient statistic for probability densities on the space of piecewise linear shapes. We also show that a variant of this statistic, the Euler Characteristic Transform ({ECT}), admits a simple exponential family formulation which is of use in providing likelihood based inference for shapes and surfaces. We illustrate the utility of this statistic on simulated and real data.},
  journaltitle = {{arXiv}:1310.1030 [math, stat]},
  author       = {Turner, Katharine and Mukherjee, Sayan and Boyer, Doug M.},
  urldate      = {2020-10-31},
  date         = {2014-07-15},
  eprinttype   = {arxiv},
  eprint       = {1310.1030},
  keywords     = {Mathematics - Statistics Theory},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/2NQBD2KW/Turner et al. - 2014 - Persistent Homology Transform for Modeling Shapes .pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/R67RPTLM/1310.html:text/html}
}


@article{maddox_simple_nodate,
  title   = {A simple baseline for {B}ayesian uncertainty in deep learning},
  author  = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  pages   = {13153--13164},
  year    = {2019}
}

@inproceedings{caruana_ensemble_2004,
  location   = {Banff, Alberta, Canada},
  title      = {Ensemble selection from libraries of models},
  url        = {http://portal.acm.org/citation.cfm?doid=1015330.1015432},
  doi        = {10.1145/1015330.1015432},
  abstract   = {We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using diﬀerent learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or {ROC} Area. Experiments with seven test problems and ten metrics demonstrate the beneﬁt of ensemble selection.},
  eventtitle = {Twenty-first international conference},
  pages      = {18},
  booktitle  = {Twenty-first international conference on Machine learning  - {ICML} '04},
  publisher  = {{ACM} Press},
  author     = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
  urldate    = {2021-04-16},
  year       = {2004},
  langid     = {english},
  file       = {Caruana et al. - 2004 - Ensemble selection from libraries of models.pdf:/Users/romainegele/Zotero/storage/3K8V8D65/Caruana et al. - 2004 - Ensemble selection from libraries of models.pdf:application/pdf}
}

@article{wenzel_hyperparameter_2021,
  title        = {Hyperparameter Ensembles for Robustness and Uncertainty Quantification},
  url          = {http://arxiv.org/abs/2006.13570},
  abstract     = {Ensembles over neural network weights trained from different random initialization, known as deep ensembles, achieve state-of-the-art accuracy and calibration. The recently introduced batch ensembles provide a drop-in replacement that is more parameter efﬁcient. In this paper, we design ensembles not only over weights, but over hyperparameters to improve the state of the art in both settings. For best performance independent of budget, we propose hyper-deep ensembles, a simple procedure that involves a random search over different hyperparameters, themselves stratiﬁed across multiple random initializations. Its strong performance highlights the beneﬁt of combining models with both weight and hyperparameter diversity. We further propose a parameter efﬁcient version, hyper-batch ensembles, which builds on the layer structure of batch ensembles and self-tuning networks. The computational and memory costs of our method are notably lower than typical ensembles. On image classiﬁcation tasks, with {MLP}, {LeNet}, {ResNet} 20 and Wide {ResNet} 28-10 architectures, we improve upon both deep and batch ensembles.},
  journaltitle = {{arXiv}:2006.13570 [cs, stat]},
  author       = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
  urldate      = {2021-04-16},
  date         = {2021-01-08},
  year         = {2021},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2006.13570},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Wenzel et al. - 2021 - Hyperparameter Ensembles for Robustness and Uncert.pdf:/Users/romainegele/Zotero/storage/JMWH6D4U/Wenzel et al. - 2021 - Hyperparameter Ensembles for Robustness and Uncert.pdf:application/pdf}
}

@article{lakshminarayanan_simple_2017,
  title        = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  url          = {http://arxiv.org/abs/1612.01474},
  abstract     = {Deep neural networks ({NNs}) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in {NNs} is a challenging and yet unsolved problem. Bayesian {NNs}, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require signiﬁcant modiﬁcations to the training procedure and are computationally expensive compared to standard (non-Bayesian) {NNs}. We propose an alternative to Bayesian {NNs} that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classiﬁcation and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian {NNs}. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on {ImageNet}.},
  journaltitle = {{arXiv}:1612.01474 [cs, stat]},
  author       = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  urldate      = {2021-04-19},
  date         = {2017-11-03},
  year         = {2017},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1612.01474},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf:/Users/romainegele/Zotero/storage/56F8BQRC/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf:application/pdf}
}

@article{hansen_neural_1990,
  title        = {Neural network ensembles},
  volume       = {12},
  issn         = {01628828},
  url          = {http://ieeexplore.ieee.org/document/58871/},
  doi          = {10.1109/34.58871},
  abstract     = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
  pages        = {993--1001},
  number       = {10},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {{IEEE} Trans. Pattern Anal. Machine Intell.},
  author       = {Hansen, L.K. and Salamon, P.},
  urldate      = {2021-04-19},
  date         = {1990-10},
  year         = {1990},
  langid       = {english},
  file         = {Hansen and Salamon - 1990 - Neural network ensembles.pdf:/Users/romainegele/Zotero/storage/24D8LUB2/Hansen and Salamon - 1990 - Neural network ensembles.pdf:application/pdf}
}

@article{hernandez-lobato_probabilistic_2015,
  title        = {Probabilistic Backpropagation for Scalable Learning of {Bayesian} Neural Networks},
  url          = {http://arxiv.org/abs/1502.05336},
  abstract     = {Large multilayer neural networks trained with backpropagation have recently achieved state-ofthe-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overﬁt the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation ({PBP}). Similar to classical backpropagation, {PBP} works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that {PBP} is signiﬁcantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that {PBP} provides accurate estimates of the posterior variance on the network weights.},
  journaltitle = {{arXiv}:1502.05336 [stat]},
  author       = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
  urldate      = {2021-04-26},
  date         = {2015-07-15},
  year         = {2015},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1502.05336},
  keywords     = {Statistics - Machine Learning},
  file         = {Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:/Users/romainegele/Zotero/storage/6QX7CL6T/Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf}
}

@article{guo_calibration_2017,
  title        = {On Calibration of Modern Neural Networks},
  url          = {http://arxiv.org/abs/1706.04599},
  abstract     = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
  journaltitle = {{arXiv}:1706.04599 [cs]},
  author       = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  urldate      = {2021-05-04},
  date         = {2017-08-03},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1706.04599},
  keywords     = {Computer Science - Machine Learning},
  file         = {Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:/Users/romainegele/Zotero/storage/6V3S2N6N/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf:application/pdf}
}

@article{schulman_proximal_2017,
  title        = {Proximal Policy Optimization Algorithms},
  url          = {http://arxiv.org/abs/1707.06347},
  abstract     = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  journaltitle = {{arXiv}:1707.06347 [cs]},
  author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  urldate      = {2018-08-24},
  date         = {2017-07-19},
  eprinttype   = {arxiv},
  eprint       = {1707.06347},
  keywords     = {Computer Science - Machine Learning},
  file         = {arXiv\:1707.06347 PDF:/Users/romainegele/Zotero/storage/2GQVTIDT/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/9NHPAQDY/1707.html:text/html}
}

@article{mnih_asynchronous_2016,
  title        = {Asynchronous Methods for Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1602.01783},
  abstract     = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  journaltitle = {{arXiv}:1602.01783 [cs]},
  author       = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  urldate      = {2018-08-29},
  date         = {2016-02-04},
  eprinttype   = {arxiv},
  eprint       = {1602.01783},
  keywords     = {Computer Science - Machine Learning},
  file         = {arXiv\:1602.01783 PDF:/Users/romainegele/Zotero/storage/SQ9NILZE/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv\:1602.01783 PDF:/Users/romainegele/Zotero/storage/D2N4PBK8/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/BQIS5QY3/1602.html:text/html}
}

@article{williams_simple_1992,
  title        = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  volume       = {8},
  issn         = {1573-0565},
  url          = {https://doi.org/10.1007/BF00992696},
  doi          = {10.1007/BF00992696},
  abstract     = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called {REINFORCE} algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  pages        = {229--256},
  number       = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  author       = {Williams, Ronald J.},
  urldate      = {2018-09-08},
  date         = {1992-05-01},
  langid       = {english},
  keywords     = {connectionist networks, gradient descent, mathematical analysis, Reinforcement learning},
  file         = {Springer Full Text PDF:/Users/romainegele/Zotero/storage/FSV8P8QK/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf:application/pdf}
}

@article{brockman_openai_2016,
  title        = {{OpenAI} Gym},
  url          = {http://arxiv.org/abs/1606.01540},
  abstract     = {{OpenAI} Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
  journaltitle = {{arXiv}:1606.01540 [cs]},
  author       = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  urldate      = {2018-09-10},
  date         = {2016-06-05},
  eprinttype   = {arxiv},
  eprint       = {1606.01540},
  keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1606.01540 PDF:/Users/romainegele/Zotero/storage/E83BN6RM/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/DLWS2DF7/1606.html:text/html}
}

@article{hafner_tensorflow_2017,
  title        = {{TensorFlow} Agents: Efficient Batched Reinforcement Learning in {TensorFlow}},
  url          = {http://arxiv.org/abs/1709.02878},
  shorttitle   = {{TensorFlow} Agents},
  abstract     = {We introduce {TensorFlow} Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in {TensorFlow}. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the {TensorFlow} execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce {BatchPPO}, an efficient implementation of the proximal policy optimization algorithm. By open sourcing {TensorFlow} Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.},
  journaltitle = {{arXiv}:1709.02878 [cs]},
  author       = {Hafner, Danijar and Davidson, James and Vanhoucke, Vincent},
  urldate      = {2018-09-10},
  date         = {2017-09-08},
  eprinttype   = {arxiv},
  eprint       = {1709.02878},
  keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1709.02878 PDF:/Users/romainegele/Zotero/storage/YMQBXTRR/Hafner et al. - 2017 - TensorFlow Agents Efficient Batched Reinforcement.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/J9AQ962R/1709.html:text/html}
}

@article{li_diffusion_2017,
  title      = {Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting},
  url        = {https://arxiv.org/abs/1707.01926},
  shorttitle = {Diffusion Convolutional Recurrent Neural Network},
  author     = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
  urldate    = {2018-10-17},
  date       = {2017-07-06},
  langid     = {english},
  file       = {Full Text PDF:/Users/romainegele/Zotero/storage/C6HZ43SN/Li et al. - 2017 - Diffusion Convolutional Recurrent Neural Network .pdf:application/pdf;Snapshot:/Users/romainegele/Zotero/storage/FQEESMAZ/1707.html:text/html}
}

@article{silver_mastering_2017,
  title        = {Mastering the game of Go without human knowledge},
  volume       = {550},
  issn         = {0028-0836, 1476-4687},
  url          = {http://www.nature.com/doifinder/10.1038/nature24270},
  doi          = {10.1038/nature24270},
  pages        = {354--359},
  number       = {7676},
  journaltitle = {Nature},
  author       = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  urldate      = {2018-11-08},
  date         = {2017-10-18},
  langid       = {english},
  file         = {Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:/Users/romainegele/Zotero/storage/JCXJAWQB/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf}
}

@article{burda_exploration_2018,
  title        = {Exploration by Random Network Distillation},
  url          = {http://arxiv.org/abs/1810.12894},
  abstract     = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation ({RND}) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  journaltitle = {{arXiv}:1810.12894 [cs, stat]},
  author       = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  urldate      = {2018-11-09},
  date         = {2018-10-30},
  eprinttype   = {arxiv},
  eprint       = {1810.12894},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {arXiv\:1810.12894 PDF:/Users/romainegele/Zotero/storage/5D3BYTEM/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/AL7HN4FS/1810.html:text/html}
}

@article{xu_how_2018,
  title        = {How Powerful are Graph Neural Networks?},
  url          = {http://arxiv.org/abs/1810.00826},
  abstract     = {Graph Neural Networks ({GNNs}) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many {GNN} variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite {GNNs} revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of {GNNs} in capturing different graph structures. Our results characterize the discriminative power of popular {GNN} variants, such as Graph Convolutional Networks and {GraphSAGE}, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of {GNNs} and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  journaltitle = {{arXiv}:1810.00826 [cs, stat]},
  author       = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  urldate      = {2018-11-21},
  date         = {2018-10-01},
  eprinttype   = {arxiv},
  eprint       = {1810.00826},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {arXiv\:1810.00826 PDF:/Users/romainegele/Zotero/storage/22EGRWES/Xu et al. - 2018 - How Powerful are Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/YLUW6RSH/1810.html:text/html}
}

@article{chaudhari_stochastic_2017,
  title        = {Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  url          = {http://arxiv.org/abs/1710.11029},
  abstract     = {Stochastic gradient descent ({SGD}) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that {SGD} minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So {SGD} does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, {SGD} does not even converge in the classical sense: we show that the most likely trajectories of {SGD} for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such “out-of-equilibrium” behavior is a consequence of highly non-isotropic gradient noise in {SGD}; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  journaltitle = {{arXiv}:1710.11029 [cond-mat, stat]},
  author       = {Chaudhari, Pratik and Soatto, Stefano},
  urldate      = {2018-11-30},
  date         = {2017-10-30},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1710.11029},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Optimization and Control},
  file         = {Chaudhari and Soatto - 2017 - Stochastic gradient descent performs variational i.pdf:/Users/romainegele/Zotero/storage/W34HAT4L/Chaudhari and Soatto - 2017 - Stochastic gradient descent performs variational i.pdf:application/pdf}
}

@article{schulman_high-dimensional_2015,
  title        = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  url          = {http://arxiv.org/abs/1506.02438},
  abstract     = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to {TD}(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  journaltitle = {{arXiv}:1506.02438 [cs]},
  author       = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  urldate      = {2019-08-26},
  date         = {2015-06-08},
  eprinttype   = {arxiv},
  eprint       = {1506.02438},
  keywords     = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
  file         = {arXiv\:1506.02438 PDF:/Users/romainegele/Zotero/storage/GQJERN65/Schulman et al. - 2015 - High-Dimensional Continuous Control Using Generali.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/ZASRNTB9/1506.html:text/html}
}

@article{zhengying_overview_nodate,
  title    = {Overview and unifying conceptualization of Automated Machine Learning},
  abstract = {We introduce a novel generic mathematical formulation of {AutoML}, resting on formal deﬁnitions of hyperparameter optimization ({HPO}) and meta-learning. In light of this formulation, we decompose various algorithms and show that {HPO} does not really address the {AutoML} problem, more than “classical” machine learning algorithms, while meta-learning does. Other branches of machine learning such as transfer learning and ensemble learning are also reviewed, re-formulated and uniﬁed. Thus, our framework allows us to systematically classify methods and provides us with formal tools to facilitate theoretical developments and future empirical research. Our brief survey of existing methods indicates that these tools already help us gain useful insights.},
  pages    = {8},
  author   = {Zhengying, Liu and Xu, Zhen and Madadi, Meysam and Junior, Julio Jacques and Escalera, Sergio and Rajaa, Shangeth and Guyon, Isabelle},
  langid   = {english},
  file     = {Zhengying et al. - Overview and unifying conceptualization of Automat.pdf:/Users/romainegele/Zotero/storage/LGFLEZP7/Zhengying et al. - Overview and unifying conceptualization of Automat.pdf:application/pdf}
}

@article{finn_model-agnostic_2017,
  title        = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  url          = {http://arxiv.org/abs/1703.03400},
  abstract     = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  journaltitle = {{arXiv}:1703.03400 [cs]},
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  urldate      = {2020-05-31},
  date         = {2017-07-18},
  eprinttype   = {arxiv},
  eprint       = {1703.03400},
  keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/NM4VW63X/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/UL4EXH8A/1703.html:text/html}
}

@article{guo_entity_2016,
  title        = {Entity Embeddings of Categorical Variables},
  url          = {http://arxiv.org/abs/1604.06737},
  abstract     = {We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.},
  journaltitle = {{arXiv}:1604.06737 [cs]},
  author       = {Guo, Cheng and Berkhahn, Felix},
  urldate      = {2021-01-03},
  date         = {2016-04-22},
  eprinttype   = {arxiv},
  eprint       = {1604.06737},
  note         = {version: 1},
  keywords     = {Computer Science - Machine Learning},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/IVJ9KTLH/Guo and Berkhahn - 2016 - Entity Embeddings of Categorical Variables.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/L3VWRWZW/1604.html:text/html}
}

@article{babuji_targeting_2020,
  title        = {Targeting {SARS}-{CoV}-2 with {AI}- and {HPC}-enabled Lead Generation: A First Data Release},
  url          = {http://arxiv.org/abs/2006.02431},
  shorttitle   = {Targeting {SARS}-{CoV}-2 with {AI}- and {HPC}-enabled Lead Generation},
  abstract     = {Researchers across the globe are seeking to rapidly repurpose existing drugs or discover new drugs to counter the the novel coronavirus disease ({COVID}-19) caused by severe acute respiratory syndrome coronavirus 2 ({SARS}-{CoV}-2). One promising approach is to train machine learning ({ML}) and artificial intelligence ({AI}) tools to screen large numbers of small molecules. As a contribution to that effort, we are aggregating numerous small molecules from a variety of sources, using high-performance computing ({HPC}) to computer diverse properties of those molecules, using the computed properties to train {ML}/{AI} models, and then using the resulting models for screening. In this first data release, we make available 23 datasets collected from community sources representing over 4.2 B molecules enriched with pre-computed: 1) molecular fingerprints to aid similarity searches, 2) 2D images of molecules to enable exploration and application of image-based deep learning methods, and 3) 2D and 3D molecular descriptors to speed development of machine learning models. This data release encompasses structural information on the 4.2 B molecules and 60 {TB} of pre-computed data. Future releases will expand the data to include more detailed molecular simulations, computed models, and other products.},
  journaltitle = {{arXiv}:2006.02431 [cs, q-bio, stat]},
  author       = {Babuji, Yadu and Blaiszik, Ben and Brettin, Tom and Chard, Kyle and Chard, Ryan and Clyde, Austin and Foster, Ian and Hong, Zhi and Jha, Shantenu and Li, Zhuozhao and Liu, Xuefeng and Ramanathan, Arvind and Ren, Yi and Saint, Nicholaus and Schwarting, Marcus and Stevens, Rick and van Dam, Hubertus and Wagner, Rick},
  urldate      = {2020-07-07},
  date         = {2020-05-27},
  eprinttype   = {arxiv},
  eprint       = {2006.02431},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods},
  file         = {arXiv Fulltext PDF:/Users/romainegele/Zotero/storage/R2PFHYSY/Babuji et al. - 2020 - Targeting SARS-CoV-2 with AI- and HPC-enabled Lead.pdf:application/pdf;arXiv.org Snapshot:/Users/romainegele/Zotero/storage/YH8FDTGC/2006.html:text/html}
}

@article{ovadia_can_2019,
  title        = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
  url          = {http://arxiv.org/abs/1906.02530},
  shorttitle   = {Can You Trust Your Model's Uncertainty?},
  abstract     = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model’s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and {nonBayesian} methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classiﬁcation problems and investigate the effect of dataset shift on accuracy and calibration. We ﬁnd that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  journaltitle = {{arXiv}:1906.02530 [cs, stat]},
  author       = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
  urldate      = {2021-05-21},
  date         = {2019-12-17},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1906.02530},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf:/Users/romainegele/Zotero/storage/Q5WQ982E/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf:application/pdf}
}

@article{kohavi_bias_nodate,
  title    = {Bias Plus Variance Decomposition for Zero-one loss functions},
  abstract = {We present a bias-variance decomposition of expected misclassi cation rate, the most commonly used loss function in supervised classi cation learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was o ered for the more commonly used zero-one (misclassi cation) loss functions until the recent work of Kong \& Dietterich (1995) and Breiman (1996). Their decomposition su ers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the {UCI} repository.},
  pages    = {9},
  author   = {Kohavi, Ron and Wolpert, David H},
  langid   = {english},
  file     = {Kohavi and Wolpert - Bias PZluesroV-aOrniaenLceosDs eFcuonmctpioosnisti.pdf:/Users/romainegele/Zotero/storage/BYRCVV83/Kohavi and Wolpert - Bias PZluesroV-aOrniaenLceosDs eFcuonmctpioosnisti.pdf:application/pdf}
}

@article{gal_dropout_2016,
  title        = {Dropout as a {Bayesian} Approximation: Representing Model Uncertainty in Deep Learning},
  url          = {http://arxiv.org/abs/1506.02142},
  shorttitle   = {Dropout as a Bayesian Approximation},
  abstract     = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks ({NNs}) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout {NNs} –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using {MNIST} as an example. We show a considerable improvement in predictive log-likelihood and {RMSE} compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
  journaltitle = {{arXiv}:1506.02142 [cs, stat]},
  author       = {Gal, Yarin and Ghahramani, Zoubin},
  urldate      = {2021-05-24},
  date         = {2016-10-04},
  year         = {2016},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1506.02142},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:/Users/romainegele/Zotero/storage/I28MAJQX/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf}
}

@article{gneiting_strictly_2007,
  title        = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  volume       = {102},
  issn         = {0162-1459, 1537-274X},
  url          = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
  doi          = {10.1198/016214506000001437},
  pages        = {359--378},
  number       = {477},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author       = {Gneiting, Tilmann and Raftery, Adrian E},
  urldate      = {2021-05-25},
  date         = {2007-03},
  langid       = {english},
  file         = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/Users/romainegele/Zotero/storage/C6L85PAN/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf}
}

@incollection{hiot_kriging_2010,
  location    = {Berlin, Heidelberg},
  title       = {Kriging Is Well-Suited to Parallelize Optimization},
  volume      = {2},
  isbn        = {978-3-642-10700-9 978-3-642-10701-6},
  url         = {http://link.springer.com/10.1007/978-3-642-10701-6_6},
  pages       = {131--162},
  booktitle   = {Computational Intelligence in Expensive Optimization Problems},
  publisher   = {Springer Berlin Heidelberg},
  author      = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  editor      = {Tenne, Yoel and Goh, Chi-Keong},
  editorb     = {Hiot, Lim Meng and Ong, Yew Soon},
  editorbtype = {redactor},
  urldate     = {2021-05-26},
  date        = {2010},
  langid      = {english},
  doi         = {10.1007/978-3-642-10701-6_6},
  year        = {2010},
  note        = {Series Title: Adaptation Learning and Optimization},
  file        = {Ginsbourger et al. - 2010 - Kriging Is Well-Suited to Parallelize Optimization.pdf:/Users/romainegele/Zotero/storage/UWHPDR7A/Ginsbourger et al. - 2010 - Kriging Is Well-Suited to Parallelize Optimization.pdf:application/pdf}
}

@article{noe_new_2018,
  title        = {On a New Improvement-Based Acquisition Function for {Bayesian} Optimization},
  url          = {http://arxiv.org/abs/1808.06918},
  abstract     = {Bayesian optimization ({BO}) is a popular algorithm for solving challenging optimization tasks. It is designed for problems where the objective function is expensive to evaluate, perhaps not available in exact form, without gradient information and possibly returning noisy values. Diﬀerent versions of the algorithm vary in the choice of the acquisition function, which recommends the point to query the objective at next. Initially, researchers focused on improvement-based acquisitions, while recently the attention has shifted to more computationally expensive informationtheoretical measures. In this paper we present two major contributions to the literature. First, we propose a new improvement-based acquisition function that recommends query points where the improvement is expected to be high with high conﬁdence. The proposed algorithm is evaluated on a large set of benchmark functions from the global optimization literature, where it turns out to perform at least as well as current state-of-the-art acquisition functions, and often better. This suggests that it is a powerful default choice for {BO}. The novel policy is then compared to widely used global optimization solvers in order to conﬁrm that {BO} methods reduce the computational costs of the optimization by keeping the number of function evaluations small. The second main contribution represents an application to precision medicine, where the interest lies in the estimation of parameters of a partial diﬀerential equations model of the human pulmonary blood circulation system. Once inferred, these parameters can help clinicians in diagnosing a patient with pulmonary hypertension without going through the standard invasive procedure of right heart catheterization, which can lead to side eﬀects and complications (e.g. severe pain, internal bleeding, thrombosis).},
  journaltitle = {{arXiv}:1808.06918 [cs, stat]},
  author       = {Noè, Umberto and Husmeier, Dirk},
  urldate      = {2021-05-26},
  date         = {2018-08-21},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1808.06918},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Noè and Husmeier - 2018 - On a New Improvement-Based Acquisition Function fo.pdf:/Users/romainegele/Zotero/storage/ALJZ8YQB/Noè and Husmeier - 2018 - On a New Improvement-Based Acquisition Function fo.pdf:application/pdf}
}

@article{johnson_training_nodate,
  title    = {Training Deep Models Faster with Robust, Approximate Importance Sampling},
  abstract = {In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure ({RAIS}) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, {RAIS} provides much of the beneﬁt of exact importance sampling with drastically reduced overhead. Empirically, we ﬁnd {RAIS}-{SGD} and standard {SGD} follow similar learning curves, but {RAIS} moves faster through these paths, achieving speed-ups of at least 20\% and sometimes much more.},
  pages    = {11},
  author   = {Johnson, Tyler B and Guestrin, Carlos},
  langid   = {english},
  file     = {Johnson and Guestrin - Training Deep Models Faster with Robust, Approxima.pdf:/Users/romainegele/Zotero/storage/H8AKPHM9/Johnson and Guestrin - Training Deep Models Faster with Robust, Approxima.pdf:application/pdf}
}

@article{katharopoulos_not_nodate,
  title    = {Not All Samples Are Created Equal:  Deep Learning with Importance Sampling},
  abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on “informative” examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: ﬁrst, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard {SGD} procedure, and we demonstrate experimentally, on image classiﬁcation, {CNN} ﬁne-tuning, and {RNN} training, that for a ﬁxed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
  pages    = {10},
  author   = {Katharopoulos, Angelos and Fleuret, François},
  langid   = {english},
  file     = {Katharopoulos and Fleuret - Not All Samples Are Created Equal  Deep Learning .pdf:/Users/romainegele/Zotero/storage/3SI76GQD/Katharopoulos and Fleuret - Not All Samples Are Created Equal  Deep Learning .pdf:application/pdf}
}

@article{ayhan2018test,
  title  = {Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks},
  author = {Ayhan, Murat Seckin and Berens, Philipp},
  year   = {2018}
}
@article{mohamed2016learning,
  title   = {Learning in implicit generative models},
  author  = {Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal = {arXiv preprint:1610.03483},
  year    = {2016}
}
@inproceedings{pearce2018high,
  title        = {High-quality prediction intervals for deep learning: A distribution-free, ensembled approach},
  author       = {Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
  booktitle    = {International Conference on Machine Learning},
  pages        = {4075--4084},
  year         = {2018},
  organization = {PMLR}
}

@inproceedings{rasmussen2003gaussian,
  title        = {Gaussian processes in machine learning},
  author       = {Rasmussen, Carl Edward},
  booktitle    = {Summer school on machine learning},
  pages        = {63--71},
  year         = {2003},
  organization = {Springer}
}
@article{wang2019aleatoric,
  title     = {Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks},
  author    = {Wang, Guotai and Li, Wenqi and Aertsen, Michael and Deprest, Jan and Ourselin, S{\'e}bastien and Vercauteren, Tom},
  journal   = {Neurocomputing},
  volume    = {338},
  pages     = {34--45},
  year      = {2019},
  publisher = {Elsevier}
}
@article{masters_revisiting_2018,
  title        = {Revisiting Small Batch Training for Deep Neural Networks},
  url          = {http://arxiv.org/abs/1804.07612},
  abstract     = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a signiﬁcantly smaller memory footprint, which might also be exploited to improve machine throughput.},
  journaltitle = {{arXiv}:1804.07612 [cs, stat]},
  author       = {Masters, Dominic and Luschi, Carlo},
  urldate      = {2021-05-31},
  date         = {2018-04-20},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1804.07612},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file         = {Masters and Luschi - 2018 - Revisiting Small Batch Training for Deep Neural Ne.pdf:/Users/romainegele/Zotero/storage/U4EHKLET/Masters and Luschi - 2018 - Revisiting Small Batch Training for Deep Neural Ne.pdf:application/pdf}
}

@article{naitzat_topology_2020,
  title        = {Topology of deep neural networks},
  url          = {http://arxiv.org/abs/2004.06093},
  abstract     = {We study how the topology of a data set \$M = M\_a {\textbackslash}cup M\_b {\textbackslash}subseteq {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, representing two classes \$a\$ and \$b\$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error (\${\textbackslash}approx 0.01{\textbackslash}\%\$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like {ReLU} outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of \$M\$ we begin with, when passed through a well-trained neural network \$f : {\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}to {\textbackslash}mathbb\{R\}{\textasciicircum}p\$, there is a vast reduction in the Betti numbers of both components \$M\_a\$ and \$M\_b\$; in fact they nearly always reduce to their lowest possible values: \${\textbackslash}beta\_k{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 0\$ for \$k {\textbackslash}ge 1\$ and \${\textbackslash}beta\_0{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 1\$, \$i =a, b\$. Furthermore, (2) the reduction in Betti numbers is significantly faster for {ReLU} activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
  journaltitle = {{arXiv}:2004.06093 [cs, math, stat]},
  author       = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
  urldate      = {2021-06-02},
  date         = {2020-04-13},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2004.06093},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, Mathematics - Algebraic Topology},
  file         = {Naitzat et al. - 2020 - Topology of deep neural networks.pdf:/Users/romainegele/Zotero/storage/RYNRG4AL/Naitzat et al. - 2020 - Topology of deep neural networks.pdf:application/pdf}
}

@article{carriere_perslay_2020-1,
  title        = {{PersLay}: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures},
  url          = {http://arxiv.org/abs/1904.09378},
  shorttitle   = {{PersLay}},
  abstract     = {Persistence diagrams, the most common descriptors of Topological Data Analysis, encode topological properties of data and have already proved pivotal in many diﬀerent applications of data science. However, since the metric space of persistence diagrams is not Hilbert, they end up being diﬃcult inputs for most Machine Learning techniques. To address this concern, several vectorization methods have been put forward that embed persistence diagrams into either ﬁnite-dimensional Euclidean space or implicit inﬁnite dimensional Hilbert space with kernels.},
  journaltitle = {{arXiv}:1904.09378 [cs, math, stat]},
  author       = {Carrière, Mathieu and Chazal, Frédéric and Ike, Yuichi and Lacombe, Théo and Royer, Martin and Umeda, Yuhei},
  urldate      = {2021-06-02},
  date         = {2020-03-08},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1904.09378},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Algebraic Topology, Computer Science - Computational Geometry},
  file         = {Carrière et al. - 2020 - PersLay A Neural Network Layer for Persistence Di.pdf:/Users/romainegele/Zotero/storage/H8XBCX5B/Carrière et al. - 2020 - PersLay A Neural Network Layer for Persistence Di.pdf:application/pdf}
}

@article{le_supervised_nodate,
  title    = {Supervised autoencoders: Improving generalization performance with unsupervised regularizers},
  abstract = {Generalization performance is a central goal in machine learning, with explicit generalization strategies needed when training over-parametrized models, like large neural networks. There is growing interest in using multiple, potentially auxiliary tasks, as one strategy towards this goal. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that jointly predicts targets and inputs (reconstruction). We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error—particularly as an improvement on simplistic regularization such as norms. We then demonstrate empirically that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can improve generalization.},
  pages    = {11},
  author   = {Le, Lei and Patterson, Andrew and White, Martha},
  langid   = {english},
  file     = {Le et al. - Supervised autoencoders Improving generalization .pdf:/Users/romainegele/Zotero/storage/CG6EY38E/Le et al. - Supervised autoencoders Improving generalization .pdf:application/pdf}
}

@article{raschka_model_2020,
  title        = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
  url          = {http://arxiv.org/abs/1811.12808},
  abstract     = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different ﬂavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to conﬁdence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F -test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  journaltitle = {{arXiv}:1811.12808 [cs, stat]},
  author       = {Raschka, Sebastian},
  urldate      = {2021-06-02},
  date         = {2020-11-10},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1811.12808},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file         = {Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf:/Users/romainegele/Zotero/storage/6K8TFXZA/Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf:application/pdf}
}

@article{amini_deep_nodate,
  title    = {Deep Evidential Regression},
  abstract = {Deterministic neural networks ({NNs}) are increasingly being deployed in safety critical domains, where calibrated, robust, and efﬁcient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian {NNs} to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the {NN} to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution ({OOD}) examples for training, thus enabling efﬁcient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and {OOD} test samples.},
  pages    = {11},
  author   = {Amini, Alexander and Schwarting, Wilko and Soleimany, Ava and Rus, Daniela},
  langid   = {english},
  file     = {Amini et al. - Deep Evidential Regression.pdf:/Users/romainegele/Zotero/storage/GRLIC2BR/Amini et al. - Deep Evidential Regression.pdf:application/pdf}
}

@inproceedings{caruana_getting_2006,
  location   = {Hong Kong, China},
  title      = {Getting the Most Out of Ensemble Selection},
  url        = {http://ieeexplore.ieee.org/document/4053111/},
  doi        = {10.1109/ICDM.2006.76},
  abstract   = {We investigate four previously unexplored aspects of ensemble selection, a procedure for building ensembles of classiﬁers. First we test whether adjusting model predictions to put them on a canonical scale makes the ensembles more effective. Second, we explore the performance of ensemble selection when different amounts of data are available for ensemble hillclimbing. Third, we quantify the beneﬁt of ensemble selection’s ability to optimize to arbitrary metrics. Fourth, we study the performance impact of pruning the number of models available for ensemble selection. Based on our results we present improved ensemble selection methods that double the beneﬁt of the original method.},
  eventtitle = {Sixth International Conference on Data Mining ({ICDM}'06)},
  pages      = {828--833},
  booktitle  = {Sixth International Conference on Data Mining ({ICDM}'06)},
  publisher  = {{IEEE}},
  author     = {Caruana, Rich and Munson, Art and Niculescu-Mizil, Alexandru},
  urldate    = {2021-06-10},
  date       = {2006-12},
  year       = {2006},
  langid     = {english},
  year       = {206},
  note       = {{ISSN}: 1550-4786},
  file       = {Caruana et al. - 2006 - Getting the Most Out of Ensemble Selection.pdf:/Users/romainegele/Zotero/storage/HZH26ANW/Caruana et al. - 2006 - Getting the Most Out of Ensemble Selection.pdf:application/pdf}
}

@article{abdar_review_2021,
  title        = {A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges},
  volume       = {76},
  issn         = {15662535},
  url          = {http://arxiv.org/abs/2011.06225},
  doi          = {10.1016/j.inffus.2021.05.008},
  shorttitle   = {A Review of Uncertainty Quantification in Deep Learning},
  abstract     = {Uncertainty quantiﬁcation ({UQ}) plays a pivotal role in the reduction of uncertainties during both optimization and decision making, applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two of the most widely-used {UQ} methods in the literature. In this regard, researchers have proposed different {UQ} methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classiﬁcation and segmentation), natural language processing (e.g., text classiﬁcation, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in {UQ} methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlight the fundamental research challenges and directions associated with the {UQ} ﬁeld.},
  pages        = {243--297},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  author       = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
  urldate      = {2021-06-24},
  date         = {2021-12},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2011.06225},
  keywords     = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
  file         = {Abdar et al. - 2021 - A Review of Uncertainty Quantification in Deep Lea.pdf:/Users/romainegele/Zotero/storage/VUX68998/Abdar et al. - 2021 - A Review of Uncertainty Quantification in Deep Lea.pdf:application/pdf}
}


@article{hullermeier_aleatoric_2021,
  title        = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
  volume       = {110},
  issn         = {0885-6125, 1573-0565},
  url          = {http://link.springer.com/10.1007/s10994-021-05946-3},
  doi          = {10.1007/s10994-021-05946-3},
  shorttitle   = {Aleatoric and epistemic uncertainty in machine learning},
  abstract     = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  pages        = {457--506},
  number       = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  author       = {Hüllermeier, Eyke and Waegeman, Willem},
  urldate      = {2021-07-15},
  date         = {2021-03},
  langid       = {english},
  file         = {Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf:/Users/romainegele/Zotero/storage/K6WAPZGJ/Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf:application/pdf}
}

@misc{Dua:2019,
  author      = {Dua, Dheeru and Graff, Casey},
  year        = {2017},
  title       = {{UCI} Machine Learning Repository},
  url         = {http://archive.ics.uci.edu/ml},
  institution = {University of California, Irvine, School of Information and Computer Sciences}
}

@article{barron1994approximation,
  title     = {Approximation and estimation bounds for artificial neural networks},
  author    = {Barron, Andrew R},
  journal   = {Machine learning},
  volume    = {14},
  pages     = {115--133},
  year      = {1994},
  publisher = {Springer}
}

@inproceedings{ying2019overview,
  title        = {An overview of overfitting and its solutions},
  author       = {Ying, Xue},
  booktitle    = {Journal of physics: Conference series},
  volume       = {1168},
  pages        = {022022},
  year         = {2019},
  organization = {IOP Publishing}
}

@article{sipser1996introduction,
  title     = {Introduction to the Theory of Computation},
  author    = {Sipser, Michael},
  journal   = {ACM Sigact News},
  volume    = {27},
  number    = {1},
  pages     = {27--29},
  year      = {1996},
  publisher = {ACM New York, NY, USA}
}

@inproceedings{bottou2010large,
  title        = {Large-scale machine learning with stochastic gradient descent},
  author       = {Bottou, L{\'e}on},
  booktitle    = {Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages        = {177--186},
  year         = {2010},
  organization = {Springer}
}

@article{bergstra2012random,
  title   = {Random search for hyper-parameter optimization.},
  author  = {Bergstra, James and Bengio, Yoshua},
  journal = {Journal of machine learning research},
  volume  = {13},
  number  = {2},
  year    = {2012}
}

@article{kotthoff2017auto,
  title   = {Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA},
  author  = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H and Hutter, Frank and Leyton-Brown, Kevin},
  journal = {Journal of Machine Learning Research},
  volume  = {18},
  number  = {25},
  pages   = {1--5},
  year    = {2017}
}

@inproceedings{eggensperger2013towards,
  title     = {Towards an empirical foundation for assessing bayesian optimization of hyperparameters},
  author    = {Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger and Leyton-Brown, Kevin and others},
  booktitle = {NIPS workshop on Bayesian Optimization in Theory and Practice},
  volume    = {10},
  number    = {3},
  year      = {2013}
}

@article{cho2020basic,
  title     = {Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of deep neural networks},
  author    = {Cho, Hyunghun and Kim, Yongjin and Lee, Eunjung and Choi, Daeyoung and Lee, Yongjae and Rhee, Wonjong},
  journal   = {IEEE access},
  volume    = {8},
  pages     = {52588--52608},
  year      = {2020},
  publisher = {IEEE}
}

@inproceedings{snoek2014input,
  title        = {Input warping for Bayesian optimization of non-stationary functions},
  author       = {Snoek, Jasper and Swersky, Kevin and Zemel, Rich and Adams, Ryan},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1674--1682},
  year         = {2014},
  organization = {PMLR}
}

@inproceedings{defreitas2012regretbounds,
  author    = {De Freitas, Nando and Smola, Alex J. and Zoghi, Masrour},
  title     = {Exponential regret bounds for Gaussian process bandits with deterministic observations},
  year      = {2012},
  isbn      = {9781450312851},
  publisher = {Omnipress},
  address   = {Madison, WI, USA},
  abstract  = {This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al., 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, (Srinivas et al., 2010) proved that the regret vanishes at the approximate rate of O(1/√t), where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to O(e-τt/(ln t)d/4) with high probability. Here, d is the dimension of the search space and τ is a constant that depends on the behaviour of the objective function near its global maximum.},
  booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages     = {955–962},
  numpages  = {8},
  location  = {Edinburgh, Scotland},
  series    = {ICML'12}
}

@inproceedings{wang2018batched,
  title        = {Batched large-scale Bayesian optimization in high-dimensional spaces},
  author       = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {745--754},
  year         = {2018},
  organization = {PMLR}
}

@article{astete2016simple,
  title     = {Simple and cumulative regret for continuous noisy optimization},
  author    = {Astete-Morales, Sandra and Cauwet, Marie-Liesse and Liu, Jialin and Teytaud, Olivier},
  journal   = {Theoretical Computer Science},
  volume    = {617},
  pages     = {12--27},
  year      = {2016},
  publisher = {Elsevier}
}

@article{dolan2002benchmarking,
  title     = {Benchmarking optimization software with performance profiles},
  author    = {Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal   = {Mathematical programming},
  volume    = {91},
  pages     = {201--213},
  year      = {2002},
  publisher = {Springer}
}

% Scalable NAS and Reinforcement Learning
@inproceedings{balaprakash2019rlnas,
  author    = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Wild, Stefan and Vishwanath, Venkatram and Xia, Fangfang and Brettin, Tom and Stevens, Rick},
  title     = {Scalable Reinforcement-Learning-Based Neural Architecture Search for Cancer Deep Learning Research},
  year      = {2019},
  isbn      = {9781450362290},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3295500.3356202},
  doi       = {10.1145/3295500.3356202},
  abstract  = {Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {37},
  numpages  = {33},
  keywords  = {neural architecture search, cancer, reinforcement learning, deep learning},
  location  = {Denver, Colorado},
  series    = {SC '19}
}

% Scalable NAS and GA for RNN Search
@inproceedings{maulik2020rnnnas,
  author    = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
  title     = {Recurrent Neural Network Architecture Search for Geophysical Emulation},
  year      = {2020},
  isbn      = {9781728199986},
  publisher = {IEEE Press},
  abstract  = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. Constructing neural networks for forecasting such data is non-trivial, however, and often requires trial and error. To address these limitations, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks (POD-LSTMs). We develop a scalable neural architecture search for generating stacked LSTMs to forecast temperature in the NOAA Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies POD-LSTMs that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {8},
  numpages  = {14},
  keywords  = {geophysics, emulation, recurrent neural networks},
  location  = {Atlanta, Georgia},
  series    = {SC '20}
}

@inproceedings{egele2023asynchronous,
  title        = {Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization},
  author       = {Egel{\'e}, Romain and Guyon, Isabelle and Vishwanath, Venkatram and Balaprakash, Prasanna},
  booktitle    = {2023 IEEE 19th International Conference on e-Science (e-Science)},
  pages        = {1--10},
  year         = {2023},
  organization = {IEEE}
}

% Joint HPO and NAS for Tabular Data
@inproceedings{egele2021agebo,
  author    = {\'{E}gel\'{e}, Romain and Balaprakash, Prasanna and Guyon, Isabelle and Vishwanath, Venkatram and Xia, Fangfang and Stevens, Rick and Liu, Zhengying},
  title     = {AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with Autotuned Data-Parallel Training for Tabular Data},
  year      = {2021},
  isbn      = {9781450384421},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3458817.3476203},
  doi       = {10.1145/3458817.3476203},
  abstract  = {Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {30},
  numpages  = {14},
  keywords  = {neural architecture search, neural networks, data-parallelism},
  location  = {St. Louis, Missouri},
  series    = {SC '21}
}

% Romain et al.'s paper on UQ with deep ensembles -- performed better with diverse architectures if I read correctly?
@inproceedings{egele2022autodeuq,
  title        = {{AutoDEUQ}: Automated deep ensemble with uncertainty quantification},
  author       = {Egele, Romain and Maulik, Romit and Raghavan, Krishnan and Lusch, Bethany and Guyon, Isabelle and Balaprakash, Prasanna},
  booktitle    = {26th International Conference on Pattern Recognition (ICPR)},
  pages        = {1908--1914},
  year         = {2022},
  organization = {IEEE}
}

% DeepHyper Software
@misc{deephyper_software,
  title        = {"DeepHyper: A Python Package for Scalable Neural Architecture and Hyperparameter Search"},
  author       = {{DeepHyper Development Team}},
  organization = {DeepHyper Team},
  year         = 2018,
  url          = {https://github.com/deephyper/deephyper}
}

%%% Applications %%%

% Romit et al.'s paper on deep ensembles with UQ for weather forecasting

@misc{maulik2023quantifying,
  title     = {Quantifying uncertainty for deep learning based forecasting and flow-reconstruction using neural architecture search ensembles},
  volume    = {454},
  url       = {http://dx.doi.org/10.1016/j.physd.2023.133852},
  doi       = {10.1016/j.physd.2023.133852},
  journal   = {Physica D: Nonlinear Phenomena},
  publisher = {Elsevier BV},
  author    = {Maulik, Romit and Egele, Romain and Raghavan, Krishnan and Balaprakash, Prasanna},
  year      = {2023},
  month     = nov,
  pages     = {133852},
  language  = {en}
}

@inproceedings{dorier2022transferlearning,
  author    = {M. Dorier and R. Egele and P. Balaprakash and J. Koo and S. Madireddy and S. Ramesh and A. D. Malony and R. Ross},
  booktitle = {2022 IEEE International Conference on Cluster Computing (CLUSTER)},
  title     = {HPC Storage Service Autotuning Using Variational- Autoencoder -Guided Asynchronous Bayesian Optimization},
  year      = {2022},
  volume    = {},
  issn      = {},
  pages     = {381-393},
  abstract  = {Distributed data storage services tailored to specific applications have grown popular in the high-performance computing (HPC) community as a way to address I/O and storage challenges. These services offer a variety of specific interfaces, semantics, and data representations. They also expose many tuning parameters, making it difficult for their users to find the best configuration for a given workload and platform. To address this issue, we develop a novel variational-autoencoder-guided asynchronous Bayesian optimization method to tune HPC storage service parameters. Our approach uses transfer learning to leverage prior tuning results and use a dynamically updated surrogate model to explore the large parameter search space in a systematic way. We implement our approach within the DeepHyper open-source framework, and apply it to the autotuning of a high-energy physics workflow on Argonne&#x27;s Theta supercomputer. We show that our transfer-learning approach enables a more than 40 x search speedup over random search, compared with a 2.5 x to 10 x speedup when not using transfer learning. Additionally, we show that our approach is on par with state-of-the-art autotuning frameworks in speed and outperforms them in resource utilization and parallelization capabilities.},
  keywords  = {systematics;transfer learning;semantics;supercomputers;probability distribution;bayes methods;resource management},
  doi       = {10.1109/CLUSTER51413.2022.00049},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CLUSTER51413.2022.00049},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {sep}
}

@article{egele2023one,
  title     = {Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?},
  url       = {http://dx.doi.org/10.14428/esann/2023.ES2023-84},
  doi       = {10.14428/esann/2023.es2023-84},
  journal   = {ESANN 2023 proceesdings},
  publisher = {Ciaco - i6doc.com},
  author    = {Egele, Romain and Guyon, Isabelle and Sun, Yixuan and Balaprakash, Prasanna},
  year      = {2023}
}

% A whitepaper describing Cereberus's wafer scale neuromorphic computing
% architectures, as used in the AI incubator at Argonne
@techreport{lavely2022,
  author={Lavely, Adam},
  title={Powering Extreme-Scale {HPC} with {C}erebras Wafer-Scale Accelerators},
  year={2022},
  institution={Cereberas Systems, Inc.},
  address={Sunnyvale, CA, USA},
  url={https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Powering-Extreme-Scale-HPC-with-Cerebras.pdf}
}

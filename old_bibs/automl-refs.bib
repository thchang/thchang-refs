% YAHPO gym is yet another hyperparameter optimization gym consisting of
% benchmark and test problems for testing neural architecture search and
% hyperparameter optimization algorithms.  Most of the test problems are based
% on xgboost, knn, or svm models of real data.  As of 2022, this one was not as
% mature as HPOBench and JAHS-Bench, so we didn't use it.  They claim to now
% offer multiobjective optimization test problems as well.  Open source Python
% software implementation available at:  github.com/slds-lmu/yahpo_gym
@inproceedings{pfisterer2022yahpo,
  title = 	 {{YAHPO} Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization},
  author =       {Pfisterer, Florian and Schneider, Lennart and Moosbauer, Julia and Binder, Martin and Bischl, Bernd},
  booktitle = 	 {Proceedings of the First International Conference on Automated Machine Learning},
  pages = 	 {3/1--39},
  year = 	 {2022},
  editor = 	 {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
  volume = 	 {188},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v188/pfisterer22a.html},
}


% HPOBench is another suite of automl (hyperparameter tuning) benchmark
% problems from the automl research group.  The problems can be configured to
% be either single or multi-fidelity.  They can either be run tabular (meaning
% the raw data is accessed in a table and only configurations in the dataset
% can be evaluated) or with a regression model (XGBoost).  The open source
% software is available for download in Python at: github.com/automl/HPOBench
@inproceedings{eggensperger2021hpobench,
  title     = {{HPOB}ench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for {HPO}},
  author    = {Katharina Eggensperger and Philipp M{\"u}ller and Neeratyoy Mallik and Matthias Feurer and Rene Sass and Aaron Klein and Noor Awad and Marius Lindauer and Frank Hutter},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2021},
  url       = {https://openreview.net/forum?id=1k4rJYEwda-}
}

% This is a modification to the NSGA-II multiobjective optimization algorithm
% where the authors introduce a pipeline of smaller "islands" of populations,
% which evolve independently.  Cross migration between these islands keeps the
% populations from diverging too far apart and allowing for progress to be
% shared between all islands through eventual consistency.  This modification
% allows NSGA-II to run fully asynchronously.  This is the version used by
% Optuna
@inproceedings{martens2013asynchronous,
  title     = {The asynchronous island model and NSGA-II: study of a new migration operator and its performance},
  author    = {M{\"a}rtens, Marcus and Izzo, Dario},
  booktitle = {Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages     = {1173--1180},
  year      = {2013}
}

% The DARTS algorithm trains a probability distribution of possible weights,
% layer types, and connection topologies to sample.  This creates a continuous
% relaxation of the neural architecture search problem, allowing it to be
% optimized using derivative-based techniques in fewer operations.  This allows
% it to train faster, however, this method doesn't work that well in practice
% because even if it trains nice distributions, it generally doesn't actually
% sample well-performing networks with high probability.
@inproceedings{liu2019darts,
  title     = {{DARTS}: Differentiable Architecture Search},
  author    = {Hanxiao Liu and Karen Simonyan and Yiming Yang},
  booktitle = {Proc. 7th International Conference on Learning Representations (ICLR '19)},
  year      = {2019},
  url       = {https://openreview.net/forum?id=S1eYHoC5FX}
}

% NSGA-Net is the NSGA-II/pymoo team's multiobjective genetic algorithm based
% NAS solver.  The open source python software is available from:
% github.com/ianwhale/nsga-net
@article{lu2018nsga,
  title     = {NSGA-NET: a multi-objective genetic algorithm for neural architecture search},
  author    = {Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and  Banzhaf, Wolfgang},
  booktitle = {GECCO-2019},
  year      = {2018}
}

% Original tree-parzen estimator (TPE) reference.  The authors observe that the
% structure of most hyperparameter optimization problems is tree-like in that
% the values of certain parameters are only relevant given the choices of
% earlier parameters.  This leads them to train a statistical distribution over
% the tree of decisions, which will allow them to traverse the tree with high
% probability of sampling good models.  They show that this is better than
% Bayesian optimization with Gaussian processes and random search.  They also
% have the insight (still holds true today) that for these problems, it is
% difficult to do better than random search
@inproceedings{NIPS2011_86e8f7ab,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

% The official publication of SMAC3 -- the latest version of SMAC from the
% automl group.  SMAC was the first hyperparameter optimization and neural
% architecture search software to use random forest surrogates for modeling the
% hyperparameter configuration space.  They also use a hierarchy of
% hyperparameters to handle "hidden parameters".  They support multi and
% single-fidelity NAS applications.  The open source software is written in
% Python and available from:  github.com/automl/SMAC3
@article{smac3_2022,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

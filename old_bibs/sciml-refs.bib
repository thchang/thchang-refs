% Differentiabl convex optimization layers are introduced pytorch and tensorflow via an update to cvxpy, introducing differentiation through the solution to a convex optimization problem modeled in cvxpy. The open source numerical optimization software is available at github.com/cvxgrp/cvxpylayers in Python
@inproceedings{agrawal2019differentiable,
	author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, J. Zico},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch\'{e}-Buc, F. d\textquotesingle and Fox, E. and Garnett, R.},
	title = {Differentiable Convex Optimization Layers},
	year = {2019},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {32},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, autograd, algorithmic differentiation, backpropagation, neural networks},
}

% Optuna open source software for fully distributed hyperparameter optimization. Maintained by a private startup in Japan (named Optuna). This software provides high-quality distributed wrappers for many neural architecture search and generic optimization algorithms.
@inproceedings{akiba2019optuna,
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	title = {Optuna: A next-generation hyperparameter optimization framework},
	year = {2019},
	month = {7},
	booktitle = {Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
	pages = {2623--2631},
	organization = {ACM},
	location = {Anchorage AK USA},
	doi = {10.1145/3292500.3330701},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, decision trees, uncertainty quantification, UQ},
}

% Brandon's original paper on differentiabl optimization layers in neural networks -- i.e., solving an optimization problem within a layer of a neural network and propogating the gradient through that solution so that the network parameters can still be trained
@inproceedings{amos2017optnet,
	author = {Amos, Brandon and Kolter, J. Zico},
	editor = {Precup, Doina and Teh, Yee Whye},
	title = {{O}pt{N}et: Differentiable Optimization as a Layer in Neural Networks},
	year = {2017},
	month = {06--11 Aug},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	series = {Proceedings of Machine Learning Research},
	volume = {70},
	pages = {136--145},
	organization = {PMLR},
	url = {https://proceedings.mlr.press/v70/amos17a.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, autograd, algorithmic differentiation, backpropagation, neural networks},
}

% Brandon introduces using neural networks and other scientific machine learning methods to warm-start optimization and control problems. An open source numerical software tutorial is available at github.com/facebookresearch/amortized-optimization-tutorial
@article{amos2023tutorial,
	author = {Amos, Brandon},
	title = {Tutorial on Amortized Optimization},
	year = {2023},
	journal = {Found. Trends Mach. Learn.},
	volume = {16},
	number = {5},
	numpages = {156},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
	doi = {10.1561/2200000102},
	url = {https://doi.org/10.1561/2200000102},
	issn = {1935-8237},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, neural networks, regression},
}

% Introducing the open source PDLP solver (now part of Google OR tools) for solving large scale linear programming problems, where the constraint matrices would never fit in memory using first-order (matrix free) descent methods. The algorithm is based on PDHG, which is an improvement to ADMM specialized for solving LPs. The main contributions of this solver and its predecessor PDHG are adaptive step-sizes and preconditioning to prevent oscillatory motions plus adpaptive an adaptive restart procedures and presolve to prevent stalling out. This solver is not as fast simplex or interior point methods on problems that do fit in memory, but is extremely scalable and robust on larger problems. Obviously, it cannot give a basic solution since first-order methods are always approximate (with order 10^-8 accuracy). Open source code is available through Google OR tools: github.com/google/or-tools
@inproceedings{applegate2021practical,
	author = {Applegate, David and Diaz, Mateo and Hinder, Oliver and Lu, Haihao and Lubin, Miles and O\textquotesingle Donoghue, Brendan and Schudy, Warren},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	title = {Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient},
	year = {2021},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {34},
	pages = {20243--20257},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a8fbbd3b11424ce032ba813493d95ad7-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, high dimension},
}

% BoTorch: Modular bayesian optimization framework and numerical software package. Uses Ax for deploying A/B testing applications. Uses pytorch for autograd, and uses monte carlo sampling and kernel reparameterization tricks in order to efficiently evaluate composite objective and acquisition functions and non gaussian kernels. Great example of high-impact open source numerical software and optimization software
@inproceedings{balandat2020botorch,
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	title = {{BoTorch}: A Framework for Efficient {M}onte-{C}arlo {B}ayesian Optimization},
	year = {2020},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {33},
	pages = {21524--21538},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, Gaussian process, uncertainty quantification, UQ},
}

% Prasanna and Stefan's original DeepHyper publication -- DeepHyper is an open source extreme-scale distributed optimization package, designed to scale to Argonne's exascale HPCs. Able to train ensembles of neural networks with diverse architectures at scale, with both single and multiobjective hyperparameter tuning support
@inproceedings{balaprakash2018deephyper,
	author = {Balaprakash, Prasanna and Salim, Michael and Uram, Thomas D. and Vishwanath, Venkat and Wild, Stefan M.},
	title = {{DeepHyper}: Asynchronous hyperparameter search for deep neural networks},
	year = {2018},
	month = {12},
	booktitle = {IEEE 25th international conference on high performance computing (HiPC)},
	pages = {42--51},
	organization = {IEEE},
	location = {Bengaluru, India},
	doi = {10.1109/hipc.2018.00014},
	url = {https://ieeexplore.ieee.org/document/8638041},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, decision trees, uncertainty quantification, UQ},
}

% An analysis showing that the condition number of the RBF (and GP) interpolation matrices grow (and lowering bounding them) as the minimum separation distance shrinks. I.e., when data gets too dense, the RBF conditioning is guaranteed to become arbitrarily bad.
@article{ball1992sensitivity,
	author = {Ball, Keith and Sivakumar, Natarajan and Ward, Joseph D},
	title = {On the sensitivity of radial basis interpolation to minimal data separation distance},
	year = {1992},
	month = {12},
	journal = {Constructive Approximation},
	volume = {8},
	number = {4},
	pages = {401--426},
	publisher = {Springer},
	doi = {10.1007/bf01203461},
	url = {http://link.springer.com/10.1007/BF01203461},
	issn = {0176-4276},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, approximation theory},
}

% Official PROX-QP paper -- the best open source QP solver according to several benchmarks. Open source C++ code is availabe through the proxsuite github: https://github.com/Simple-Robotics/proxsuite and a Python interface is available through CvxPy
@inproceedings{bambade2022proxqp,
	author = {Bambade, Antoine and El-Kazdadi, Sarah and Taylor, Adrien and Carpentier, Justin},
	title = {{PROX-QP}: Yet another Quadratic Programming Solver for Robotics and beyond},
	year = {2022},
	month = {June},
	booktitle = {RSS 2022 - Robotics: Science and Systems},
	organization = {Robotics: Science and Systems Foundation},
	location = {New York, United States},
	doi = {10.15607/rss.2022.xviii.040},
	url = {https://hal.inria.fr/hal-03683733},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% Publication showing that (a) interpolating classification and regression data in order to make predictions is often a reasonable thing to do especially in high dimensions (b) Delaunay interpolation is the correct way to do so and (c) using Delaunay interpolation on high dimensional noisy classification data yields predictions whose misclassification risk can be bounded by twice the Bayes risk (theoretical lowest possible risk for the data given) in the limit as the dimension increases
@inproceedings{belkin2018overfitting,
	author = {Belkin, Mikhail and Hsu, Daniel and Mitra, Partha P.},
	title = {Overfitting or perfect fitting? {R}isk bounds for classification and regression rules that interpolate},
	year = {2018},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS'18)},
	pages = {2306--2317},
	organization = {Curran Associates Inc.},
	location = {Montr{\'e}al, Canada},
    url = {https://proceedings.neurips.cc/paper/2018/hash/e22312179bf43e61576081a2f250f845-Abstract.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, overfitting, Delaunay triangulation, high dimension, interpolation, regression, classification, approximation theory},
}

% A massive review article on the magic of just interpolating noisy machine learning and deep learning data in high dimensions, and some theorems on why this is acceptable and even desirable as the dimension increases. Delaunay interpolation is spotlighted as one of the interpolation methods to use
@article{belkin2021fit,
	author = {Belkin, Mikhail},
	title = {Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
	year = {2021},
	month = {5},
	journal = {Acta Numerica},
	volume = {30},
	pages = {203--248},
	publisher = {Cambridge University Press},
	doi = {10.1017/S0962492921000039},
	url = {https://www.cambridge.org/core/product/identifier/S0962492921000039/type/journal_article},
	issn = {0962-4929},
	keywords = {machine learning, ML, scientific machine learning, SciML, overfitting, Delaunay triangulation, high dimension, interpolation, regression, classification, approximation theory},
}

% A review article on representation learning and unifying the ideas of representation learning, density estimation, and manifold learning under a single geometric umbrella. The authors show broadly how representation learning has been the key to the performance of neural networks across many fields, often allowing us to break the curse of dimensionality. Additionally, they discuss the general requirements (priors) about a problem / function for representation learning to be an effective approach.
@article{bengio2013representation,
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	title = {Representation Learning: A Review and New Perspectives},
	year = {2013},
	month = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = {35},
	number = {8},
	pages = {1798--1828},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TPAMI.2013.50},
	url = {http://ieeexplore.ieee.org/document/6472238/},
	issn = {0162-8828},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning, high dimension, dimension reduction, regression, classification},
}

% Methodolgies for calibrating the Fayans EDF model to experimental data. Data is expensive and limited and the model itself is computationally expensive, so this is a classical inverse problem. The problem is actually multiobective because the data themselves come from various categories representing different types of observations, and the standard deviations for each of these observables is not known. Could be configured as a 3 or 9-objective problem
@article{bollapragada2020optimization,
	author = {Bollapragada, Raghu and Menickelly, Matt and Nazarewicz, Witold and O'Neal, Jared and Reinhard, Paul-Gerhard and Wild, Stefan M.},
	title = {Optimization and supervised machine learning methods for fitting numerical physics models without derivatives},
	year = {2020},
	month = {2},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	volume = {48},
	number = {2},
	numpages = {24001},
	publisher = {IOP Publishing},
	doi = {10.1088/1361-6471/abd009},
	url = {https://iopscience.iop.org/article/10.1088/1361-6471/abd009},
	issn = {0954-3899},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, regression},
}

% The famous textbook on convex optimization (from which I learned most concepts) covering concepts such as basic convexity definitions and theorems, basic algorithms and optimality conditions, handling constraints, Lagrangian duality, multiobjective optimization basics, gradient descent and newton's method, sequential quadratic programming, linear programming, and a few applications and modeling basics
@book{boyd2004convex,
	author = {Boyd, Stephen P and Vandenberghe, Lieven},
	title = {Convex optimization},
	year = {2004},
	publisher = {Cambridge university press},
    url = {https://stanford.edu/~boyd/cvxbook/},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% The recommended citation for the jax software project -- one of my personal favorite open source numerical software in Python. Performs autograd (or algorithmic differentiation) in either forward or reverse mode, is strongly typed, can act as a drop-in replacement for numpy, and can be just-in-time (jit) compiled for massive speedups
@misc{bradbury2018jax,
	author = {Bradbury, J. and others, },
	title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
	year = {2018},
	number = {0.3.13},
	url = {http://github.com/google/jax},
	note = {Last accessed: Jul 2024},
	keywords = {machine learning, ML, scientific machine learning, SciML, autograd, algorithmic differentiation, backpropagation, computational linear algebra},
}

% The largest repository of open source AI, machine learning, and control benchmark problems, maintained by OpenAI. This was the primary benchmark problem environment for all reinforcement learning researchers not affiliated with another company with their own private environments (such as Meta and Google). Still available at github.com/openai/gym
@techreport{brockman2016openai,
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	title = {OpenAI Gym},
	year = {2016},
	institution = {arXiv:1606.01540},
    git = {https://github.com/openai/gym},
	keywords = {machine learning, ML, scientific machine learning, SciML, benchmarking, reinforcement learning, RL},
}

% The new classic textbook on geometric deep learning -- this book (by the creator of the field) covers how all of representation learning can be broken down into exploiting symmetries, stabilities, and invariances in data, through the lens of group theory. For example, a convolutional layer exploits a symmetry between equivalent groups of images; and a max pooling layer exploits a scale invariance in image size/resolution; and when we talk about feature encodings, we are often looking for mappings that maintain stability (i.e., similar feature vectors remain close together after encoding).
@book{bronstein2021geometric,
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	year = {2021},
	publisher = {arXiv cs.LG},
	doi = {10.48550/arXiv.2104.13478},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning, high dimension, dimension reduction},
}

% Large review paper covering all aspects of RBFs, their interpolation error bounds, their convergence rates, and numerical computation for various kernel functions.
@article{buhmann2000radial,
	author = {Buhmann, M. D.},
	title = {Radial basis functions},
	year = {2000},
	month = {1},
	journal = {Acta Numerica},
	volume = {9},
	pages = {1--38},
	publisher = {Cambridge University Press},
	doi = {10.1017/S0962492900000015},
	url = {https://www.cambridge.org/core/product/identifier/S0962492900000015/type/journal_article},
	issn = {0962-4929},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, approximation theory},
}

% The DelaunaySparse software, demonstrates how to calculate simplices from a Delauay triangulation in very high dimensions scalably (and in parallel) using a highly customized simplex method like solver. The resulting Fortran numerical software is fully open source with a C and Python interface
@article{chang2020algorithm,
	author = {Chang, Tyler H. and Watson, Layne T. and Lux, Thomas C. H. and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Algorithm 1012: {DELAUNAYSPARSE}: Interpolation via a Sparse Subset of the {D}elaunay Triangulation in Medium to High Dimensions},
	year = {2020},
	month = {12},
	journal = {ACM Trans. Math. Softw.},
	volume = {46},
	number = {4},
	articleno = {38},
	numpages = {20},
	publisher = {ACM},
	address = {New York, NY, USA},
	doi = {10.1145/3422818},
	url = {https://dl.acm.org/doi/10.1145/3422818},
	issn = {0098-3500},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, Delaunay triangulation, high dimension, interpolation},
}

% My PhD thesis, including multiobjective optimization techniques, algorithm, performance analysis, and software review; description of VTMOP, running parallel simulations, integrating with libE. Also scientific machine learning via Delaunay interpolation and algorithms and proofs for doing so. Several applications related to HPC performance modeling and autotuning.
@phdthesis{chang2020mathematical,
	author = {Chang, Tyler H.},
	title = {Mathematical Software for Multiobjective Optimization Problems},
	year = {2020},
	school = {Virginia Tech, Dept. of Computer Science},
	url = {https://vtechworks.lib.vt.edu/handle/10919/98915},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, computational linear algebra, Delaunay triangulation, interpolation},
}

% Our SciML paper analyzing and comparing the usefulness of error bounds of various interpolation methods for scientific machine learning applications. Ultimately, we show that the Delaunay bounds are most useful (I personally suspect it has something to do with less smooth "low order methods" being more robust for noisy data) and we demonstrate how they can be used for interpretability and verification on an aircraft design application (predicting lift/drag ratios for various airfoils)
@article{chang2025leveraging,
	author = {Chang, Tyler H. and Gillette, Andrew K. and Maulik, Romit},
	title = {Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning},
	year = {2025},
	month = {3},
	journal = {Journal of Computational Physics},
	volume = {524},
	articleno = {113726},
	numpages = {23},
	publisher = {Elsevier BV},
	doi = {10.1016/j.jcp.2025.113726},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999125000099},
	issn = {0021-9991},
	keywords = {machine learning, ML, scientific machine learning, SciML, representation learning, Delaunay triangulation, RBFs, Gaussian process, high dimension, dimension reduction, interpolation, regression, approximation theory, uncertainty quantification, UQ},
}

% A great article on the optimality of Delaunay triangulations for interpolation. The authors show that the Delaunay triangulation can be equivalently defined as the interpolation mesh that minimizes interpolation error for the perfect quadratic function f(x) = <x, x>. Then, they explore how we can define new metrics to yield a new Delaunay triangulation that is optimal for an arbitrary function. Ultimatey, it is hard to compute such Delaunay triangulations, so they explore relaxations and definitions of "nearly optimal" Delaunay triangulations.
@article{chen2004optimal,
	author = {Chen, Long and Xu, Jin-chao},
	title = {Optimal {D}elaunay triangulations},
	year = {2004},
	journal = {Journal of Computational Mathematics},
	volume = {22},
	number = {2},
	pages = {299--308},
    url = {https://www.math.uci.edu/~chenlong/Papers/Chen.L%3BXu.J2004.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, Delaunay triangulation, high dimension, interpolation, regression, approximation theory, uncertainty quantification, UQ},
}

% The publication for the XGBoost numerical software. XGBoost can be used to efficiently compute optimized gradient boosted decision trees on massive datasets via a fully-distributed algorithm that can be configured to run on Hadoop, SGE, and MPI. It can also be run on NVIDIA GPUs using CUDA. The software is fully open-source and written in highly optimized C++, though everyone uses it through its Python interface. The download is available at github.com/dmlc/xgboost. Most data science competition winners use XGBoost for tabular data
@inproceedings{chen2016xgboost,
	author = {Chen, Tianqi and Guestrin, Carlos},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	year = {2016},
	month = {8},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16)},
	pages = {785--794},
	organization = {ACM},
	location = {San Francisco, California, USA},
	doi = {10.1145/2939672.2939785},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	keywords = {machine learning, ML, scientific machine learning, SciML, decision trees, regression, classification},
}

% The classic textbook by Cheney and Light on the fundamentals of approximation theory for multivariate functions -- topics include: basics of interpolation, approximation theory, and linear operators; multivariate polynomials, their interpolation nodes, and error kernels; selecting good polynomial interpolants via Newton and Lagrange type methods; positive-definite functions, kernel interpretations, and good kernels for interpolation; basis functions, orthonormal bases, common bases for interpolation and convergence rates; Chebyshev nodes; B-splines, Box splines, and thin-plate splines; and basics of artificial neural networks. Other topics include wavelets, orthogonal projection algorithms, Hilbert spaces, and reproducing kernel Hilbert spaces (RKHS).
@book{cheney2009course,
	author = {Cheney, Elliott W. and Light, William A.},
	title = {A Course in Approximation Theory},
	year = {2009},
	month = {1},
	booktitle = {Graduate Studies in Mathematics},
	series = {Graduate Studies in Mathematics},
	publisher = {AMS},
	address = {Providence, RI, USA},
	doi = {10.1090/gsm/101},
	url = {http://www.ams.org/gsm/101},
	isbn = {9780821847985},
	issn = {1065-7339},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RBFs, high dimension, interpolation, regression, approximation theory},
}

% The Keras docs -- great and highly impactful open source Python software, needs no introduction. A simplified interface for quickly building neural networks and other deep learning models with various backends frameworks such as Tensorflow, jax, and Pytorch.
@misc{chollet2015keras,
	author = {Chollet, Fran\c{c}ois and others, },
	title = {Keras},
	year = {2015},
	howpublished = {GitHub repository},
	publisher = {GitHub},
    url = {https://keras.io},
	note = {Last accessed: Nov 2023},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, classification},
}

% A paper on the similarities and differences between kriging and Gaussian processes -- in practice, I see very little difference other than that people often perform more performance optimizations on Gaussian processes and consider Kriging to be a bit more strict
@article{christianson2022traditional,
	author = {Christianson, Ryan B and Pollyea, Ryan M and Gramacy, Robert B},
	title = {Traditional kriging versus modern Gaussian processes for large-scale mining data},
	year = {2022},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	publisher = {Wiley Online Library},
    url = {https://arxiv.org/abs/2207.10138},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, regression, uncertainty quantification, UQ},
}

% Andrew Conn's landmark paper on interpolation dataset geometry -- leads to the definition of sets being "well-poised" for interpolation, meaning that when the interpolation set's geometry meats some local geometric conditions (basically bounded away from singularity), then the resulting interpolant's error (and gradient / hessian errors) can be bounded and the resulting models can be used to perform gradient descent or SQP within a trust-region framework with guaranteed convergence
@article{conn2008geometry,
	author = {Conn, Andrew R and Scheinberg, Katya and Vicente, Lu{\'\i}s N},
	title = {Geometry of interpolation sets in derivative free optimization},
	year = {2008},
	month = {6},
	journal = {Mathematical programming},
	volume = {111},
	number = {1-2},
	pages = {141--172},
	publisher = {Springer},
	doi = {10.1007/s10107-006-0073-5},
	url = {http://link.springer.com/10.1007/s10107-006-0073-5},
	issn = {0025-5610},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, interpolation, approximation theory},
}

% Interesting paper on why it is generally OK to use local optimizers when solving non convex optimization problems in high-dimensional spaces. In general, in high-dimensional spaces, almost every critical point will be a saddle point with high probability. Therefore, first-order methods tend to perform very well on these problems as they converge quickly but are not attracted to saddle points and therefore tend to find the global optimum in the limit. The analysis of the probability that a critical point will be a saddle point is based on a spectral analysis of the hessian at each critical point other than the global minimum/maximum -- all of the eigenvalues must be positive or negative for the critical point to be a local minima / maxima, and the probability of this occurring decays as the number of eigenvalues grows with the dimension of the Hessian. The authors also experimentally validate these claims by extracting critical points from the loss landscapes of single layer MLPs trained on down-sampled versions of MNIST and CIFAR-10.
@inproceedings{dauphin2014identifying,
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	year = {2014},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {27},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/04192426585542c54b96ba14445be996-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, high dimension},
}

% DeBoor's classic textbook on splines and B-spline interpolation, covering polynomial interpolation, divided differences, and error kernels; node spacing and chebyshev nodes; piecewise linear and polynomial interpolation; piecewise polynomials as combination of basis functions; generalized B-spline via a recursive definition; algorithms for stably building and evaluating B-Splines; knot and node placement and error rates and diminishing returns; The Schoenberg-Whitney Theorem and B-splines as RKHS kernels; multivariate B-splines as tensor products; applications; and Fortran subroutines for all steps in the process.
@book{deboor1978practical,
	author = {de Boor, Carl R.},
	title = {A Practical Guide to Splines},
	year = {1978},
	month = {jan},
	booktitle = {Applied Mathematical Sciences},
	volume = {34},
	number = {149},
	publisher = {Springer-Verlag},
	address = {New York, NY, USA},
	doi = {10.2307/2006241},
	url = {https://www.researchgate.net/publication/200744645_A_Practical_Guide_to_Spline},
	issn = {0025-5718},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RBFs, high dimension, interpolation, regression, approximation theory},
}

% Original reference for Delaunay triangulations -- I have not read this, it's in French, but this is the proper way to cite the original idea for Delaunay triangulations
@article{delaunay1934sur,
	author = {Delaunay, B.},
	title = {Sur la sph\'ere vide},
	year = {1934},
	journal = {Bull. Acad. Science USSR VII: Class. Sci. Math.,},
    number = {6},
    url = {https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=im&paperid=4937&option_lang=eng},
	keywords = {machine learning, ML, scientific machine learning, SciML, Delaunay triangulation},
}

% A framework for deriving error bounds for neural networks in scientific machine learning applications, specifically for physics-informed neural networks (PINNs)
@article{deryck2022generic,
	author = {De Ryck, Tim and Mishra, Siddhartha},
	title = {Generic bounds on the approximation error for physics-informed (and) operator learning},
	year = {2022},
	journal = {Advances in Neural Information Processing Systems},
	volume = {35},
	pages = {10945--10958},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/46f0114c06524debc60ef2a72769f7a9-Paper-Conference.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, PINNs, regression, approximation theory, uncertainty quantification, UQ},
}

% BERT was one of the first transformer-based neural network architectures for solving language-related tasks, such as language translations, at Google. This was also by far the largest model of its time. BERT paved the way for modern large language models, probably moreso than the Attention is all you need paper
@inproceedings{devlin2019bert,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	year = {2019},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	pages = {4171--4186},
	organization = {Association for Computational Linguistics},
	location = {Minneapolis, Minnesota},
	doi = {10.18653/v1/N19-1423},
	url = {https://aclanthology.org/N19-1423/},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, transformers},
}

% CVXPY is an open source Python optimization and modeling language for solving convex optimization problems in a disciplined way (meaning that we ensure convexity through hard rules on the problem definition). From the lab of Stephen Boyd
@article{diamond2016cvxpy,
	author = {Diamond, Steven and Boyd, Stephen},
	title = {{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization},
	year = {2016},
	journal = {Journal of Machine Learning Research},
	volume = {17},
	number = {83},
	pages = {1--5},
	url = {http://jmlr.org/papers/v17/15-408.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% ECOS is an open source numerical software for solving second-order cone optimization problems, from the lab of Stephen Boyd. In my experience, this software is the best tool from Boyd's lab and the most robust to degeneracy
@inproceedings{domahidi2013ecos,
	author = {Domahidi, Alexander and Chu, Eric and Boyd, Stephen},
	title = {{ECOS}: {A}n {SOCP} solver for embedded systems},
	year = {2013},
	month = {7},
	booktitle = {European Control Conference (ECC)},
	pages = {3071--3076},
	organization = {IEEE},
	location = {Z{\"u}rich, Switzerland},
	doi = {10.23919/ECC.2013.6669541},
	url = {https://ieeexplore.ieee.org/document/6669541},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% The original paper for AdaGrad (adaptive subgradient method) which replaced the subgradient method with an adaptive estimate for the gradient, where each component of the gradient is rescaled by an adaptive estimate for the standard deviation in that direction based on previous iterates. This adaptive estimate for standard deviation in each axis-aligned direction serves as a diagonal approximation to the Hessian matrix, giving second-order like properties to the method and greatly improving the practical convergence. AdaGrad was very popular and considered the state-of-the-art optimization algorithm for training neural networks upon its initial release, but was quickly replaced by Adam, which added a Nesterov momentum esque smoothing to this adaptive gradient estimation in order to further improve convergence rates on nonsmooth, highly stochastic, and ill-conditioned problems
@article{duchi2011adaptive,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	year = {2011},
	journal = {Journal of Machine Learning Research},
	volume = {12},
	number = {61},
	pages = {2121--2159},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, neural networks},
}

% AutoDEUQ paper proposing that an ensemble of neural networks can be a more powerful and scalable tool for uncertainty quantification than a Bayesian neural network. They trained AutoDEUQ using DeepHyper in multiobjective mode. I don't think the code is available, but an example can be found on the DeepHyper GitHub
@inproceedings{egele2022autodeuq,
	author = {Egele, Romain and Maulik, Romit and Raghavan, Krishnan and Lusch, Bethany and Guyon, Isabelle and Balaprakash, Prasanna},
	title = {{AutoDEUQ}: Automated deep ensemble with uncertainty quantification},
	year = {2022},
	month = {8},
	booktitle = {2022 26th International Conference on Pattern Recognition (ICPR)},
	pages = {1908--1914},
	organization = {IEEE},
	location = {Montreal, QC, Canada},
	doi = {10.1109/icpr56361.2022.9956231},
	url = {https://ieeexplore.ieee.org/document/9956231/},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Gaussian process, regression, uncertainty quantification, UQ},
}

% Publication and whitepaper on Pathmind, an RL-based solver for simulation optimization problems. They also offer multiobjective support but only by using a priori scalarization provided by the user (so not real multiobjective support). This tool is not open source, it is a service provided by a YC startup of the same name. Therefore, it could be considered industry software
@inproceedings{farhan2020reinforcement,
	author = {Farhan, Mohammed and G{\"o}hre, Brett},
	title = {Reinforcement Learning in {AnyLogic} Simulation Models: A Guiding Example using {Pathmind}},
	year = {2020},
	month = {12},
	booktitle = {Proc. 2020 Winter Simulation Conference (WSC 2020)},
	pages = {3212--3223},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC48552.2020.9383916},
	url = {https://ieeexplore.ieee.org/document/9383916},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, reinforcement learning, RL},
}

% The culmination of a line of work from Google DeepMind on discovering better matrix factorization algorithms using reinforcement learning. They defined a tensor game for trying to solve matrix multiplication at a fixed size in fewer moves, then trained a RL agent to play the game using monte carlo tree search and training a neural network to predict a manageable subset of states and expected state outcomes since the number of states was too numerous for true MCTS. They were able to match the performance of Strassen's algorithm on 5x5 matrices (this is how recursive block-based recursive multiplication is done) and exceed performance on other sizes and rings (such as modular arithmetic rings)
@article{fawzi2022discovering,
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others, },
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	year = {2022},
	month = {10},
	journal = {Nature},
	volume = {610},
	number = {7930},
	pages = {47--53},
	publisher = {Nature Publishing Group UK London},
	doi = {10.1038/s41586-022-05172-4},
	url = {https://www.nature.com/articles/s41586-022-05172-4},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, neural networks, reinforcement learning, RL},
}

% The first paper where the ReLU activation function was used as the activation function in neural network models. This would later become the standard for many years (and still is for regressor models)
@article{fukushima1975cognitron,
	author = {Fukushima, Kunihiko},
	title = {Cognitron: A self-organizing multilayered neural network},
	year = {1975},
	journal = {Biological cybernetics},
	volume = {20},
	number = {3},
	pages = {121--136},
	publisher = {Springer},
	doi = {10.1007/bf00342633},
	url = {http://link.springer.com/10.1007/BF00342633},
	issn = {0340-1200},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, classification},
}

% Manisha's conference paper on SF-SFD -- the theory is far from ready, but the problem of concentration of measure in high-dimensions making robust sampling difficult and the ability to do something better than just random or latin hypercube sampling are clearly demonstrated
@inproceedings{garg2023sfsfd,
	author = {Garg, Manisha and Chang, Tyler H. and Raghavan, Krishnan},
	title = {{SF-SFD}: {S}tochastic optimization of {F}ourier coefficients for space-filling designs},
	year = {2023},
	month = {12},
	booktitle = {Proc. 2023 Winter Simulation Conference (WSC 2023)},
	pages = {3636--3646},
	organization = {IEEE},
	location = {Orlando, FL, USA},
	doi = {10.1109/WSC60868.2023.10408245},
	url = {https://ieeexplore.ieee.org/document/10408245/},
	keywords = {machine learning, ML, scientific machine learning, SciML, high dimension},
}

% A community reviewed textbook on Bayesian optimization theory and implementation. Very thorough description of Gaussian process and Bayesian optimization fundamentals and theory, common techniques and acquisition functions, and implementation details, drawbacks, and real-world challenges
@book{garnett2023bayesian,
	author = {Garnett, Roman},
	title = {Bayesian Optimization},
	year = {2023},
	publisher = {Cambridge University Press},
	url = {https://bayesoptbook.com},
	isbn = {978-1108425780},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, Gaussian process, regression},
}

% The original paper on the bias-variance tradeoff curve. Now widely debunked (in my opinion) the theory that machine learning and neural networks had to tradeoff between training accuracy and overfitting (which would lead to the idea of so-called "generalization error") and had to be combatted by limiting model size or regularization rules ruled AI theory for many, many years. One key insight in this paper that was perhaps ahead of its time, is that the important part of machine learning is the representation learning, in which sense training neural networks with backpropagation for regression problems is somewhat not special
@article{geman1992neural,
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren\'e},
	title = {Neural networks and the bias/variance dilemma},
	year = {1992},
	month = {jan},
	journal = {Neural Computation},
	volume = {4},
	number = {1},
	pages = {1--58},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	doi = {10.1162/neco.1992.4.1.1},
	url = {https://direct.mit.edu/neco/article/4/1/1-58/5624},
	issn = {0899-7667},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regularization, overfitting, regression, classification},
}

% Introducing a Delaunay density diagnostic -- using the Delaunay interpolant to calculate whether they have enough data to resolve the geometry of a problem for interpolation in the context of SciML. Essentially, they use DelaunaySparse to make predictions and slowly add data. Once they have enough data, the predictions begin converging at a linear rate. Then they have enough data to use whatever method they want to solve the problem
@techreport{gillette2022datadriven,
	author = {Gillette, Andrew and Kur, Eugene},
	title = {Data-driven geometric scale detection via Delaunay interpolation},
	year = {2022},
	institution = {arXiv math.NA},
    url = {https://arxiv.org/html/2203.05685v2},
	keywords = {machine learning, ML, scientific machine learning, SciML, Delaunay triangulation, interpolation, approximation theory, uncertainty quantification, UQ},
}

% Open source numerical software for the Delaunay density diagnostic -- using the Delaunay interpolant to calculate whether they have enough data to resolve the geometry of a problem for interpolation in the context of SciML. Essentially, they use DelaunaySparse to make predictions and slowly add data. Once they have enough data, the predictions begin converging at a linear rate. Then they have enough data to use whatever method they want to solve the problem
@article{gillette2024algorithm,
	author = {Gillette, Andrew and Kur, Eugene},
	title = {Algorithm 1049: The Delaunay Density Diagnostic},
	year = {2024},
	month = {12},
	journal = {ACM Trans. Math. Softw.},
	volume = {50},
	number = {4},
	articleno = {24},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3700134},
	url = {https://doi.org/10.1145/3700134},
	issn = {0098-3500},
	keywords = {machine learning, ML, scientific machine learning, SciML, Delaunay triangulation, interpolation, approximation theory, uncertainty quantification, UQ},
}

% Online paper with interactive visualizations explaining what Nesterov's momentum is and how it works intuitively by smoothing out optimization sample paths and preventing oscillations in the optimizer that occur do to poor problem conditioning. Then, they show how the problem conditioning appears as an often ignored constant in the convergence rate of gradient descent. All this is to show intuitively and mathematically that gradient descent with Nesterov's momentum will convergence faster in practice for ill-conditioned problems
@article{goh2017why,
	author = {Goh, Gabriel},
	title = {Why Momentum Really Works},
	year = {2017},
	month = {4},
	journal = {Distill},
	volume = {2},
	number = {4},
	publisher = {Distill Working Group},
	doi = {10.23915/distill.00006},
	url = {http://distill.pub/2017/momentum},
	issn = {2476-0757},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% Classical textbook that serves as the "bible" of matrix computations and computational linear algebra -- contains all the standard factorizations, the common algorithms for computing them, and their sensitiviy analyses, pivoting, some basic approximation theory, and the basics of iterative methods
@book{golub2013matrix,
	author = {Golub, Gene H. and Van Loan, Charles F.},
	title = {Matrix computations},
	year = {2013},
	edition = {4th},
	publisher = {Johns Hopkins University Press},
	doi = {10.56021/9781421407944},
	url = {https://www.press.jhu.edu/books/title/10678/matrix-computations},
	isbn = {978-1421407944},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra},
}

% Theorems on the curse of dimensionality when it comes to drawing data points in high-dimensional spaces. The main theorem implies that the convex hull of N points in D dimensions has volume ~0 for D sufficiently large -- this occurs because of a concentration of measure type result
@article{gorban2017stochastic,
	author = {Gorban, Alexander N and Tyukin, Ivan Yu},
	title = {Stochastic separation theorems},
	year = {2017},
	month = {10},
	journal = {Neural Networks},
	volume = {94},
	pages = {255--259},
	publisher = {Elsevier},
	doi = {10.1016/j.neunet.2017.07.014},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017301776},
	issn = {0893-6080},
	keywords = {machine learning, ML, scientific machine learning, SciML, high dimension, approximation theory},
}

% An interesting analysis by Bobby Gramacy on how adding a "nugget" (i.e., a small perturbation \varepsilon I to the diagonal of a matrix results in vastly improved numerical stability and therefore performance when modeling data science and other approximation problems via Gaussian processes
@article{gramacy2012cases,
	author = {Gramacy, Robert B. and Lee, Herbert K. H.},
	title = {Cases for the nugget in modeling computer experiments},
	year = {2012},
	month = {5},
	journal = {Statistics and Computing},
	volume = {22},
	number = {3},
	pages = {713--722},
	publisher = {Springer},
	doi = {10.1007/s11222-010-9224-x},
	url = {http://link.springer.com/10.1007/s11222-010-9224-x},
	issn = {0960-3174},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, RBFs, Gaussian process, high dimension, interpolation, regression, approximation theory},
}

% First published work using Geoff Hinton's unpublished optimization algorithm RMSProp (root mean squared propogation, an adjustment to AdaGrad using an adaptive learning rate in each dimension). The author uses RMSProp to train a recurrent neural network (RNN) with long short-term memory (LSTM) in order to generate handwritten digits
@techreport{graves2014generating,
	author = {Graves, Alex},
	title = {Generating Sequences With Recurrent Neural Networks},
	year = {2014},
	institution = {arXiv cs.NE preprint},
	url = {https://arxiv.org/abs/1308.0850},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, neural networks, RNNs},
}

% An error analysis of ReLU multilayer perceptrons (MLPs). Provides lower bounds on approximation error for feed-forward neural networks with ReLU activation functions. A good step in SciML, but we typically want upper bounds not lower bounds in practice
@article{guhring2020error,
	author = {G{\"u}hring, Ingo and Kutyniok, Gitta and Petersen, Philipp},
	title = {Error bounds for approximations with deep ReLU neural networks in $W^{s,p}$ norms},
	year = {2020},
	journal = {Analysis and Applications},
	volume = {18},
	number = {05},
	pages = {803--859},
	publisher = {World Scientific},
    doi = {10.1142/S0219530519410021},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, approximation theory, uncertainty quantification, UQ},
}

% The official publication of the open source numerical software numpy: the standard for basic multivariable computations, vector operations, and simple linear algebra in Python
@article{harris2020array,
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, St{\'{e}}fan J. van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and R{\'{i}}o, Jaime Fern{\'{a}}ndez del and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	title = {Array programming with {NumPy}},
	year = {2020},
	month = {9},
	journal = {Nature},
	volume = {585},
	number = {7825},
	pages = {357--362},
	publisher = {Springer Science and Business Media {LLC}},
	doi = {10.1038/s41586-020-2649-2},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra},
}

% Original publication of the Boltzman machine, an early type of AI model that made predictions by modeling the Ising spin-glass model with public input units (set by the user with the independent variables during inference) and hidden internal units that are correlated to eachother and the public units through learned Ising model weights and biases. Each unit has a binary {0, 1} state. The energy of the system is the output (prediction). The weights and biases are learned through simulated annealing, and in this way, the model is not unlike quantum annealing. The model learns a distribution of potential inference values for each input combination. Thus, it is a form of distribution learning. These were a commonly used alternative to neural networks for a time, and often cited as explainable alternatives. The binary state variables would later be replaced by sigmoidal activation functions (for smoothness)
@inproceedings{hinton1983optimal,
	author = {Hinton, Geoffrey E and Sejnowski, Terrence J},
	title = {Optimal perceptual inference},
	year = {1983},
	booktitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
	volume = {448},
	pages = {448--453},
	organization = {Citeseer},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b89e9f0cef5ace08946a7c07bf7284854c418445},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Boltzmann machine, regularization, overfitting, regression, classification},
}

% Original publication on using long-short term memory within a recurrent neural network framework to address the issue of vanishing gradients during training. The idea is to truncate gradients for certain blocks (short-term memory units) and chain these blocks to generate skip connections for long term memory. These would be the state-of-the-art in natural language processing and other sequential prediction tasks until BERT replaces them with transformer models.
@article{hochreiter1997long,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	title = {Long Short-Term Memory},
	year = {1997},
	month = {11},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735--1780},
	publisher = {MIT Press},
	doi = {10.1162/neco.1997.9.8.1735},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	issn = {0899-7667},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, transformers},
}

% An article on Google GlassBox research: Google's research division dedicated to interpretable machine learning
@article{hof2015google,
	author = {Hof, Robert D.},
	title = {Google Tries to Make Machine Learning a Little More Human},
	year = {2015},
	month = {nov},
	journal = {MIT Technology Review},
	url = {https://www.technologyreview.com/2015/11/05/165175/google-tries-to-make-machine-learning-a-little-more-human},
	note = {Last accessed: June 20, 2022},
	keywords = {machine learning, ML, scientific machine learning, SciML},
}

% The official publication for HiGHS numerical optimization open source software package and solver for linear programming problems. The first successful effort to parallelize a simplex method based solver, specifically, they have successfully parallelized the dual revised simplex approach. HiGHS is now the default solver in most scipy.optimize. HiGHS is written in C++ with Fortran, C, and Python interfaces -- support CPU and GPU parallelism, the authors don't get perfect scaling by any means but this is the first successful effort to parallelize linear programming and still an achievement
@article{huangfu2018parallelizing,
	author = {Huangfu, Qi and Hall, JA Julian},
	title = {Parallelizing the dual revised simplex method},
	year = {2018},
	month = {3},
	journal = {Mathematical Programming Computation},
	volume = {10},
	number = {1},
	pages = {119--142},
	publisher = {Springer},
	doi = {10.1007/s12532-017-0130-5},
	url = {http://link.springer.com/10.1007/s12532-017-0130-5},
	issn = {1867-2949},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% An analysis of how adversarial examples for neural network and other AI models are typically indicative of the model using highly predictive but brittle features to make predictions -- i.e., features that are highly predictive but nevertheless not robust enough to use for actually making predictions
@inproceedings{ilyas2019adversarial,
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch\'{e}-Buc, F. d\textquotesingle and Fox, E. and Garnett, R.},
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	year = {2019},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {32},
	pages = {125--136},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, classification},
}

% Original tech report where recurent neural networks were originally applied at scale to a language processing (next word prediction) application
@techreport{jordan1986serial,
	author = {Jordan, Michael I.},
	title = {Serial order: a parallel distributed processing approach},
	year = {1986},
	institution = {Institute for Cognitive Science, University of California},
	address = {San Diego, La Jolla, CA, USA},
	url = {https://www.osti.gov/biblio/6910294},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RNNs},
}

% One of the earliest applications of DelunaySparse: predicting oblique shocks in supersonic flows for aerospace engineering applications. This is also a great example of an early example of scientific machine learning since the authors were also using neural networks and other more modern AI methods to make predictions
@inproceedings{jrad2019selflearning,
	author = {Jrad, Mohamed and Kapania, Rakesh K. and Schetz, Joseph A. and Watson, Layne T.},
	title = {Self-Learning, Adaptive Software for Aerospace Engineering Applications: Example of Oblique Shocks in Supersonic Flow},
	year = {2019},
	month = {1},
	booktitle = {AIAA Scitech 2019 Forum},
	organization = {AIAA},
	location = {San Diego, CA, USA},
	doi = {10.2514/6.2019-1704},
	url = {https://arc.aiaa.org/doi/10.2514/6.2019-1704},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Delaunay triangulation, interpolation, regression},
}

% Introducing Dragonfly: an open source numerical software package for solving neural architecture search problems via Bayesian optimization and solving an optimal transport problem to evaluate the distance between two networks. Considered a bit of a landmark paper for neural network architecture search problems. The open source Python software is widely used for a variety of applications outside NAS, including molecular discovery
@article{kandasamy2020tuning,
	author = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabus and Xing, Eric P.},
	title = {Tuning Hyperparameters without Grad Students: Scalable and Robust {Bayesian} Optimisation with {Dragonfly}},
	year = {2020},
	journal = {Journal of Machine Learning Research},
	volume = {21},
	number = {81},
	pages = {1--27},
	url = {http://jmlr.org/papers/v21/18-223.html},
	git = {http://github.com/dragonfly/dragonfly},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, Gaussian process, uncertainty quantification, UQ, representation learning},
}

% An introductory-level paper to theory-guided data science. I haven't read but I assume it covers the same topics he taught in his class at Virginia Tech
@article{karpatne2017theoryguided,
	author = {Karpatne, Anuj and Atluri, Gowtham and Faghmous, James H. and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
	title = {Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data},
	year = {2017},
	month = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	volume = {29},
	number = {10},
	pages = {2318--2331},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/TKDE.2017.2720168},
	url = {http://ieeexplore.ieee.org/document/7959606},
	issn = {1041-4347},
	keywords = {machine learning, ML, scientific machine learning, SciML, regression, classification},
}

% Landmark paper showing that given a target function f, set of scattered data (xi, f(xi)), and family of SPD kernel functions K(x1, x2) spanning the RKHS, the unique basis for the RKHS is K(xi, x), and furthermore the minimizer for any definite integral of Lf for (where L is any linear operator) can be found by solving a least squares problem with the kernel matrix K(xi, xj)*b = f(xi) for any pair xi, xj in (xi, f(xi)). This is the theoretical basis for all RBF interpolation and regularized RBFs / RBFs with tail.
@article{kimeldorf1971some,
	author = {Kimeldorf, George and Wahba, Grace},
	title = {Some results on {Tchebycheffian} spline functions},
	year = {1971},
	month = {1},
	journal = {Journal of mathematical analysis and applications},
	volume = {33},
	number = {1},
	pages = {82--95},
	publisher = {Elsevier},
	doi = {10.1016/0022-247x(71)90184-3},
	url = {https://pages.stat.wisc.edu/~wahba/ftp1/oldie/kw71.pdf},
	issn = {0022-247X},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RBFs, high dimension, interpolation, regression, approximation theory},
}

% The original paper defining variational autoencoders, a standard practice in performing dimension reduction and training models that encode continuous latent spaces. The idea being to train and optimize an encoder neural network model whose posterior is a continuous latent space to generate samples in the latent space distribution based on inputs from the original dataset without "losing information", then jointly train a decoder model that samples the latent distribution to produce the original observations. The idea being to randomly sample new data points that look like the original data. When trained jointly, these models can also be used as embedder/extractor or compression/decompression pairs.
@inproceedings{kingma2014autoencoding,
	author = {Kingma, Diederik P and Welling, Max},
	title = {Auto-encoding variational {B}ayes},
	year = {2014},
	booktitle = {2nd International Conference on Learning Representations (ICLR 2014)},
	url = {https://arxiv.org/abs/1312.6114},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning, dimension reduction},
}

% The original paper on Adam: an adaptive gradient and moment estimator that uses second order moments to approximate curvature (i.e., Hessian information) in order to accelerate the convergence of AdaGrad. In particular, this means applying Nesterov's momentum to both the gradient and curvature estimations. From 2015-2024 this was the state-of-the-art algorithm for optimization of neural network weights during training, and was what was typically meant when people talked about stochastic gradient descent.
@inproceedings{kingma2015adam,
	author = {Kingma, Diedrik and Ba, Jimmy},
	title = {Adam: A method for stochastic optimization},
	year = {2015},
	booktitle = {3rd International Conference on Learning Representations (ICLR 2015)},
	numpages = {11},
	location = {San Diego, CA, USA},
	url = {https://arxiv.org/abs/1412.6980},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, neural networks},
}

% The original paper on neural operators, a now ubiquitous method in scientific machine learning where an operator (i.e., for solving PDEs) between infinite dimensional function spaces is learned by composing integral operator basis functions and regular ReLU layers
@article{kovachki2021neural,
	author = {Kovachki, Nikola B. and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew M. and Anandkumar, Anima},
	title = {Neural Operator: Learning Maps Between Function Spaces},
	year = {2021},
	journal = {CoRR},
	volume = {abs/2108.08481},
    url = {https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, neural operators, approximation theory},
}

% The official publication for AlexNet -- one of the first truly massive overparameterized convolutional neural networks, which threw away a lot of the conventional wisdom around overfitting and achieved state-of-the-art performance and fine generalization errors on the ImageNet benchmark problem. This could be considered the beginning of "deep" learning in the sense of adding many many layers
@inproceedings{krizhevsky2012imagenet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	year = {2012},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {25},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, benchmarking, neural networks, CNNs, overfitting, classification},
}

% Stochastic approximation algorithm (i.e., stochastic gradient descent) and how to analyze its radius of convergence for a fixed step-size -- you can decay its step size at a square-summable but not summable rate to guarantee convergence in the limit
@article{lai2003stochastic,
	author = {Lai, Tze Leung},
	title = {Stochastic approximation},
	year = {2003},
	journal = {The annals of Statistics},
	volume = {31},
	number = {2},
	pages = {391--406},
	publisher = {Institute of Mathematical Statistics},
    url = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-2/Stochastic-approximation-invited-paper/10.1214/aos/1051027873.full},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% Peter Lax's classic textbook on functional analysis and all the core theorems Banach spaces, Hilbert spaces, and approximation theory
@book{lax2002functional,
	author = {Lax, Peter D.},
	title = {Functional analysis},
	year = {2002},
	series = {Pure and Applied Mathematics (New York)},
	pages = {xx--580},
	publisher = {Wiley-Interscience [John Wiley \& Sons], New York},
	isbn = {0-471-55604-1},
    url = {https://books.google.com/books?hl=en&lr=&id=18VqDwAAQBAJ&oi=fnd&pg=PR17&ots=8FU4i_jAff&sig=Lh4GrUCrS_gt-HKqHSq4X7BaxMs#v=onepage&q&f=false},
	keywords = {machine learning, ML, scientific machine learning, SciML, approximation theory},
}

% LeCun et al. show that convolutional nets can outperform all other methods on handwritten digit classification and other perception problems
@article{lecun1989backpropagation,
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	year = {1989},
	month = {12},
	journal = {Neural Computation},
	volume = {1},
	number = {4},
	pages = {541--551},
	publisher = {MIT Press},
	doi = {10.1162/neco.1989.1.4.541},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	issn = {0899-7667},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, CNNs, classification},
}

% LeCun's original paper on convolutional neural network architecture -- the first example of a problem where neural networks trained with backpropagation actually outperformed other models, and one of the early examples of the magic of representation learning and how exploiting problem structure is essential to making machine learning work. This remained the state-of-the art in image classification and image processing for over 20 years until the event of transformers, and whether transformers are better for image problems remains debatable
@article{lecun1995convolutional,
	author = {LeCun, Yann and Bengio, Yoshua and others, },
	title = {Convolutional networks for images, speech, and time series},
	year = {1995},
	journal = {The handbook of brain theory and neural networks},
	volume = {3361},
	number = {10},
	numpages = {1995},
	publisher = {Citeseer},
    url = {https://www.cs.utoronto.ca/~bonner/courses/2016s/csc321/readings/Convolutional%20networks%20for%20images,%20speech,%20and%20time%20series.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, CNNs, transformers, representation learning, classification},
}

% LeCun et al. show that convolutional nets can outperform all other methods on handwriting classification for efficient document processing
@article{lecun1998gradientbased,
	author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	title = {Gradient-based learning applied to document recognition},
	year = {1998},
	journal = {Proceedings of the IEEE},
	volume = {86},
	number = {11},
	pages = {2278--2324},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	doi = {10.1109/5.726791},
	url = {http://ieeexplore.ieee.org/document/726791/},
	issn = {0018-9219},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, CNNs, classification},
}

% The official publication of SMAC3 -- the latest version of SMAC from the automl group. SMAC was the first hyperparameter optimization and neural architecture search software to use random forest surrogates for modeling the hyperparameter configuration space. They also use a hierarchy of hyperparameters to handle "hidden parameters". They support multi and single-fidelity NAS applications. The open source software is written in Python and available from: github.com/automl/SMAC3
@article{lindauer2022smac3,
	author = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, André and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, René and Hutter, Frank},
	title = {{SMAC3}: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
	year = {2022},
	journal = {Journal of Machine Learning Research},
	volume = {23},
	number = {54},
	pages = {1--9},
	url = {http://jmlr.org/papers/v23/21-0888.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, decision trees, uncertainty quantification, UQ},
}

% A paper on the Delaunay triangulation learner -- an alternative to ReLU multilayer perceptrons where they fit a Delaunay triangulation to a small set of nodes and then train the response values of those nodes to make the Delaunay interpolant match the data
@inproceedings{liu2019nonparametric,
	author = {Liu, Yehong and Yin, Guosheng},
	title = {Nonparametric functional approximation with {D}elaunay triangulation learner},
	year = {2019},
	month = {11},
	booktitle = {Proceedings of the 2019 IEEE International Conference on Big Knowledge (ICBK)},
	pages = {167--174},
	organization = {IEEE},
	location = {Beijing, China},
	doi = {10.1109/icbk.2019.00030},
	url = {https://ieeexplore.ieee.org/document/8944414},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Delaunay triangulation, interpolation, regression, classification},
}

% Official citation for the open source numerical Python software for evaluating SHAP scores for interpreting feature importance in a model agnostic way for various machine learning (primarily scientific machine learning) applications. Very popular in many fields, including finance, computational medicine, etc. github.com/shap/shap Based on the original publication by Shapley + some additional methods and improvements proposed by the authors and previous works
@inproceedings{lundberg2017unified,
	author = {Lundberg, Scott M and Lee, Su-In},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	title = {A Unified Approach to Interpreting Model Predictions},
	year = {2017},
	booktitle = {Advances in Neural Information Processing Systems 30},
	pages = {4765--4774},
	organization = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, high dimension, dimension reduction, regression, classification},
}

% Fitting nonparametric distribution models by interpolating the cumulative distribution functions -- the application is that the CDFs measure HPC throughput distributions, so it is also a little bit of a HPC performance modeling paper
@inproceedings{lux2018nonparametric,
	author = {Lux, Thomas C. H. and Watson, Layne T. and Chang, Tyler H. and Bernard, Jon and Li, Bo and Yu, Xiadong and Xu, Li and Back, Godmar and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Nonparametric distribution models for predicting and managing computational performance variability},
	year = {2018},
	month = {4},
	booktitle = {Proceedings of IEEE SoutheastCon 2018},
	pages = {1--7},
	organization = {IEEE},
	location = {St. Petersburg, FL, USA},
	doi = {10.1109/secon.2018.8478814},
	url = {https://ieeexplore.ieee.org/document/8478814},
	keywords = {machine learning, ML, scientific machine learning, SciML, interpolation, uncertainty quantification, UQ},
}

% Using various interpolation, neural networks, and other scientific machine learning methods to predict and model HPC performance based on system configuration parameters -- explores Delaunay interpolation, support vector regressors, Shepard's method, and multilayer perceptrons
@inproceedings{lux2018predictive,
	author = {Lux, Thomas C. H. and Watson, Layne T. and Chang, Tyler H. and Bernard, Jon and Li, Bo and Xu, Li and Back, Godmar and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Predictive modeling of {I/O} characteristics in high performance computing systems},
	year = {2018},
	booktitle = {Proceedings of the 2018 Spring Simulation Conference (SpringSim 2018), the 26th High Performance Computing Symposium (HPC '18)},
	articleno = {8},
	organization = {SCS},
	location = {Baltimore, MD, USA},
    url = {https://par.nsf.gov/servlets/purl/10111447},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Delaunay triangulation, interpolation, regression},
}

% Our paper proposing test functions for convex optimization problems with very specific properties, in order to gauge the effectiveness and robustness of various techniques subject to various forms of degeneracy
@inproceedings{lux2020analytic,
	author = {Lux, Thomas C. H. and Chang, Tyler H.},
	title = {Analytic test functions for generalizable evaluation of convex optimization techniques},
	year = {2020},
	month = {3},
	booktitle = {Proc. IEEE SoutheastCon 2020},
	numpages = {8},
	organization = {Institute of Electrical and Electronics Engineers},
	location = {Raleigh, NC, USA},
	doi = {10.1109/SoutheastCon44009.2020.9368254},
	url = {https://ieeexplore.ieee.org/document/9368254/},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, high dimension},
}

% Error bounds for piecewise linear interpolation within a simplex (i.e., Delaunay interpolation error bounds) for scattered data. Also, critically, empirical evidence that you can just interpolate noisy data and it does just as well as true regression in high dimensions. First hand evidence that feeds my opinion that "overfitting is a lie"
@article{lux2021interpolation,
	author = {Lux, Thomas C. H. and Watson, Layne T. and Chang, Tyler H. and Bernard, Jon and Li, Bo and Xu, Li and Back, Godmar and Butt, Ali R. and Cameron, Kirk W. and Hong, Yili},
	title = {Interpolation of sparse high-dimensional data},
	year = {2021},
	month = {9},
	journal = {Numerical Algorithms},
	volume = {88},
	number = {1},
	pages = {281--313},
	publisher = {Springer},
	doi = {10.1007/s11075-020-01040-2},
	url = {https://link.springer.com/10.1007/s11075-020-01040-2},
	issn = {1017-1398},
	keywords = {machine learning, ML, scientific machine learning, SciML, overfitting, Delaunay triangulation, high dimension, interpolation, regression, approximation theory, uncertainty quantification, UQ},
}

% Thomas' open source montone quintic spline interpolation software -- a high order spline interpolant implemented in modern Fortran. Really nice high performance numerical software. Available open source on Dr. Watson's GitHub. His main application was distribution modeling, but I think for noisy data, lower order methods may actually be more robust
@article{lux2023algorithm,
	author = {Lux, Thomas C. H. and Watson, Layne T. and Chang, Tyler H. and Thacker, William I.},
	title = {Algorithm 1031: {MQSI}---{M}onotone quintic spline interpolation},
	year = {2023},
	month = {3},
	journal = {ACM Transactions on Mathematical Software},
	volume = {49},
	number = {1},
	articleno = {6},
	numpages = {17},
	publisher = {Association of Computing Machinery},
	doi = {10.1145/3570157},
	url = {https://dl.acm.org/doi/10.1145/3570157},
	issn = {0098-3500},
	keywords = {machine learning, ML, scientific machine learning, SciML, interpolation},
}

% Original paper on Bayesian neural networks (or just Bayesian networks), which learn joint-probability distributions of neural network weights and biases instead of just single sets. The original proposal was as a means of regularization, to avoid overfitting. However, sampling networks from this distribution can also be used as a means of uncertainty quantification.
@article{mackay1992practical,
	author = {MacKay, David JC},
	title = {A practical Bayesian framework for backpropagation networks},
	year = {1992},
	month = {5},
	journal = {Neural computation},
	volume = {4},
	number = {3},
	pages = {448--472},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	doi = {10.1162/neco.1992.4.3.448},
	url = {https://direct.mit.edu/neco/article/4/3/448-472/5654},
	issn = {0899-7667},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Gaussian process, regularization, overfitting, regression, classification, uncertainty quantification, UQ},
}

% A thorough foundation on analysis of RBF interpolant error bounds, through the lens of the RKHS
@article{manton2015primer,
	author = {Manton, Jonathan H and Amblard, Pierre-Olivier and others, },
	title = {A primer on reproducing kernel {H}ilbert spaces},
	year = {2015},
	journal = {Foundations and Trends{\textregistered} in Signal Processing},
	volume = {8},
	number = {1--2},
	pages = {1--126},
	publisher = {Now Publishers, Inc.},
    doi = {10.1561/2000000050},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, interpolation, approximation theory},
}

% Original word2vec publication: embedding words into a continuous vector space in a "dense" way, meaning that the embedding is somewhat minimal and no extra dimensions are added and word relationships are preserved (as in semantic regularization). The authors show that a relatively small and simple model with semantic regularization is sufficient to train such an embedding in less than a day, and the resulting word2vec embedding layer is fast and cheap to evaluate. The word2vec embedding would go on to be the basis for most early language models, until being replaced by BPE for transformer models and modern LLMs. The software is available for download from Google: https://code.google.com/archive/p/word2vec
@inproceedings{mikolov2013efficient,
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	title = {Efficient estimation of word representations in vector space},
	year = {2013},
	booktitle = {1st International Conference on Learning Representations (ICLR 2013)},
	location = {Scottsdale, AZ, USA},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60&noteld=C8Vn84fq},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning},
}

% Introduces the idea of semantic regularization for generating word embeddings. This is a massive improvement over bag of word embeddings, where each word gets a dimension and sentences / documents are embedded as vectors listing the number of occurrences for each word. Instead, for this work, words are compressed into a continuous latent space using a RNN-based compression model. In the compression, the embedding is regularized by relationships between words. For example, embedding(king):embedding(queen) ~ embedding(man):embedding(woman) and embedding(clothes):embedding(shirt) ~ embedding(dishes):embedding(bowl). These relationships are enforced using the cosin similarity score of their embeddings. The authors also collected a massive dataset of english words and the corresponding relationships, and successfully trained the first word to vector embedding that (mostly) preserves these relationships, meaning that all vector operations remain meaningful
@inproceedings{mikolov2013linguistic,
	author = {Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
	title = {Linguistic regularities in continuous space word representations},
	year = {2013},
	booktitle = {Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
	pages = {746--751},
    url = {https://aclanthology.org/N13-1090.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning, high dimension, dimension reduction},
}

% Original reference for barycentric weight based interpolation -- I have not read this, it's in German, but this is the proper way to cite barycentric interpolation
@book{mobius1827der,
	author = {M\"obius, August Ferdinand},
	title = {Der barycentrische Calcul},
	year = {1827},
	pages = {1--388},
	publisher = {Verlag von Johann Ambrosius Barth},
    address = {Leipzig, Germany},
    url = {https://books.google.com/books?id=eFPluv_UqFEC&printsec=frontcover#v=onepage&q&f=false},
	keywords = {machine learning, ML, scientific machine learning, SciML, interpolation},
}

% MORDRED: A 3D molecular descriptor calculator, which is widely used for embedding molecules into a continuous latent space (parameterized by their descriptors) which can be used to solve chemical property optimization problems. The MORDRED software is available open source in Python.
@article{moriwaki2018mordred,
	author = {Moriwaki, Hirotomo and Tia, Yu-Shi and Kawashita, Norihito and Takagi, Tatsuya},
	title = {Mordred: a molecular descriptor calculator},
	year = {2018},
	month = {12},
	journal = {Journal of Cheminformatics},
	volume = {10},
	number = {1},
	articleno = {4},
	numpages = {14},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1186/s13321-018-0258-y},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0258-y},
	issn = {1758-2946},
	keywords = {machine learning, ML, scientific machine learning, SciML, representation learning},
}

% The classical textbook on response surface methodology and modeling practices. Contains useful information on the basic framework and applications of RSM. Also a useful reference for many of the options for specific techniques: Chapter 7 is a good reference for basic techniques in multiobjective RSM and Chapters 8-9 surveys the basic methods in design-of-experiments
@book{myers2016response,
	author = {Myers, Raymond H. and Montgomery, Douglas C. and Anderson-Cook, Christine M.},
	title = {Response Surface Methodology: Process and Design Optimization Using Designed Experiments},
	year = {2016},
	edition = {4},
	publisher = {John Wiley \& Sons, Inc.},
	address = {Hoboken, NJ, USA},
	isbn = {9781118916032},
    url = {https://books.google.com/books?hl=en&lr=&id=T-BbCwAAQBAJ&oi=fnd&pg=PR13&dq=Response+Surface+Methodology:+Process+and+Product+Optimization+Using+Designed+Experiments,+4th+Edition&ots=O3jdPna83T&sig=IimJlE46JBVkHOu7eik3RN9Z5GA#v=onepage&q=Response%20Surface%20Methodology%3A%20Process%20and%20Product%20Optimization%20Using%20Designed%20Experiments%2C%204th%20Edition&f=false},
	keywords = {machine learning, ML, scientific machine learning, SciML, regression},
}

% Paper where ReLU functions were first used to replace sigmoidal activations in a restricted Boltzmann machine with much success. After this, ReLU became the standard activation function in all neural network regressive models
@inproceedings{nair2010rectified,
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	title = {Rectified linear units improve restricted boltzmann machines},
	year = {2010},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	series = {ICML'10},
	pages = {807--814},
	organization = {Omnipress},
	location = {Haifa, Israel},
	isbn = {9781605589077},
    url = {https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Boltzmann machine, regression, classification},
}

% Original publication on Nesterov's momentum. I haven't read it (it is hard to find a copy and likely in Russian) but this is the preferred citation. The equation for Nesterov momentum in gradient descent is instead of using the update: x' = x - a*g(x), use x' = x - a*g(y) - b*v where y = x - b*v and v = b*v + a*g(x) -- in this equation, b*v is the momentum term which smooths out poor conditioning in the problem by encouraging the algorithm to continue in the direction it was headed instead of oscillating. Nesterov proves that this term also leads to better convergence rates. For best results, b is usually chosen to be a large value such as 0.9 or 0.99
@article{nesterov1983method,
	author = {Nesterov, Yurii},
	title = {A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
	year = {1983},
	journal = {Dokl. Akad. Nauk SSSR},
	volume = {269},
    number = {3},
	pages = {543--547},
    url = {https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization},
}

% Not an original paper, as L1 and L2 regularization have been around since the beginning of machine learning and applied math, and are generally not attributed to anyone in particular, but a nice review paper on the usage and effects of L1 and L2 regularization in machine learning and neural network training with back propagation. Regularization was considered an essential part of neural network training (and still is in scientific machine learning) for many many years.
@inproceedings{ng2004feature,
	author = {Ng, Andrew Y.},
	title = {Feature selection, L1 vs. L2 regularization, and rotational invariance},
	year = {2004},
	booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
	series = {ICML '04},
	numpages = {78},
	organization = {Association for Computing Machinery},
	location = {Banff, Alberta, Canada},
	doi = {10.1145/1015330.1015435},
	url = {https://doi.org/10.1145/1015330.1015435},
	isbn = {1581138385},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regularization, overfitting, dimension reduction, regression, classification},
}

% An early review paper on methods and algorithms in "geometric learning", i.e., a form of topological data analysis that focuses on the usage of geometric algorithms and modeling for machine learning and data science applications
@article{omohundro1990geometric,
	author = {Omohundro, Stephen M.},
	title = {Geometric learning algorithms},
	year = {1990},
	month = {6},
	journal = {Physica D: Nonlinear Phenomena},
	volume = {42},
	number = {1},
	pages = {307--321},
	publisher = {Elsevier BV},
	doi = {10.1016/0167-2789(90)90085-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0167278990900854},
	issn = {0167-2789},
	keywords = {machine learning, ML, scientific machine learning, SciML, dimension reduction, regression, classification},
}

% Original definition of RBF networks, an early form of neural networks that focused on the usage of combining RBF basis functions. These networks are not used much anymore, although occasionally still come up in the context of scientific machine learning theory and proofs
@article{park1991universal,
	author = {Park, Jooyoung and Sandberg, Irwin W},
	title = {Universal approximation using radial-basis-function networks},
	year = {1991},
	month = {6},
	journal = {Neural computation},
	volume = {3},
	number = {2},
	pages = {246--257},
	publisher = {MIT Press},
	doi = {10.1162/neco.1991.3.2.246},
	url = {https://direct.mit.edu/neco/article/3/2/246-257/5580},
	issn = {0899-7667},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RBFs, regression, approximation theory},
}

% The official publication for pytorch a gold standard in open source software, providing automatic differentiation and numerical linear algebra in Python, targeted at implementing deep learning algorithms. Pytorch is pretty much a standard in not just open source software, but also machine learning software, and also numerical software
@inproceedings{paszke2019pytorch,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch\'{e}-Buc, F. d\textquotesingle and Fox, E. and Garnett, R.},
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	year = {2019},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = {32},
	pages = {1--12},
	organization = {Curran Associates, Inc.},
	url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, autograd, algorithmic differentiation, backpropagation, computational linear algebra, neural networks},
}

% Anita's commentary paper on recent advancements in SciML for computable phenotypes in the field of computational medicine (i.e., computing medical risks from patient data without human intervention). She congratulates recent advancments in using uncertainty aware, well-validated SciML methods such as ensemble models and gradient-boosted decision trees (XGBoost) to implement and calibrate (optimize) computable phenotypes in real clinical studies
@article{patel2025marriage,
	author = {Patel, Anita K.},
	title = {The Marriage of Computable Phenotypes With Machine Learning—A Pathway to Evidence-Based Care for Critically Ill Children},
	year = {2025},
	month = {02},
	journal = {JAMA Network Open},
	volume = {8},
	number = {2},
	pages = {e2457422--e2457422},
	publisher = {American Medical Association (AMA)},
	doi = {10.1001/jamanetworkopen.2024.57422},
	url = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2829845/patel\_2025\_ic\_240355\_1738013025.0942.pdf},
	issn = {2574-3805},
	keywords = {machine learning, ML, scientific machine learning, SciML, decision trees, regression, classification, uncertainty quantification, UQ},
}

% Original paper proposing PCA for dimension reduction. PCA is equivalent to taking the SVD of a the data matrix A = USV^T (where A \in R^(n X d) then truncating the first r < d columns of U, S, and V such that all the entries in S >= eps. This reduces the dimension of the data in A from d->r while maintaining that the reconstruction error (information loss) is O(eps^2). There is also a statistical explanation that I find less concise, in terms of minimizing the variance in truncated dimensions.
@article{pearson1901liii,
	author = {Pearson, Karl},
	title = {{LIII.} On lines and planes of closest fit to systems of points in space},
	year = {1901},
	month = {11},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	volume = {2},
	number = {11},
	pages = {559--572},
	publisher = {Taylor \& Francis},
	doi = {10.1080/14786440109462720},
	url = {https://www.tandfonline.com/doi/full/10.1080/14786440109462720},
	issn = {1941-5982},
	keywords = {machine learning, ML, scientific machine learning, SciML, representation learning, high dimension, dimension reduction},
}

% The official publication for scikit-learn a gold standard in open source software, providing a clean interface to several standard implementations of numerical approximation, optimization, machine learning, and deep learning algorithms. Scikit-learn is pretty much a standard in not just open source software, but also machine learning software, and also numerical software
@article{pedregosa2011scikitlearn,
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others, },
	title = {Scikit-learn: Machine learning in {P}ython},
	year = {2011},
	journal = {Journal of Machine Learning Research},
	volume = {12},
	pages = {2825--2830},
	url = {https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, Gaussian process, regression, classification},
}

% A thorough analysis and review article by Powell on how to bound the error of thin-plate spline (TPS) RBF interpolants for scattered data
@article{powell1994uniform,
	author = {Powell, Michael JD},
	title = {The uniform convergence of thin plate spline interpolation in two dimensions},
	year = {1994},
	month = {6},
	journal = {Numerische Mathematik},
	volume = {68},
	number = {1},
	pages = {107--128},
	publisher = {Springer},
	doi = {10.1007/s002110050051},
	url = {http://link.springer.com/10.1007/s002110050051},
	issn = {0029-599X},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, interpolation, approximation theory},
}

% Official publication where PINNs (physics-informed neural networks) were first defined by Karniadakis' lab at Brown. Sometimes credited with starting the field of scientific machine learning (SciML) -- or at least boosting its popularity to the main stream
@article{raissi2019physicsinformed,
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	year = {2019},
	month = {2},
	journal = {Journal of Computational Physics},
	volume = {378},
	pages = {686--707},
	publisher = {Elsevier BV},
	doi = {10.1016/j.jcp.2018.10.045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
	issn = {0021-9991},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, PINNs, regression, approximation theory},
}

% Attempting to make Gaussian process interpolation (i.e., Kriging) numerically stable as the minimum pairwise distance between points shrinks. In particular, they propose a lower bound on the nugget that must be added to the diagonal to guarantee a sufficient distance to singularity
@article{ranjan2011computationally,
	author = {Ranjan, Pritam and Haynes, Ronald and Karsten, Richard},
	title = {A computationally stable approach to Gaussian process interpolation of deterministic computer simulation data},
	year = {2011},
	month = {11},
	journal = {Technometrics},
	volume = {53},
	number = {4},
	pages = {366--378},
	publisher = {Taylor \& Francis},
	doi = {10.1198/tech.2011.09141},
	url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.09141},
	issn = {0040-1706},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, regression, approximation theory},
}

% The book that scipy and scikit-learn cite when it comes to Gaussian process regression fundamentals and algorithms. It's a good thorough book, and I find some of the error bound derivations to be useful, but I generally prefer Garnet's book
@book{rasmussen2006gaussian,
	author = {Rasmussen, Carl Edward and Williams, Christopher KI and others, },
	title = {Gaussian processes for machine learning},
	year = {2006},
	volume = {2},
	publisher = {The MIT Press},
    url = {https://gaussianprocess.org/gpml},
    isbn = {978-0-262-18253-9},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, regression, uncertainty quantification, UQ},
}

% One of the original papers proposing that overfitting, the bias variance tradeoff curve, and generalization error are all misunderstood and misused ideas. The authors present a true holdout set of lost and never-before-seen ImageNet and CIFAR-10 data, and show that massive interpolatory models (models trained to zero error) generalize just as well as the well-regularized models on this new data (better because their training error was lower and total error = training error + generalization error). Ben would go on to claim that (1) you want to interpolate a lot of data that we claim we don't want to overfit (such as image data). (2) Models that interpolate don't generalize poorly in practice and large overparameterized models always perform better in practice. And (3) Hyperparameter tuning is basically a form of training on the test set in order to find the best model that interpolates the training data. This paper was an eye-opener for me personally, and lead me to firmly believe in interpolation for high-dimensional data
@inproceedings{recht2019imagenet,
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	title = {Do {I}mage{N}et Classifiers Generalize to {I}mage{N}et?},
	year = {2019},
	month = {09--15 Jun},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	series = {Proceedings of Machine Learning Research},
	volume = {97},
	pages = {5389--5400},
	organization = {PMLR},
	url = {https://proceedings.mlr.press/v97/recht19a.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, CNNs, regularization, overfitting, high dimension, interpolation, classification},
}

% The calculus of simplex gradients: a detailed analysis of simplex gradients and their properties for approximating true gradients when solving derivative-free and blackbox optimization problems
@article{regis2015calculus,
	author = {Regis, Rommel G.},
	title = {The calculus of simplex gradients},
	year = {2015},
	month = {6},
	journal = {Optimization Letters},
	volume = {9},
	number = {5},
	pages = {845--865},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1007/s11590-014-0815-x},
	url = {http://link.springer.com/10.1007/s11590-014-0815-x},
	issn = {1862-4472},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, high dimension, interpolation, approximation theory, uncertainty quantification, UQ},
}

% Rosenblatt's original paper proposing the multi-layer perceptron with an input layer, output layer, and a single hidden layer: this is the foundation for all AI and neural network research. It is proposed as a simulation model for biologists and psychologists to study brain function. It would not be useful as a tool for regression, classification, or prediction for many years until the advent of representation learning. He originally used a step (discontinuous) activation function since it was well-known from functional analysis and approximation theory that the span of these functions is dense in L2 space. The sigmoidal activation would later be introduced as a continuous smoothing of the step function (so that back propogation would have gradients to train on), but there is no clear reference for who first proposed this
@article{rosenblatt1958perceptron,
	author = {Rosenblatt, Frank},
	title = {The perceptron: a probabilistic model for information storage and organization in the brain},
	year = {1958},
	journal = {Psychological review},
	volume = {65},
	number = {6},
	numpages = {386},
	publisher = {American Psychological Association},
	doi = {10.1037/h0042519},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	issn = {1939-1471},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, classification},
}

% Official publication of the scipy.stats.qmc module, which is the newly released module for performing quasi-monte carlo sampling and design-of-experiments in scipy. Scipy is an open source numerical software package which is the standard for advanced numerical methods and scientific software packages in Python. Most of scipy are wrappers for much older Fortran or C++ code, that has been highly optimized.
@article{roy2023quasimonte,
	author = {Roy, Pamphile T. and Owen, Art B. and Balandat, Maximilian and Haberland, Matt},
	title = {Quasi-Monte Carlo Methods in Python},
	year = {2023},
	month = {4},
	journal = {Journal of Open Source Software},
	volume = {8},
	number = {84},
	numpages = {5309},
	publisher = {The Open Journal},
	doi = {10.21105/joss.05309},
	url = {https://joss.theoj.org/papers/10.21105/joss.05309},
	issn = {2475-9066},
	keywords = {machine learning, ML, scientific machine learning, SciML, high dimension},
}

% Original tech report introducing recurrent neural networks (RNNs), which use a recursively defined state variable to track the context of sequential data observations so far, plus the value of the current output to predict the next output. The authors derive how the gradients can be backpropogated through this entire chain for efficient training. This idea had been around since the 60s, but this is the first paper explicitly proposing such recurrent layers in a representation learning context. The authors propose these recurrent layers as a means of representation learning for sequence data (such as next-word prediction in language models). RNNs would go on to become the state-of-the-art in language models and natural language processing for almost 30 years until they were replaced by transformers with the advent of BERT in 2017. Although the idea of encoding state information is a valid solution, RNNs notoriously suffered from a vanishing gradient issue where tokens further back in the sequence had little effect on the current prediction.
@techreport{rumelhart1985learning,
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	title = {Learning internal representations by error propagation},
	year = {1985},
	month = {9},
	number = {ICS 8504},
	institution = {Institute for Cognitive Science, University of California},
	address = {San Diego, CA, USA},
	doi = {10.21236/ada164453},
	url = {https://www.cs.toronto.edu/~hinton/absps/pdp8.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, RNNs, transformers, representation learning},
}

% Original paper on back-propagation for training neural networks. Equivalent to reverse-mode algorithmic differentiation or more simply applying the chain rule recursively.
@article{rumelhart1986learning,
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	title = {Learning representations by back-propagating errors},
	year = {1986},
	month = {10},
	journal = {Nature},
	volume = {323},
	number = {6088},
	pages = {533--536},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/323533a0},
	url = {https://www.nature.com/articles/323533a0},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML, autograd, algorithmic differentiation, backpropagation, neural networks, regression, classification},
}

% Techniques and optimal sampling criteria for performing adaptive sampling to enable downstream Gaussian process regression modeling
@article{sapsis2022optimal,
	author = {Sapsis, Themistoklis P. and Blanchard, Antoine},
	title = {Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with {Gaussian} process regression},
	year = {2022},
	month = {8},
	journal = {Philosophical Transactions of the Royal Society A},
	volume = {380},
	number = {2229},
	numpages = {20210197},
	publisher = {The Royal Society},
	doi = {10.1098/rsta.2021.0197},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0197},
	issn = {1364-503X},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, RBFs, Gaussian process, regression, uncertainty quantification, UQ},
}

% Some thorough analysis on upper and lower error bounds for RBF interpolants, which ultimately yield the famous "uncertainty princple for RBFs" -- i.e., a large shape parameter is needed for accuracy but a small shape parameter is needed for solvability of the resulting RBF system. Both are not possible, so you cannot have accuracy and solvability at once. Generally, the author recommends making the parameter as large as possible without causing singularity, a tradeoff that is often optimized in modern Gaussian process implementations via L-BFGS-B. There is also a really nice table of kernel errors for various kernels, which can be directly plugged into the generic RBF interpolation error formula to get error bounds for most common RBF basis functions
@article{schaback1995error,
	author = {Schaback, Robert},
	title = {Error estimates and condition numbers for radial basis function interpolation},
	year = {1995},
	month = {4},
	journal = {Advances in Computational Mathematics},
	volume = {3},
	number = {3},
	pages = {251--264},
	publisher = {Springer},
	doi = {10.1007/BF02432002},
	url = {http://link.springer.com/10.1007/BF02432002},
	issn = {1019-7168},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, approximation theory, uncertainty quantification, UQ},
}

% Proximal policy optimization (PPO) is the state-of-the-art policy gradient optimization method for reinforcement learning. Proximal gradient methods computes the update by directly optimizing the parameters for a given batch of observations in order to maximize the intermediate value function. PPO limits the step size in each iteration and enforces a penalization to discourage models from drifting too far from the original "base" model.
@techreport{schulman2017proximal,
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	title = {Proximal Policy Optimization Algorithms},
	year = {2017},
	institution = {arXiv cs.LG},
	url = {https://arxiv.org/abs/1707.06347},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, reinforcement learning, RL},
}

% The original paper for applying byte pair encoding (BPE) to words. This is the method used to embed words into numerical tokens for modern LLMs. The idea is to recursively combine the most commonly paired letters into a single token. Then an encoding value (number) is assigned to every token in the resulting vocabulary, to generate a massive encoder/decoder lookup table.
@inproceedings{sennrich2016neural,
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	editor = {Erk, Katrin and Smith, Noah A.},
	title = {Neural Machine Translation of Rare Words with Subword Units},
	year = {2016},
	month = {aug},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {1715--1725},
	organization = {Association for Computational Linguistics},
	location = {Berlin, Germany},
	doi = {10.18653/v1/P16-1162},
	url = {https://aclanthology.org/P16-1162/},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, representation learning, high dimension, dimension reduction},
}

% Original paper on Shapley values (used to calculate SHAP scores). The idea is to interpret the importance of features to a model agonstically by making predictions with (and without) every combination of features available. Then, we compute the weighted average of each feature's importance by calculating how much each feature improves the model performance. This is an expensive but polynomial time procedure given fixed number of features, and can be used to plot the contributions. The software is in lundberg et al 2017 (github.com/shap/shap)
@article{shapley1953value,
	author = {Shapley, Lloyd S and others, },
	title = {A value for n-person games},
	year = {1953},
	journal = {Contributions to the Theory of Games},
	volume = {2},
	number = {28},
	pages = {307--317},
	publisher = {Princeton University Press Princeton},
	url = {http://www.library.fa.ru/files/Roth2.pdf#page=39},
	keywords = {machine learning, ML, scientific machine learning, SciML, high dimension, dimension reduction, regression, classification},
}

% Extremely thorough analysis of using linear models inside a simplex (e.g., tetrahedral meshes) for interpolation: covering basic algorithm, conditioning, approximation error, quality metrics, etc.
@inproceedings{shewchuk2002what,
	author = {Shewchuk, Jonathan},
	title = {What is a good linear finite element? {I}nterpolation, conditioning, anisotropy, and quality measures},
	year = {2002},
	booktitle = {Proceedings of the 11th International Meshing Roundtable},
	pages = {115--126},
    url = {https://people.eecs.berkeley.edu/~jrs/papers/elemj.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, interpolation, approximation theory, uncertainty quantification, UQ},
}

% The EDBO software: open source Python software for performing multiobjective bayesian optimization for chemical synthesis and molecular discovery. Links a multiobjective optimization solver with the MORDRED software for getting molecular descriptors and optimizes for the desired properties
@article{shields2021bayesian,
	author = {Shields, Benjamin J. and Stevens, Jason and Li, Jun and Parasram, Marvin and Damani, Farhan and Alvarado, Jesus I. M. and Janey, Jacob M. and Adams, Rryan P. and Doyle, Abigail G.},
	title = {Bayesian reaction optimization as a tool for chemical synthesis},
	year = {2021},
	month = {2},
	journal = {Nature},
	volume = {590},
	number = {7844},
	pages = {89--96},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41586-021-03213-y},
	url = {https://www.nature.com/articles/s41586-021-03213-y},
	issn = {0028-0836},
	git = {http://github.com/b-shields/edbo},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, representation learning},
}

% A numerical algorithm for performing a rank-revealing rank-1 QR update (given an existing orthonormal (QR) matrix factorization and updating just one column of the basis Q)
@article{shroff1992adaptive,
	author = {Shroff, Gautam M. and Bischof, Christian H.},
	title = {Adaptive condition estimation for rank-one updates of {QR} factorizations},
	year = {1992},
	month = {10},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {13},
	number = {4},
	pages = {1264--1278},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
	doi = {10.1137/0613077},
	url = {http://epubs.siam.org/doi/10.1137/0613077},
	issn = {0895-4798},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, dimension reduction},
}

% A survey on how deep learning models (such as transformer models and neural networks/multilayer perceptrons) still do not perform well (are not the state of the art) when it comes to tabular data, which remains the most important application for most businesses. Gradient boosted trees remain the most reliable predictors for these applications
@article{shwartzziv2022tabular,
	author = {Shwartz-Ziv, Ravid and Armon, Amitai},
	title = {Tabular data: Deep learning is not all you need},
	year = {2022},
	month = {5},
	journal = {Information Fusion},
	volume = {81},
	pages = {84--90},
	publisher = {Elsevier},
	doi = {10.1016/j.inffus.2021.11.011},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521002360},
	issn = {1566-2535},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, decision-trees, transformers, regression, classification},
}

% Google DeepMind's AlphaGo was the first reinforcement learning agent to beat pro players (exceed the maximum human skill) in Go, which is a much more complex game than chess and required modeling an enumerable amount of game states and move possibilities. This required combining monte carlo tree search (MCTS) with neural networks to filter down to a reasonable number of potential states. This is credited as the first RL model to use MCTS, which is now a standard in RL
@article{silver2016mastering,
	author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others, },
	title = {Mastering the game of Go with deep neural networks and tree search},
	year = {2016},
	month = {1},
	journal = {nature},
	volume = {529},
	number = {7587},
	pages = {484--489},
	publisher = {Nature Publishing Group},
	doi = {10.1038/nature16961},
	url = {https://www.nature.com/articles/nature16961},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, reinforcement learning, RL},
}

% Original publication on diffusion models. The paper introduces a framework inspired by non-equilibrium thermodynamics where a forward diffusion process gradually destroys structure in the data distribution, transforming it into a simple distribution like Gaussian noise. A reverse diffusion process is then learned to reconstruct the original data distribution, serving as a generative mode. Data can be generated by training the distribution through diffusion, then applying the reverse diffusion operator to sample (or "generate") new data. This is the basis for most image and video generation models
@inproceedings{sohldickstein2015deep,
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	editor = {Bach, Francis and Blei, David},
	title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	year = {2015},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	series = {Proceedings of Machine Learning Research},
	volume = {37},
	pages = {2256--2265},
	organization = {PMLR},
	location = {Lille, France},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks},
}

% The original dropout paper, this was standard practice in training deep neural networks, especially massive convolutional nets and RNNs for image and language processing for a long time, prior to the invention of transformers -- the idea being that you zero out the effects (and updates to) a small percentage of the nodes in each layer in each iteration, in order to (1) make training cheaper, (2) prevent overfitting since over reliance on any individual node(s) makes the predictions brittle, and (3) redistribute weights to earlier layers and avoid vanishing gradients
@article{srivastava2014dropout,
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	year = {2014},
	journal = {The Journal of Machine Learning research},
	volume = {15},
	number = {1},
	pages = {1929--1958},
	publisher = {JMLR. org},
    url = {https://jmlr.org/papers/v15/srivastava14a.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regularization, overfitting},
}

% The FAIR principles of data management: Scientific data should be findable, accessible, interpretable, and reproducible
@article{stall2019make,
	author = {Stall, Shelley and Yarmey, Lynn and Cutcher-Gershenfeld, Joel and Hanson, Brooks and Lehnert, Kerstin and Nosek, Brian and Parsons, Mark and Robinson, Erin and Wyborn, Lesley},
	title = {Make scientific data {FAIR}},
	year = {2019},
	month = {6},
	journal = {Nature},
	volume = {570},
	number = {7759},
	pages = {27--29},
	publisher = {Nature Publishing Group},
	doi = {10.1038/d41586-019-01720-7},
	url = {https://www.nature.com/articles/d41586-019-01720-7},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML},
}

% A benchmark data science problem from the field of computer security: robustly classifying suspicious network traffic based on metadata about the requests
@inproceedings{tavallaee2009detailed,
	author = {Tavallaee, Mahbod and Bagheri, Ebrahim and Lu, Wei and Ghorbani, Ali A.},
	title = {A detailed analysis of the {KDD} {CUP} 99 data set},
	year = {2009},
	month = {7},
	booktitle = {2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications},
	pages = {1--6},
	organization = {IEEE},
	location = {Ottawa, ON, Canada},
	doi = {10.1109/CISDA.2009.5356528},
	url = {http://ieeexplore.ieee.org/document/5356528},
	keywords = {machine learning, ML, scientific machine learning, SciML, benchmarking, classification},
}

% The SHEPPACK numerical software package contains a variety of open source Fortran codes for computing Shepard's method interpolants in two, three, and four dimensions. There is also a linear Shepard's method interpolant that can be used for interpolation in arbitrary dimension.
@article{thacker2010algorithm,
	author = {Thacker, William I. and Zhang, Jingwei and Watson, Layne T. and Birch, Jeffrey B. and Iyer, Manjula A. and Berry, Michael W.},
	title = {Algorithm 905: {SHEPPACK}: {M}odified {S}hepard algorithm for interpolation of scattered multivariate data},
	year = {2010},
	month = {9},
	journal = {ACM Transactions on Mathematical Software},
	volume = {37},
	number = {3},
	numpages = {34},
	publisher = {Association for Computing Machinery (ACM)},
	doi = {10.1145/1824801.1824812},
	url = {https://dl.acm.org/doi/10.1145/1824801.1824812},
	issn = {0098-3500},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, interpolation},
}

% Simulated annealing and reinforcement learning based FPGA placement. The RL contributions are tenuous at best, but still an example of potential RL impact in industry
@inproceedings{tian2022improving,
	author = {Tian, Chunsheng and Chen, Lei and Wang, Yuan and Wang, Shuo and Zhou, Jing and Zhang, Yaowei and Li, Guang},
	title = {Improving Simulated Annealing Algorithm for {FPGA} Placement Based on Reinforcement Learning},
	year = {2022},
	month = {6},
	booktitle = {2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)},
	volume = {10},
	number = {},
	pages = {1912--1919},
	organization = {IEEE},
	location = {Chongqing, China},
	doi = {10.1109/ITAIC54216.2022.9836761},
	url = {https://ieeexplore.ieee.org/document/9836761/},
	keywords = {machine learning, ML, scientific machine learning, SciML, reinforcement learning, RL},
}

% Original publication of t-SNE, which performs latent space embedding and dimension reduction for high-dimensional data. The idea is to minimize the distribution error via KL-divergence between a target distribution and the a low-dimensional embedding of that distribution that is trained via gradient descent. The KL-divergence is calculated based on a similarity score between points in the target distribution (which perfectly maintains the relative distances / exponentiated relative distances between points in the original high-dimensional space) and the low-dimensional embedding, so the objective is ultimately to maintain the relative distances between points in the embedding. The method was originally proposed as a visualization technique for high-dimensional data (where the embedding produces clusters in 2 or 3 dimensions that can be plotted), but t-SNE can be and is also used for generic dimension reduction and latent-space embedding. The original software is not open source, but many open source implementations are linked on the original author's website: lvdmaaten.github.io/tsne
@article{vandermaaten2008visualizing,
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	title = {Visualizing Data using {t-SNE}},
	year = {2008},
	journal = {Journal of Machine Learning Research},
	volume = {9},
	number = {86},
	pages = {2579--2605},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	keywords = {machine learning, ML, scientific machine learning, SciML, representation learning, high dimension, dimension reduction},
}

% The landmark paper showing that transformers alone are capable of capturing all structure needed for language conversion, i.e., recurence relations and the sequential structure of time-series predictions can be encoded into which tokens to "pay attention to" during next word (or any next item) predictions. This paper is often cited as the inspiration for large language models (LLMs), which rely heavily on the transformer architecture, which became a standard after this
@inproceedings{vaswani2017attention,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	title = {Attention is all you need},
	year = {2017},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS '17)},
	pages = {1--11},
	organization = {Curran Associates, Inc.},
	location = {Long Beach, California, USA},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, transformers},
}

% Google DeepMind's AlphaStar II was the first reinforcement learning agent that could beat pro players in video games, which is a signficantly more complex environment than a board game environment with a finite state and set of possible actions
@article{vinyals2019grandmaster,
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	title = {Grandmaster level in {StarCraft II} using multi-agent reinforcement learning},
	year = {2019},
	month = {11},
	journal = {Nature},
	volume = {575},
	number = {7782},
	pages = {350--354},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41586-019-1724-z},
	url = {https://doi.org/10.1038/s41586-019-1724-z},
	isbn = {1476-4687},
	issn = {0028-0836},
	keywords = {machine learning, ML, scientific machine learning, SciML, reinforcement learning, RL},
}

% SciPy official publication: Scipy is an open source numerical software package which is the standard for advanced numerical methods and scientific software packages in Python. Most of scipy are wrappers for much older Fortran or C++ code, that has been highly optimized.
@article{virtanen2020scipy,
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Jarrod Millman, K. and Mayorov, Nikolay and Nelson, Andrew R.~J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, CJ and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E.~A. and Harris, Charles R and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1.0},
	title = {{SciPy} 1.0: {F}undamental Algorithms for Scientific Computing in {P}ython},
	year = {2020},
	month = {3},
	journal = {Nature Methods},
	volume = {17},
	number = {3},
	pages = {261--272},
	publisher = {Springer Science and Business Media LLC},
	doi = {10.1038/s41592-019-0686-2},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	issn = {1548-7091},
	keywords = {machine learning, ML, scientific machine learning, SciML, computational linear algebra, interpolation},
}

% An analysis and empirical study on how RBF interpolants interpolation matrices tend to always become ill-conditioned in high dimensions
@article{wang2018numerical,
	author = {Wang, Ruoxi and Li, Yingzhou and Darve, Eric},
	title = {On the numerical rank of radial basis function kernels in high dimensions},
	year = {2018},
	month = {1},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {39},
	number = {4},
	pages = {1810--1835},
	publisher = {SIAM},
	doi = {10.1137/17m1135803},
	url = {https://epubs.siam.org/doi/10.1137/17M1135803},
	issn = {0895-4798},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, high dimension, interpolation, approximation theory},
}

% A spectral analysis on the conditioning of the kernel matrics (i.e., interpolation matrices) for RBF interpolants and Gaussian processes, showing the singular values trending to zero exponentially in both theory and practice
@article{wathen2015spectral,
	author = {Wathen, Andrew J and Zhu, Shengxin},
	title = {On spectral distribution of kernel matrices related to radial basis functions},
	year = {2015},
	month = {12},
	journal = {Numerical Algorithms},
	volume = {70},
	number = {4},
	pages = {709--726},
	publisher = {Springer},
	doi = {10.1007/s11075-015-9970-0},
	url = {http://link.springer.com/10.1007/s11075-015-9970-0},
	issn = {1017-1398},
	keywords = {machine learning, ML, scientific machine learning, SciML, RBFs, Gaussian process, interpolation, approximation theory},
}

% A summary paper for an influential line of work in the field of scientific machine learning. Some standard techniques for computing error bounds for neural networks using traditional techniques from numerical analysis and approximation theory, and introducing a universal approximation theorem for neural networks. I.e., showing the existence of a two-layer multilayer perceptron or similar neural network that can approximate any Sobolev function to arbitrary required accuracy. Equivalently, we could say that such neural networks are dense in this Sobolev space
@article{weinan2020machine,
	author = {Weinan, E},
	title = {Machine learning and computational mathematics},
	year = {2020},
	month = {1},
	journal = {Communications in Computational Physics},
	volume = {28},
	number = {5},
	pages = {1639--1670},
	publisher = {Global Science Press},
	doi = {10.4208/cicp.OA-2020-0185},
	url = {https://global-sci.com/article/79736/machine-learning-and-computational-mathematics},
	issn = {1991-7120},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regression, approximation theory},
}

% An experiment showing that when training labels are replaced with random values (pure noise), most neural network classifier methods can still be trained to zero training error. (I.e., obviously overfitting). However, regularization techniques don't prevent them from doing so. The authors conclude that regularization isn't doing what we think it's doing and may not actually be related to generalization error
@inproceedings{zhang2017understanding,
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	title = {Understanding deep learning requires rethinking generalization},
	year = {2017},
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=Sy8gdB9xx},
	keywords = {machine learning, ML, scientific machine learning, SciML, neural networks, regularization, overfitting, interpolation, classification},
}

% The original publication for L-BFGS-B software, which solves bound-constrained optimization problems using a limited-memory BFGS. This is the standard implementation that is used in all L-BFGS-B codes to date, such as scipy, all machine learning codes, and most engineering codes and nonlinear systems solvers. The code is open source high-quality numerical software, written in old-style Fortran
@article{zhu1997algorithm,
	author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
	title = {Algorithm 778: {L-BFGS-B}: {F}ortran Subroutines for Large-Scale Bound-Constrained Optimization},
	year = {1997},
	month = {12},
	journal = {ACM Transactions on Mathematical Software},
	volume = {23},
	number = {4},
	pages = {550--560},
	publisher = {ACM},
	doi = {10.1145/279232.279236},
	url = {https://dl.acm.org/doi/10.1145/279232.279236},
	issn = {0098-3500},
	keywords = {machine learning, ML, scientific machine learning, SciML, optimization, convex optimization, computational linear algebra},
}


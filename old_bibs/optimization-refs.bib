% The textbook on evolutionary multiobjective optimization (MOO), including
% common algorithms, evaluation methodologies, fundamental techniques, and test
% problems
@book{abraham2005,
    editor={Abraham, Ajith and Jain, Lakhmi and Goldberg, Robert},
    title={Evolutionary Multiobjective Optimization: Theoretical Advances and
    Applications},
    year={2005},
    series={Advanced Information and Knowledge Processing Series},
    publisher={Springer Verlag},
    address={London, UK},
    doi={10.1007/1-84628-137-7}
}

% The Dakota blackbox and derivative-free simulation optimization framework, a
% numerical software package (in C++) maintained by Sandia that offers support
% for AI/ML surrogate modeling, multifidelity modeling, uncertainty
% quantification (UQ), and distributed and parallel computing
@techreport{adams2022,
  title = {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.16 User's Manual},
  author = {Adams, Brian M. and Bohnhoff, William J. and Dalbey, Keith R. and Ebeida, Mohamed S. and Eddy, John P. and Eldred, Michael S. and Hooper, Russell W. and Hough, Patricia D. and Hu, Kenneth T. and Jakeman, John D. and Khalil, Mohammad and Maupin, Kathryn A. and Monschke, Jason A. and Ridgeway, Elliott M. and Rushdi, Ahmad A. and Seidl, D. Thomas and Stephens, J. Adam and Swiler, Laura P. and Tran, Anh and Winokur, Justin G.},
  year = {2022},
  institution = {Sandia National Laboratory},
  address = {Albuquerque, NM, USA},
  number = {SAND2022-6171 version 6.16},
  url = {https://dakota.sandia.gov/sites/default/files/docs/6.16.0/Users-6.16.0.pdf}
}

% Using RBF surrogates to solve blackbox / derivative-free multiobjective
% optimization problems
@article{akhtar2016,
  author={Akhtar, Taimoor and Shoemaker, Christine A.},
  title={Multi objective optimization of computationally expensive multi-modal functions with {RBF} surrogates and multi-rule selection},
  year={2016},
  journal={Journal of Global Optimization},
  volume={64},
  number={1},
  pages={17--32},
  publisher={Springer},
  doi={10.1007/s10898-015-0270-y}
}

% A numerical software package written in MATLAB -- provides a surrogate
% modeling toolbox for multiobjective optimization problems
@inproceedings{aldujali2016a,
  author={Al-Dujaili, Abdullah and Suresh, Sundaram},
  title={A {MATLAB} toolbox for surrogate-assisted multi-objective optimization: A preliminary study},
  year={2016},
  booktitle={Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
  organization={ACM},
  location={Denver, CO, USA},
  pages={1209--1216},
  doi={10.1145/2908961.2931703}
}

% Introducing an algorithm for solving multiobjective optimization (MOO)
% problems via the global optimization algorithm DIRECT.  Some theory and
% preliminary results, but no software.  Likely not scalable for real-world
% computationally expensive problems, due to the number of boxes that would
% need to be divided per iteration using this method.  More of a theoretical
% foundation for a later practical algorithm
@inproceedings{aldujali2016b,
  author={Al-Dujaili, Abdullah and Suresh, Sundaram},
  title={Dividing rectangles attack multi-objective optimization},
  year={2016},
  booktitle={Proc. 2016 IEEE Congress on Evolutionary Computation (CEC '16)},
  organization={IEEE},
  location={Vancouver, BC, Canada},
  pages={3606--3613},
  doi={10.1109/CEC.2016.7744246}
}

% A survey and review of surrogate modeling and response-surface modeling (RSM)
% techniques used in engineering
@article{alizadeh2020,
  title={Managing computational complexity using surrogate models: a critical review},
  author={Alizadeh, Reza and Allen, Janet K. and Mistree, Farrokh},
  journal={Research in Engineering Design},
  volume={31},
  number={3},
  pages={275--298},
  year={2020},
  publisher={Springer},
  doi={10.1007/s00163-020-00336-7}
}


% Is derivative / gradient information counterproductive for solving MOOs?  In
% this paper, it appears to make direct-search type methods perform worse by
% the metrics used.  It could be that the metrics favor diversity over
% convergence, in which case one can get better diversity by taking bad
% evaluations.  But I need to read more carefully to decide whether that is
% what's going on here
@article{andreani2022,
  author = {Andreani, R. and Cust{\'o}dio, Ana Lu{\'i}sa and Raydan, M.},
  title = {Using first-order information in direct multisearch for multiobjective optimization},
  journal = {Optimization Methods and Software},
  volume = {37},
  number = {6},
  pages = {2135--2156},
  year  = {2022},
  publisher = {Taylor \& Francis},
  doi = {10.1080/10556788.2022.2060971}
}

% Applying grey-box bayesian optimization tutorial: using Bayesian optimization
% on structured problems, where a blackbox function is composed with an
% algebraic function, just like with ParMOO and Jeff's GOOMBAH paper.  Tutorial
% performed using BoTorch
@inproceedings{astudillo2021,
    author = {Astudillo, Raul and Frazier, Peter I.},
    title = {Thinking inside the box: a tutorial on grey-box bayesian optimization},
    year = {2021},
    booktitle = {Proc. 2021 Winter Simulation Conference (WSC 2021)},
    publisher = {IEEE},
    location = {Phoenix, Arizona},
    articleno = {2},
    numpages = {15},
    doi={10.1109/WSC52266.2021.9715343}
}

% pyOED an open source Python numerical software library for performing optimal
% experimental design, e.g, for sensor placement at Argonne
@techreport{chowhary2024,
author = {Chowdhary, Abhijit and Ahmed, Shady E. and Attia, Ahmed},
title = {{PyOED}: An Extensible Suite for Data Assimilation and Model-Constrained Optimal Design of Experiments},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3653071},
doi = {10.1145/3653071},
journal = {ACM Transactions on Mathematical Software},
month = {6},
articleno = {11},
numpages = {22},
}

% A thorough survey of over 50 commonly used performance indicators in
% multiobjective optimization -- key takeaways:  performance metrics can
% measure different properties of an algorithm, such as whether it is
% converging to the true Pareto front, the coverage of the true Pareto front,
% and the average diversity of solutions.  One of the only metrics that is
% monotonic (i.e., cannot become worse when a solution contains a previous
% solution) is the hypervolume indicator, which is the standard in evolutionary
% algorithms
@article{audet2021,
author = {Charles Audet and Jean Bigeon and Dominique Cartier and Sébastien {Le Digabel} and Ludovic Salomon},
title = {Performance indicators in multiobjective optimization},
year = {2021},
journal = {European Journal of Operational Research},
volume = {292},
number = {2},
pages = {397-422},
issn = {0377-2217},
doi = {10.1016/j.ejor.2020.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720309620},
keywords = {Multiobjective optimization, Quality indicators, Performance indicators},
}

% Handling categorical/integer/mixed variables in blackbox optimization:  This
% is the method used to perform hyperparameter tuning of neural-networks and
% other AI models via MADS.  In general, they decompose variables into meta
% variables (which determine whether other variables are active or not, such as
% the number of layers in the network which can deactivate variables associated
% with inactive layers), categorical variables (which either need to be embedded
% somehow or can be explored in an unordered manner via direct search /
% generalized pattern search), and finally standard variables which includes
% both continuous and relaxed integer variables
@inproceedings{audet2023,
  title={A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables},
  author={Audet, Charles and Hall{\'e}-Hannan, Edward and Le Digabel, S{\'e}bastien},
  booktitle={Operations Research Forum},
  volume={4},
  number={1},
  pages={12},
  year={2023},
  organization={Springer},
  doi={10.1007/s43069-022-00180-6}
}

% Book on fundamental methods, terminology, and theory in blackbox and
% derivative-free optimization (DFO) -- covers topics such as definitions of
% blackbox functions, heuristics, classical methods, positive bases and minimum
% spanning sets, generalized pattern search, and direct search, fully linear
% and quadratic models, model-drive descent and trust-region methods and
% ensuring model quality, general surrogate modeling, constraints, and
% multiobjective basics.
@book{audet2017,
  author={Audet, Charles and Hare, Warren},
  title={Derivative-free and blackbox optimization},
  year={2017},
  series={Springer Series in Operations Research and Financial Engineering},
  publisher={Springer International},
  address={Charm, Switzerland},
  doi={10.1007/978-3-319-68913-5}
}

% BiMADS -- a biobjective direct search / generalized pattern search via the
% MADS algorithm with an adaptive weighting scheme to trace the Pareto front
% from one end to the other.  The numerical software implementation is part of
% the NOMAD software package (written in C++)
@article{audet2008,
  author       = {Audet, Charles and Savard, Gilles and Zghal, Walid},
  title        = {Multiobjective optimization through a series of single-objective formulations},
  year         = {2008},
  journal = {SIAM Journal on Optimization},
  volume       = {19},
  number       = {1},
  pages        = {188--210},
  doi          = {10.1137/060677513},
}

% The progressive barrier penalty for nonlinear blackbox optimization methods.
% Basically adds a progressive penalty for violating constraints based on the
% distance to feasibility
@article{audet2009,
  author = {Audet, Charles and Dennis, John E.},
  title = {A Progressive Barrier for Derivative-Free Nonlinear Programming},
  journal = {SIAM Journal on Optimization},
  volume = {20},
  number = {1},
  pages = {445-472},
  year = {2009},
  doi = {10.1137/070692662},
}

% Multi-MADS -- a multiobjective direct search / generalized pattern search via
% MADS algorithm, using normal boundary intersection (NBI) for adaptive
% weighting
@article{audet2010,
  author       = {Audet, Charles and Savard, Gilles and Zghal, Walid},
  title        = {A mesh adaptive direct search algorithm for multiobjective optimization},
  year         = {2010},
  journal = {European Journal of Operational Research},
  volume       = {204},
  number       = {3},
  pages        = {545--556},
  doi          = {10.1016/j.ejor.2009.11.010},
}

% NOMAD v4 -- open source numerical software package (in C++) for blackbox and
% derivative-free optimization via the MADS algorithms. After publication, they
% have added support for multiobjective optimization, mixed variables,
% nonlinear constraints, etc.  Great example of high-impact open source
% numerical and optimization software.  Improvements over NOMAD v3 include
% improvements to fundamental algorithms, coding practices, release process,
% and general project structure to support continuous research and development
% into the future
@article{audet2022,
  author = {Audet, Charles and Le Digabel, S\'{e}bastien and Rochon Montplaisir, Viviane and Tribes, Christophe},
  title = {{Algorithm 1027}: {NOMAD} Version 4: Nonlinear Optimization with the {MADS} Algorithm},
  year = {2022},
  journal = {ACM Transactions on Mathematical Software},
  publisher = {ACM},
  volume = {48},
  number = {3},
  articleno = {35},
  numpages = {22},
  doi = {10.1145/3544489},
}

% BoTorch: Modular bayesian optimization framework and numerical software
% package.  Uses Ax for deploying A/B testing applications.  Uses pytorch for
% autograd, and uses monte carlo sampling and kernel reparameterization tricks
% in order to efficiently evaluate composite objective and acquisition
% functions and non gaussian kernels.  Great example of high-impact open source
% numerical software and optimization software
@inproceedings{balandat2020,
 author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G and Bakshy, Eytan},
 title = {{BoTorch}: A Framework for Efficient {M}onte-{C}arlo {B}ayesian Optimization},
 year = {2020},
 booktitle = {Advances in Neural Information Processing Systems},
 volume = {33},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21524--21538},
 publisher = {Curran Associates, Inc.},
 url = {https://proceedings.neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
}

% The PETSc user's guide.  I haven't used it but PETSc is a widely-used C++
% numerical software library and linear algebra / iterative algorithms
% framework developed at Argonne and used for implementing many well-known
% iterative solvers, especially in the area of CFD.  This is a great example of
% high-impact open source numerical software and best practices in open source
% scientific software.  Now ships together with TAO, a similar simulation
% optimization software package
@techreport{balay2022,
  author = {Balay, Satish and Abhyankar, Shrirang and Adams, Mark F. and Benson, Steven and Brown, Jed and Brune, Peter and Buschelman, Kris and Constantinescu, Emil and Dalcin, Lisandro and Dener, Alp and Eijkhout, Victor and Gropp, William D. and Hapla, V\'{a}clav and Isaac, Tobin and Jolivet, Pierre and Karpeev, Dmitry and Kaushik, Dinesh and Knepley, Matthew G. and Kong, Fande and Kruger, Scott and May, Dave A. and McInnes, Lois Curfman and Mills, Richard Tran and Mitchell, Lawrence and Munson, Todd and Roman, Jose E. and Rupp, Karl and Sanan, Patrick and Sarich, Jason and Smith, Barry F. and Zampini, Stefano and Zhang, Hong and Zhang, Hong and Zhang, Junchao},
  title       = {{PETSc/TAO} Users Manual},
  year        = {2022},
  institution = {Argonne National Laboratory},
  number      = {ANL-21/39 - Revision 3.17},
  address     = {Lemont, IL, USA},
  url         = {https://petsc.org/release/docs/manual/manual.pdf}
}

% AMOSA algorithm -- apparently this is a widely-known standard in
% multiobjective simulated annealing because reviewers regularly ask me to cite
% this.  But I've never met anyone who uses this and I can't find the software
% anywhere.  The algorithm seems very reasonable though
@article{bandyopadhyay2008,
  author={Bandyopadhyay, Sanghamitra and Saha, Sriparna and Maulik, Ujjwal and Deb, Kalyanmoy},
  title={A Simulated Annealing-Based Multiobjective Optimization Algorithm:
  {AMOSA}},
  year={2008},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={12},
  number={3},
  pages={269-283},
  doi={10.1109/TEVC.2007.900837}
}

% An open source numerical software library for solving multiobjective
% optimization problems in java in real-time via heuristics.  The authors
% combine jMetal with data streaming via Apache Spark to solve distributed
% multiobjective optimization problems with streaming data in real-time
@article{barbagonzalez2018,
    author = {Barba-González, Cristóbal and García-Nieto, José and Nebro, Antonio J. and Cordero, José A. and Durillo, Juan J. and Navas-Delgado, Ismael and Aldana-Montes, José F.},
    title = {{jMetalSP}: A framework for dynamic multi-objective big data optimization},
    year = {2018},
    journal = {Applied Soft Computing},
    volume = {69},
    pages = {737--748},
    doi = {https://doi.org/10.1016/j.asoc.2017.05.004},
}

% The FAIR principles for open source scientific software, data, source code,
% and experiments should by findable (via DOIs or other), accessible (clear
% purpose and metadata), interoperable (should use standard interfaces, data
% formats, and schemas), and reusable (well documented, understandable, and not
% overly specialized to an unnecessarilly niche use-case).  These are good
% principles for any open source software development practices
@article{barker2022,
  author={Barker, Michelle and Chue Hong, Neil P. and Katz, Daniel S. and Lamprecht, Anna-Lena and Martinez-Ortiz, Carlos and Psomopoulos, Fotis and Harrow, Jennifer and Castro, Leyla Jael and Gruenpeter, Morane and Martinez, Paula Andrea and Honeyman, Tom},
  title={Introducing the {FAIR} Principles for research software},
  year={2022},
  publisher={Nature Publishing Group},
  journal={Scientific Data},
  volume={9},
  number={1},
  pages={622},
  doi={10.1038/s41597-022-01710-x}
}

% jMetalPy -- a Python framework for solving MOOs with EAs -- this open source
% numerical software is a Python implementation of jMetal, with some
% improvements to code quality and new features for better open source software
% development and parallelism
@article{benitezhidalgo2019,
  title = {{jMetalPy}: A {P}ython framework for multi-objective optimization with metaheuristics},
  author = {Antonio Ben{\'i}tez-Hidalgo and Antonio J. Nebro and Jos{\'e} Garc{\'i}a-Nieto and Izaskun Oregi and Javier {Del Ser}},
  year = {2019},
  journal = {Swarm and Evolutionary Computation},
  volume = {51},
  pages = {100598},
  doi = {10.1016/j.swevo.2019.100598},
}

% A numerical algorithm for multiobjective descent, using RBF surrogates +
% trust regions.  Builds heavily off of Stefan's PhD thesis (ORBIT)
@article{berkemeier2021,
  author={Berkemeier, Manuel and Peitz, Sebastian},
  title={Derivative-Free Multiobjective Trust Region Descent Method Using Radial Basis Function Surrogate Models},
  year={2021},
  journal={Mathematical and Computational Applications},
  volume={26},
  number={2},
  pages={31},
  publisher={Multidisciplinary Digital Publishing Institute},
  doi={10.3390/mca26020031}
}

% Proof that the complexity of calculating the hypervolume indicator with o
% objectives is exponential.  Roughly the same reasons that calculating
% simplices in an o-dimensional Delaunay triangulation or computing the facets
% of an o-dimensional convex hull are exponential
@article{beume2009,
  author={Beume, Nicola and Fonseca, Carlos M. and Lopez-Ibanez, Manuel and Paquete, Luis and Vahrenhold, Jan},
  title={On the complexity of computing the hypervolume indicator},
  year={2009},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={13},
  number={5},
  pages={1075--1082},
  publisher={IEEE},
  doi={10.1109/TEVC.2009.2015575}
}

% NASA's FUN3D CFD solver.  This is one of the oldest and standard numerical
% softwares for solving CFD problems.  Written in mostly Fortran 90.  Uses a
% form of the problem that yields the adjoints, which can be used to optimize
% structures in fewer steps and perform sensitivity analyses.  The kernel uses
% an iterative solver to solve a massive block-sparse linear system (I think
% derived from the weak form).  Some a priori multiobjective optimization
% solvers are described in Section 9.9
@techreport{biedron2019,
  author={Biedron, Robert T. and Carlson, Jan Renee and Derlaga, Joseph M. and Gnoffo, Peter A. and Hammond, Dana P. and Jones, William T. and Kleb, Bill and Lee-Rausch, Elizabeth M. and Nielson, Eric J. and Park, Michael A. and Rumsey, Christopher L. and Thomas, James L. and Thompson, Kyle B. and Wood, William A.},
  title={{FUN3D Manual}: 13.6},
  year={2019},
  institution={NASA Langley Research Center},
  number={{NASA} {T}echnical {M}emorandum ({TM}) 2019-220416},
  address={Hampton, VA, USA},
  url={https://fun3d.larc.nasa.gov/papers/FUN3D_Manual-13.6.pdf}
}

% DMulti-MADS: Improved Multi-MADS using direct search / generalized pattern
% search plus some improvements to the Multi-MADS algorithm -- I need to
% re-read this to remember what the improvements were
@article{bigeon2020,
  author={Bigeon, Jean and Le Digabel, S{\'e}bastien and Salomon, Ludovic},
  title={{DM}ulti-{MADS}: {M}esh adaptive direct multisearch for blackbox multiobjective optimization},
  year={2020},
  journal={Computational Optimization and Applications},
  volume={79},
  number={2},
  pages={301--338},
  doi={10.1007/s10589-021-00272-9}
}

% pagmo/pygmo - Parallel frameworks for solving multiobjective optimization
% problems (MOO) in Java and Python.  Great example of open source numerical
% software, published in JOSS
@article{biscani2020,
  author = {Francesco Biscani and Dario Izzo},
  title = {A parallel global multiobjective framework for optimization: pagmo},
  year = {2020},
  journal = {Journal of Open Source Software},
  publisher = {The Open Journal},
  volume = {5},
  number = {53},
  pages = {2338},
  doi = {10.21105/joss.02338},
}

% pymoo is an open source software package implementing NSGA-II, NSGA-III, and
% many other multiobjective evolutionary algorithms (MOEAs), plus extensions
% for handling things such as categorical variables.  This is a well-maintained
% and well-documented open-source numerical software package.  It is maintained
% by the lab of the original NSGA-II author, and therefore could be considered
% the official NSGA-II implementation.  all source code in Python
@article{blank2020,
  author   = {Blank, Julian and Deb, Kalyanmoy},
  title    = {{pymoo}: Multi-Objective Optimization in {Python}},
  year     = {2020},
  journal  = {IEEE Access},
  volume   = {8},
  pages    = {89497--89509},
  software = {http://github.com/anyoptimization/pymoo},
  doi = {10.1109/ACCESS.2020.2990567}
}

% Methodolgies for calibrating the Fayans EDF model to experimental data.  Data
% is expensive and limited and the model itself is computationally expensive,
% so this is a classical inverse problem.  The problem is actually
% multiobective because the data themselves come from various categories
% representing different types of observations, and the standard deviations for
% each of these observables is not known.  Could be configured as a 3 or
% 9-objective problem
@article{bollapragada2020,
  author={Bollapragada, Raghu and Menickelly, Matt and Nazarewicz, Witold and O'Neal, Jared and Reinhard, Paul-Gerhard and Wild, Stefan M.},
  title={Optimization and supervised machine learning methods for fitting numerical physics models without derivatives},
  year={2020},
  journal={Journal of Physics G: Nuclear and Particle Physics},
  volume={48},
  number={2},
  pages={024001},
  publisher={IOP Publishing},
  doi = {10.1088/1361-6471/abd009}
}

% The open source numerical software package (in Python) pySMT.  This is a
% surrogate modeling and Bayesian optimization toolbox for solving
% multidisciplinary engineering design optimization (MDO) problems, while
% utilizing derivatives and providing numerical stability analysis for each
% surrogate model class.
@article{bouhlel2019,
  author = {Mohamed Amine Bouhlel and John T. Hwang and Nathalie Bartoli and Rémi Lafage and Joseph Morlier and Joaquim R.R.A. Martins},
  title = {A {P}ython surrogate modeling framework with derivatives},
  year = {2019},
  journal = {Advances in Engineering Software},
  volume = {135},
  pages = {102--662},
  doi = {10.1016/j.advengsoft.2019.03.005},
}

% A Multiobjective Bayesian optimization algorithm, very similar to ParEGO --
% this algorithm uses the Gaussian process surrogates with NSGA-II to solve the
% problem.  However, spectral sampling and thompson sampling are then employed
% to subselect a diverse set of candidates for batch evaluation.  The resulting
% algorithm is called TSEMO
@article{bradford2018,
  author={Bradford, Eric and Schweidtmann, Artur M. and Lapkin, Alexei},
  title={Efficient multiobjective optimization employing {Gaussian} processes, spectral sampling and a genetic algorithm},
  year={2018},
  journal={Journal of Global Optimization},
  volume={71},
  number={2},
  pages={407--438},
  publisher={Springer},
  doi={10.1007/s10898-018-0609-2}
}

% A study on utilizing polynomial surrogate models during multiobjective direct
% search and generalized pattern search techniques
@article{bras2020,
  title={On the use of polynomial models in multiobjective directional direct search},
  author={Br{\'a}s, Carmo P. and Cust{\'o}dio, Ana Lu{\'\i}sa},
  journal={Computational Optimization and Applications},
  volume={77},
  number={3},
  pages={897--918},
  year={2020},
  publisher={Springer},
  doi={10.1007/s10589-020-00233-8}
}

% Analysis of hypervolume indicator as proxy for solution set quality --
% results for 2-objectives only, this paper shows that the hypervolume
% indicator is the best single indicator we have, but with some caveates
@article{bringmann2013,
  author = {Bringmann, Karl and Friedrich, Tobias},
  title = {Approximation quality of the hypervolume indicator},
  year = {2013},
  journal = {Artificial Intelligence},
  volume = {195},
  pages = {265--290},
  number = {0004-3702},
  doi = {10.1016/j.artint.2012.09.005}
}

% Key findings from the VarSys project on modeling HPC performance variability
% with surrogates and RSM, and using these models to inform decision making
% through visualizations, optimization, and otherwise
@article{cameron2019,
author={Cameron, Kirk W. and Anwar, Ali and Cheng, Yue and Xu, Li and Li, Bo
Ananth, Uday and Bernard, Jon and Jearls, Chandler and Lux, Thomas
and Hong, Yili and Watson, Layne T. and Butt, Ali R.},
year={2019},
title={{MOANA}: {M}odeling and analyzing {I/O} variability
in parallel system experimental design},
journal={IEEE Transactions on Parallel and Distributed Systems},
volume={30},
number={8},
pages={1843--1856}
}

% The open source numerical software MODIR is proposed here. 
% MODIR can be used to solve multiobjective blackbox optimization problems via
% a multiobjective variant of the DIRECT blackbox algorithm (direct search
% method).  The motivating application is a multidisciplinary shiphull
% engineering design problem
@article{campana2018,
  author={Campana, Emilio Fortunato and Diez, Matteo and Liuzzi, Giampaolo and Lucidi, Stefano and Pellegrini, Riccardo and Piccialli, Veronica and Rinaldi, Francesco and Serani, Andrea},
  title={A multi-objective {DIRECT} algorithm for ship hull optimization},
  year={2018},
  journal={Computational Optimization and Applications},
  volume={71},
  number={1},
  pages={53--72},
  doi={10.1007/s10589-017-9955-0}
}

% My PhD thesis, including multiobjective optimization techniques, algorithm,
% performance analysis, and software review; description of VTMOP, running
% parallel simulations, integrating with libE.  Also scientific machine
% learning via Delaunay interpolation and algorithms and proofs for doing so.
% Several applications related to HPC performance modeling and autotuning.
@phdthesis{chang2020a,
  author={Chang, Tyler H.},
  title={Mathematical Software for Multiobjective Optimization Problems},
  year={2020},
  school={Virginia Tech, Dept. of Computer Science},
  url={https://vtechworks.lib.vt.edu/handle/10919/98915}
}

% Paper on the challenges of integrating VTMOP into the libEnsemble parallel
% computing Python software library at Argonne
@inproceedings{chang2020b,
  author={Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T. and Lux, Thomas C. H.},
  title={Managing computationally expensive blackbox multiobjective optimization problems using {libEnsemble}},
  year={2020},
  booktitle={Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20)},
  organization={SCS},
  location={Fairfax, VA, USA},
  pages={31},
  doi={10.22360/SpringSim.2020.HPC.001}
}

% A study on the multiobjective optimization of the LINPACK benchmark's config
% files on the leadership class HPC Bebop at Argonne National Laboratory.  We
% used VTMOP but some modifications were required to ensure that mixed
% variables were properly handled.  Some of the techniques that we used here
% inspired me to provide automatic support in ParMOO.  Ultimately, we achieve
% 3x reduction in performance variability without sacrificing max/mean
% throughput.
@inproceedings{chang2020c,
  author={Chang, Tyler H. and Larson, Jeffrey and Watson, Layne T.},
  title={Multiobjective optimization of the variability of the high-performance {LINPACK} solver},
  year={2020},
  booktitle={Proc. 2020 Winter Simulation Conference (WSC 2020)},
  publisher={IEEE},
  location={Orlando, FL, USA},
  pages={3081--3092},
  doi={10.1109/WSC48552.2020.9383875}
}

% Publication of my second open source numerical software package: VTMOP a
% Fortran software for solving blackbox multiobjective optimization problems.
% Uses surrogate modeling (RSM), with an adaptive weighting scheme, within a
% trust region framework.  The motivating application is a particle accelerator
% tuning problem at SLAC
@article{chang2022a,
  author   = {Chang, Tyler H. and Watson, Layne T. and Larson, Jeffrey and Neveu, Nicole and Thacker, William I. and Deshpande, Shubhangi and Lux, Thomas C. H.},
  title    = {{Algorithm 1028}: {VTMOP}: Solver for Blackbox Multiobjective
              Optimization Problems},
  year     = {2022},
  volume   = {48},
  number   = {3},
  pages    = {36},
  journal  = {{ACM} Transactions on Mathematical Software},
  software = {https://github.com/Libensemble/libe-community-examples/tree/main/vtmop},
  doi={10.1145/3529258}
}

% The ParMOO docs -- ParMOO is my open source numerical software package and
% library ParMOO, written in Python, which can be used for implementing custom
% solvers for multiobjective simulation optimization problems, while supporting
% mixed variables, non linear constraints, and diverse and custom surrogte
% models, and composite structures where some components of the problem are
% blackbox, but the rest are algebraic.  In later releases, ParMOO also
% supports interactive visualization of results via Plotly + Dash, parallel and
% distributed function evaluations via libEnsemble, and automatic gradient
% calculations and just-in-time compilation via jax
@techreport{chang2024a,
    author      = {Chang, Tyler H. and Wild, Stefan M. and Dickinson, Hyrum},
    title       = {{ParMOO}: {P}ython library for parallel multiobjective simulation optimization},
    year        = {2024},
    institution = {Argonne National Laboratory},
    number      = {Version 0.4.1},
    address     = {Lemont, IL, USA},
    url         = {https://parmoo.readthedocs.io/en/latest}
}

% The ParMOO JOSS article -- ParMOO is my open source numerical software
% package and library ParMOO, written in Python, which can be used for
% implementing custom solvers for multiobjective simulation optimization
% problems, while supporting mixed variables, non linear constraints, and
% diverse and custom surrogte models, and composite structures where some
% components of the problem are blackbox, but the rest are algebraic.  In later
% releases, ParMOO also supports interactive visualization of results via
% Plotly + Dash, parallel and distributed function evaluations via libEnsemble,
% and automatic gradient calculations and just-in-time compilation via jax
@article{chang2023b,
  author      = {Chang, Tyler H. and Wild, Stefan M.},
  title       = {{ParMOO}: A {P}ython library for parallel multiobjective simulation optimization},
  year        = {2023},
  journal     = {Journal of Open Source Software},
  publisher = {The Open Journal},
  volume      = {8},
  number      = {82},
  pages       = {4468},
  doi         = {10.21105/joss.04468}
}

% Using ParMOO software with the MDML software (wrapper on Apache Kafka with
% automatic data logging and analysis dashboard) in order to optimize chemical
% manufacturing processes in a wet-lab environment.  The kafka querries are
% sent directly to a continuous flow reactor (CFR) through a smart-lab setup in
% the MERF at Argonne.  Through this setup, ParMOO is able to automatically
% steer the solvents, bases, temperatures, flow rates, and mixing ratios of a
% complex chemical manufacturing process in order to produce optimized yields
% and purities -- achieving multi-hundred-fold improvement over the previous
% manual process
@inproceedings{chang2023c,
  author  = {Chang, Tyler H. and Elias, Jakob R. and Wild, Stefan M. and Chaudhuri, Santanu and Libera, Joseph A.},
  title   = {A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps},
  year    = {2023},
  booktitle    = {11th Intl. Conf. on Learning Representation (ICLR 2023), Workshop on Machine Learning for Materials (ML4Materials)},
  location = {Kigali, Rwanda},
  url       = {https://openreview.net/forum?id=8KJS7RPjMqG},
  pages     = {1--10},
  note     = {to appear}
}

% This is the OJOC ParMOO repository DOI -- this is an archive of the software
% experiments for obtaining our test problems and reproducing our experimental
% results on those test problems with customized ParMOO solvers.
@misc{chang2025,
  author =        {Chang, Tyler H. and Wild, Stefan M.},
  publisher =     {INFORMS Journal on Computing},
  title =         {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
  year =          {2024},
  doi =           {10.1287/ijoc.2023.0250.cd},
  note =          {Available for download at https://github.com/INFORMSJoC/2023.0250},
}

% The ParMOO IJOC article describing the design of the ParMOO software,
% motivation, and providing examples of how ParMOO can be used to solve common
% scientific problems more efficiently with low effort -- ParMOO is my open
% source numerical software package and library ParMOO, written in Python,
% which can be used for implementing custom solvers for multiobjective
% simulation optimization problems, while supporting mixed variables, non
% linear constraints, and diverse and custom surrogte models, and composite
% structures where some components of the problem are blackbox, but the rest
% are algebraic.  In later releases, ParMOO also supports interactive
% visualization of results via Plotly + Dash, parallel and distributed function
% evaluations via libEnsemble, and automatic gradient calculations and
% just-in-time compilation via jax
@article{chang2025,
  author =        {Chang, Tyler H. and Wild, Stefan M.},
  title =         {Designing a Framework for Solving Multiobjective Simulation Optimization Problems},
  year =          {2025},
  journal =     {INFORMS Journal on Computing},
  doi =           {10.1287/ijoc.2023.0250},
}

% The funcX and updated Globus publication on the techniques and benefits of
% using a function-as-a-service (FaaS) framework to perform scientific
% experimentation at Argonne and other labs.  funcX and Globus are scientific
% software products for performing distributed function evaluations and
% parallel computing that started at Argonne, and spun off into independent
% companies
@inproceedings{chard2020,
  title={{funcX}: A federated function serving fabric for science},
  author={Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and Foster, Ian and Chard, Kyle},
  booktitle={Proc. 29th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '20)},
  year={2020},
  pages={65--76},
  organization={ACM},
  location={Stockholm, Sweden},
  doi={10.1145/3369583.3392683}
}

% A particle accelerator tuning application with ParMOO -- we achieved very
% good results on a several hundred evaluation budget for the Argonne wakefield
% accelerator project
@inproceedings{chen2023,
  author  = {Chen, Gongxiaohui and Chang, Tyler H. and Power, John and Jing, Chungunag},
  title   = {An Integrated Multi-Physics Optimization Framework for Particle Accelerator Design},
  year    = {2023},
  booktitle = {Proc. 2023 Winter Simulation Conference (WSC 2023), Industrial Applications Track},
  location = {Orlando, FL, USA},
  numpages = {2},
  doi     = {10.48550/arXiv.2311.09415}
}

% The classic textbook by Cheney and Light on the fundamentals of approximation
% theory for multivariate functions -- topics include: basics of interpolation,
% approximation theory, and linear operators; multivariate polynomials, their
% interpolation nodes, and error kernels; selecting good polynomial
% interpolants via Newton and Lagrange type methods; positive-definite
% functions, kernel interpretations, and good kernels for interpolation; basis
% functions, orthonormal bases, common bases for interpolation and convergence
% rates; Chebyshev nodes; B-splines, Box splines, and thin-plate splines; and
% basics of artificial neural networks.  Other topics include wavelets,
% orthogonal projection algorithms, Hilbert spaces, and reproducing kernel
% Hilbert spaces (RKHS).
@book{cheney2009,
  author={Cheney, Elliott W. and Light, William A.},
  year={2009},
  title={A Course in Approximation Theory},
  series={Graduate Studies in Mathematics},
  publisher={AMS},
  address={Providence, RI, USA}
}

% The Keras docs -- great and highly impactful open source Python software,
% needs no introduction.  A simplified interface for quickly building neural
% networks and other deep learning models with various backends frameworks such
% as Tensorflow, jax, and Pytorch.
@misc{chollet2015,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

% An algorithm for multiobjective implicit filtering (MOIF).  Not mentioned
% here, but the open source numerical software for MOIF on the author's GitHub
% is often cited via this paper
@article{cocchi2018,
  author       = {Cocchi, Guido and Liuzzi, Giampaolo and Papini, Alessandra and Sciandrone, Marco},
  title        = {An implicit filtering algorithm for derivative-free multiobjective optimization with box constraints},
  year         = {2018},
  journal = {Computational Optimization and Applications},
  volume       = {69},
  number       = {2},
  pages        = {267--296},
  doi          = {10.1007/s10589-017-9953-2}
}

% Building a multiobjective augmented Lagrangian -- basically, use a standard
% augmented Lagrangian penalty but apply it to all components of the objective.
% There is a proof that this will work.  I don't use this method, but I use the
% same trick with the progressive barrier of Audet all the time and usually
% cite both papers
@article{cocchi2020,
  author={Cocchi, Guido and Lapucci, Matteo},
  year={2020},
  title={An augmented {Lagrangian} algorithm for multi-objective optimization},
  journal={Computational Optimization and Applications},
  volume={77},
  number={1},
  pages={29--56},
  publisher={Springer},
  doi={10.1007/s10589-020-00204-z}
}

% Andrew Conn's landmark paper on interpolation dataset geometry -- leads to
% the definition of sets being "well-poised" for interpolation, meaning that
% when the interpolation set's geometry meats some local geometric conditions
% (basically bounded away from singularity), then the resulting interpolant's
% error (and gradient / hessian errors) can be bounded and the resulting models
% can be used to perform gradient descent or SQP within a trust-region
% framework with guaranteed convergence
@article{conn2008,
  title={Geometry of interpolation sets in derivative free optimization},
  author={Conn, Andrew R and Scheinberg, Katya and Vicente, Lu{\'\i}s N},
  journal={Mathematical programming},
  volume={111},
  pages={141--172},
  year={2008},
  publisher={Springer},
  doi={10.1007/s10107-006-0073-5}
}

% Conn and Scheinberg book on DFO -- describes the geometry of good
% interpolation sets, linear and quadratic interpolants and how to use them for
% derivative-free gradient descent and SQP frameworks, bounds on interpolation
% and gradient errors of various models, and how to efficiently restore good
% geometry when the optimization algorithm samples points in a subspace
@book{conn2009,
  author={Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
  title={Introduction to derivative-free optimization},
  year={2009},
  series={MPS-SIAM Series on Optimization},
  publisher={SIAM},
  address={Philadelphia, PA, USA},
  doi={10.1137/1.9780898718768}
}

% PyMOSO: an open source Python numerical software library for solving
% multiobjective simulation optimization problems with integer and discrete
% variables via direct search / pattern search like techniques
@article{cooper2020,
  author={Cooper, Kyle and Hunter, Susan R.},
  year={2020},
  title={{PyMOSO}: {S}oftware for multi-objective simulation optimization with
  {R-PERLE} and {R-MinRLE}},
  journal={INFORMS Journal on Computing},
  volume={32},
  number={4},
  pages={1101--1108},
  doi={10.1287/ijoc.2019.0902}
}

% RBFOpt an open source library for solving single-objective blackbox optimization
@article{costa2018,
  title={{RBFOpt}: an open-source library for black-box optimization with costly function evaluations},
  author={Costa, Alberto and Nannicini, Giacomo},
  journal={Mathematical Programming Computation},
  volume={10},
  number={4},
  pages={597--629},
  year={2018},
  publisher={Springer},
  doi={10.1007/s12532-018-0144-7}
}

% ParEGO latest code and update introducing algorithmic updates, improved
% software quality, and (I think) some parallel computing -- ParEGO is the
% first open source numerical multiobjective bayesian optimization software
% package and written in C++.  It is basically EGO (the original Bayesian
% optimization software using expected improvment acquisition) plus augmented
% Lagrangian scalarization
@inproceedings{cristescu2015,
  title={Surrogate-based multiobjective optimization: {ParEGO} update and test},
  author={Cristescu, Cristina and Knowles, Joshua},
  booktitle={Workshop on Computational Intelligence (UKCI)},
  volume={770},
  year={2015},
  code={https://github.com/CristinaCristescu/ParEGO_Eigen},
  url={https://www.cs.bham.ac.uk/~jdk/UKCI-2015.pdf}
}

% The MultiGLODS numerical software package is written in MATLAB and used for
% solving multiobjective optimization problems via direct search / pattern
% search with a clever restart algorithm for selecting directions to explore in
% order obtain good coverage of the Pareto front.  I believe this version also
% can use polynomial surrogates to pre-select good search directions and filter
% out unneeded blackbox function / simulation evaluations.  It is now part of
% the BoostDFO MATLAB numerical software toolkit, obtainable from contacting
% Custodio
@article{custodio2017,
  author       = {Cust{\'{o}}dio, Ana Lu\'isa and Madeira, Jose F. A.},
  title        = {{MultiGLODS}: global and local multiobjective optimization using direct search},
  year         = {2018},
  journal = {Journal of Global Optimization},
  volume       = {72},
  number       = {2},
  pages        = {323--345},
  doi          = {10.1007/s10898-018-0618-1}
}

% Direct multisearch (DMS) is one of Custodio's earlier multiobjective direct
% search algorithms, which I think is a precursor to MutiGLODS.  The numerical
% MATLAB software can be obtained by contacting her lab, but I'm not sure if
% they still distribute it as part of BoostDFO
@article{custodio2011,
  author       = {Cust\'odio, Ana Lu\'isa and Madeira, Jose F. A. and Vaz, A. Ismael F. and Vicente, Lu\'is N.},
  title        = {Direct Multisearch for Multiobjective Optimization},
  year         = {2011},
  journal = {SIAM Journal on Optimization},
  volume       = {21},
  number       = {3},
  pages        = {1109--1140},
  doi          = {10.1137/10079731x}
}

% Applications of low-discrepancy sequences in Monte carlo simulation
@inproceedings{dalal2008,
  author={Dalal, Ishaan L. and Stefan, Deian and Harwayne-Gidansky, Jared},
  booktitle={2008 International Conference on Application-Specific Systems, Architectures and Processors}, 
  title={Low discrepancy sequences for {M}onte {C}arlo simulations on reconfigurable platforms}, 
  year={2008},
  volume={},
  number={},
  pages={108-113},
  doi={10.1109/ASAP.2008.4580163}
}

% Description and proof of coverage for a quadratic scalarization scheme for
% scalarizing multiobjective optimization problems
@article{dandurand2016,
  author = {Dandurand, Brian and Wiecek, Margaret M.},
  title = {Quadratic scalarization for decomposed multiobjective optimization},
  year = {2016},
  journal = {{OR} Spectrum},
  volume = {38},
  number = {4},
  pages = {1071--1096},
  doi = {10.1007/s00291-016-0453-z}
}

% The normal boundary intersection (NBI) method was one of the first adaptive
% scalarization schemes for multiobjective optimization problems.  It uses
% linear scalarization but does so adaptively using the angle of the
% intersecting vector at a target point to set the weights in order to
% adaptively fill in gaps on the Pareto front
@article{das1998,
  author={Das, Indraneel and Dennis, John E.},
  year={1998},
  title={Normal-boundary intersection: A new method for generating the {P}areto surface in nonlinear multicriteria optimization problems},
  journal={SIAM Journal on Optimization},
  volume={8},
  number={3},
  pages={631--657},
  doi={10.1137/S1052623496307510}
}

% An early paper on using surrogate models to reduce the cost (in terms of true
% simulation / blackbox function evaluations) when using multiobjective
% evolutionary algorithm to solve computationally expensive blackbox and
% simulation optimization problems
@article{datta2016,
  author       = {Datta, Rituparna and Regis, Rommel G.},
  title        = {A surrogate-assisted evolution strategy for constrained multi-objective optimization},
  year         = {2016},
  journal = {Expert Systems with Applications},
  volume       = {57},
  pages        = {270--284},
  doi          = {10.1016/j.eswa.2016.03.044}
}

% A technique for differentiating expected hypervolume improvement EHVI (and
% its monte carlo variant qEHVI), which can be used as the acquisition function
% for solving multiobjective blackbox optimization problems with BoTorch
@inproceedings{daulton2020,
 author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
 title = {Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective {B}ayesian Optimization},
 year = {2020},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 volume = {33},
 pages = {9851--9864},
 publisher = {Curran Associates, Inc.},
 url = {https://proceedings.neurips.cc/paper/2020/file/6fec24eac8f18ed793f5eaad3dd7977c-Paper.pdf},
}

% The first paper on performing parallel Bayesian optimization using the
% expected hypervolume improvement acquisition function in BoTorch
@inproceedings{daulton2021,
 author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
 title = {Parallel {B}ayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement},
 year = {2021},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 volume = {34},
 pages = {2187--2200},
 publisher = {Curran Associates, Inc.},
 url = {https://proceedings.neurips.cc/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf},
}

% The orgiinal NSGA-III paper part 1: a multiobjective evolutionary algorithm
% similar to NSGA-II, solving the drawback of NSGA-II performing poorly with
% many objectives by having the user provide a collection of well-spaced
% reference points and optimizing toward those
@article{deb2013,
  title={An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part {I}: solving problems with box constraints},
  author={Deb, Kalyanmoy and Jain, Himanshu},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={18},
  number={4},
  pages={577--601},
  year={2013},
  publisher={IEEE},
  doi={10.1109/TEVC.2013.2281535}
}

% The original NSGA-II paper: a multiobjective evolutionary algorithm (MOEA)
% that scales well and performs extremely well in practice.  The main
% contribution is a fast method for performing nondominated sorting so that the
% authors can ensure all efficient points persist in every generation.  This
% method is generally considered to be the baseline in multiobjective
% optimization.  While the algorithm is a simple heuristic that is extremely
% wasteful in terms of the number of true blackbox function / simulation
% evaluations, it performs extremely well in practice by the hypervolume
% indicator.  I have found that it is very difficult to obtain better
% performance than NSGA-II on both benchmark and real-world problems in the
% limit, unless you have some "secret sauce" to exploit for your particular
% problem
@article{deb2002a,
  author={Deb, Kalyanmoy and Pratap, Amrit and Agarwel, Sameer
  and Meyarivan, T.},
  year={2002},
  title={A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={6},
  number={2},
  pages={182--197},
  doi={10.1109/4235.996017}
}

% The DTLZ test problems are the standard test problems used in all
% multiobjective evolutionary optimization papers.  They are algebraic test
% problems that can scale to as many objectives and variables as one desires.
% Each problem also has a pathological property that makes it extremely
% difficult or degenerate for multiobjective optimization algorithms.  This
% maeks the suite as a whole extremely convenient for testing, scaling, and
% evaluating results.  However, several of the problems are so difficult that
% no solvers can reliably solve them.  Additionally, all the problems have the
% property that the last "n" variables are essentially unused, with their
% optimum being 0.5 and not changing as we move across the Pareto front, which
% could be unrealistic for certain problems
@inproceedings{deb2002b,
  author={Deb, Kalyanmoy and Thiele, Lothar and Laumanns, Marco
  and Zitzler, Eckart},
  year={2002},
  title={Scalable multi-objective optimization test problems},
  booktitle={Proc. 2002 IEEE Congress on Evolutionary Computation (CEC '02)},
  organization={IEEE},
  location={Honolulu, HI, USA},
  pages={825--830},
  volume={1},
  doi={10.1109/CEC.2002.1007032}
}

% An algorithm for solving blackbox multiobjective optimization problems via
% trust region descent, using locally linear (shepard's method) surrogate
% models, and a multiobjective variant of DIRECT.  Also, the authors propose a
% novel adaptive weighting scheme within the trust regions.  The motivating
% application is an aircraft design optimization problem.
@article{deshpande2016,
  author={Deshpande, Shubhangi and Watson, Layne T. and Canfield, Robert A.},
  year={2016},
  title={Multiobjective optimization using an adaptive weighting scheme},
  journal={Optimization Methods and Software},
  volume={31},
  number={1},
  pages={110--133},
  doi={10.1080/10556788.2015.1048861}
}

% Achieving performance portability of parallel codes in the exascale computing
% project (ECP) across various GPU-based architectures, which is challenging
% since different GPU vendors use different GPU programming libraries (e.g.,
% CUDA for NVIDIA vs HIPP for AMD)
@article{dubey2021,
  author={Dubey, Anshu and McInnes, Lois Curfman and Thakur, Rajeev and Draeger, Erik W. and Evans, Thomas and Germann, Timothy C. and Hart, William E.},
  journal={Computing in Science \& Engineering},
  title={Performance Portability in the {E}xascale {C}omputing {P}roject: Exploration Through a Panel Series},
  year={2021},
  volume={23},
  number={5},
  pages={46-54},
  doi={10.1109/MCSE.2021.3098231}
}

% The JuMP modeling language in Julia -- a modeling language for modeling and
% solving linear and nonlinear programming (optimization) problems in Julia.
% The implementation is an open source numerical software
@article{dunning2017,
  author = {Dunning, Iain and Huchette, Joey and Lubin, Miles},
  title = {{JuMP}: A Modeling Language for Mathematical Optimization},
  journal = {SIAM Review},
  volume = {59},
  number = {2},
  pages = {295-320},
  year = {2017},
  doi = {10.1137/15M1020575},
}

% jMetal is an open source numerical software library implementing
% multiobjective optimization solvers in Java.  Last I checked, most of the
% solvers were heuristic methods such as evolutionary algorithms and/or
% simulated annealing.  This is widely used in some fields of engineering
@article{durillo2011,
  title = {{jMetal}: A {J}ava framework for multi-objective optimization},
  author = {Juan J. Durillo and Antonio J. Nebro},
  year = {2011},
  journal = {Advances in Engineering Software},
  volume = {42},
  number = {10},
  pages = {760-771},
  doi = {10.1016/j.advengsoft.2011.05.014},
}

% A classical textbook on the fundamentals of multiobjective optimization
% theory.  Topics include: basic definitions in multiobjective optimization,
% partial orderings and cones and basic theories, linear scalarization and its
% theory and drawbacks, other scalarization methods and their theory and
% drawbacks, standard algorithms for common types of multiobjective
% optimization problems, and sample applications
@book{ehrgott2005,
  author={Ehrgott, Matthias},
  year={2005},
  title={Multicriteria Optimization},
  edition={2},
  series={Lecture Notes in Economics and Mathematical Systems Series},
  publisher={Springer Verlag},
  address={Heidelberg, Germany},
  doi={10.1007/3-540-27659-9}
}

% The Pascoletti-Serafini scalarization and its variations, this method
% involves drawing a line through the target to reach various points on the
% Pareto front.  It is effective with nonconvex Pareto fronts, but it is not
% adaptive and not commonly used in modern algorithms
@article{eichfelder2009,
  title={Scalarizations for adaptively solving multi-objective optimization problems},
  author={Eichfelder, Gabriele},
  journal={Computational Optimization and Applications},
  volume={44},
  number={2},
  pages={249--273},
  year={2009},
  publisher={Springer},
  doi={10.1007/s10589-007-9155-4}
}

% The MDML open source software is a wrapper around Apache Kafka with protocols
% for fast data streaming and logging and dashboard generation.  This framework
% was developed for usage at the material engineering research facility (MERF)
% at Argonne in order to facilitate the creation of a "smart lab" where MDML is
% the protocol for sending experiment requests to various equipment in the lab
% and logging results -- in an old (out-of-date branch) of ParMOO, this was a
% valid backend for launching simulation / experiment requests
@article{elias2020,
  title={The Manufacturing Data and Machine Learning Platform: Enabling Real-time Monitoring and Control of Scientific Experiments via {IoT}},
  author={Elias, Jakob R. and Chard, Ryan and Libera, Joseph A. and Foster, Ian T. and Chaudhuri, Santanu},
  journal={2020 IEEE 6th World Forum on Internet of Things (WF-IoT)},
  year={2020},
  pages={1-2},
  software={GitHub: \url{https://github.com/anl-mdml/MDML_Client}},
  doi={10.1109/WF-IoT48130.2020.9221078}
}

% A chapter from an optimization textbook on performing multiobjective Bayesian
% optimization using the expected hypervolume improvement (EHVI) scalarization
% / acquisition
@inbook{emmerich2016,
  author    = {Emmerich, Michael and Yang, Kaifeng and Deutz, Andr{\'e} and Wang, Hao and Fonseca, Carlos M.},
  title     = {A Multicriteria Generalization of {Bayesian} Global Optimization},
  editor    = {Pardalos, Panos M. and Zhigljavsky, Anatoly and {\v{Z}}ilinskas, Julius},
  year      = {2016},
  pages     = {229--242},
  publisher = {Springer},
  booktitle = {Advances in Stochastic and Deterministic Global Optimization},
  doi       = {10.1007/978-3-319-29975-4}
}

% TURBO -- This is an open source numerical software for solving
% high-dimensional optimization problems via Bayesian optimization using
% BoTorch.  Since Bayesian optimization performs poorly in high dimensions,
% they have resorted to applying a rudimentary trust region framework on top of
% their Bayesian optimization algorithm.  By squeesing the trust region inward
% (standard practice in derivative-free optimization) they are able to force
% the Bayesian optimization algorithm to converge in a reasonable number of
% true blackbox function evaluations
@article{eriksson2019,
  title={Scalable global optimization via local bayesian optimization},
  author={Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  pages={1--12},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf}
}

% Publication and whitepaper on Pathmind, an RL-based solver for simulation
% optimization problems.  They also offer multiobjective support but only by
% using a priori scalarization provided by the user (so not real multiobjective
% support).  This tool is not open source, it is a service provided by a
% YC startup of the same name.  Therefore, it could be considered industry
% software
@inproceedings{farhan2020,
  title={Reinforcement Learning in {AnyLogic} Simulation Models: A Guiding Example using {Pathmind}},
  author={Farhan, Mohammed and G{\"o}hre, Brett},
  year={2020},
  booktitle={Proc. 2020 Winter Simulation Conference (WSC 2020)},
  publisher={IEEE},
  location={Orlando, FL, USA},
  pages={3212-3223},
  doi={10.1109/WSC48552.2020.9383916}
}

% A biobjective ranking and selection algorithm from Hunter's NSF Career award
@article{feldman2018,
  author = {Feldman, Guy and Hunter, Susan R.},
  title = {{SCORE} Allocations for Bi-objective Ranking and Selection},
  year = {2018},
  journal = {ACM Transactions on Modeling Computer and Simulation},
  volume = {28},
  number = {1},
  articleno = {7},
  numpages = {28},
  doi = {10.1145/3158666},
}

% A Bayesian optimization algorithm for solving constrained optimization
% problems that are both single and multiobjective -- the authors propose
% expected hypervolume improvement (EHVI) which merges expected improvement
% acquisition from Bayesian optimization with the hypervolume indicator for
% multiobjective optimization
@article{feliot2016,
  author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
  title = {A {B}ayesian approach to constrained single- and multi-objective optimization},
  year = {2016},
  journal = {Journal of Global Optimization},
  volume = {67},
  number = {1-2},
  pages = {97--133},
  doi = {10.1007/s10898-016-0427-3}
}

% The DEAP framework is a Python framework for easily implementing and
% deploying parallel and distributed evolutionary algorithms.  Fairly high
% quality open source software.  This is widely used by optimization
% practitioners, e.g., engineers and scientists that read an evolutionary
% algorithm paper and want to try it out on their problem
@article{fortain2012,
    author    = {Fortin, F\'elix-Antoine and {De Rainville}, Fran\c{c}ois-Michel and Gardner, Marc-Andr\'e and Parizeau, Marc and Gagn\'e ", Christian},
    title     = {{DEAP}: Evolutionary Algorithms Made Easy},
    year      = {2012},
    journal   = {Journal of Machine Learning Research},
    volume    = {13},
    number    = {1},
    pages     = {2171--2175},
    url = {https://www.jmlr.org/papers/v13/fortin12a.html}
}

% Landmark textbook on standard design patterns, such as builders, factories,
% observers, etc.  Nicknamed the "Gang of Four" book on design patters
@book{gamma1995,
  title={Design patterns: elements of reusable object-oriented software},
  author={Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
  year={1995},
  publisher={Addison-Wesley},
  address={Reading, MA, USA},
  isbn={0-201-63361-2}
}

% A community reviewed textbook on Bayesian optimization theory and
% implementation.  Very thorough description of Gaussian process and Bayesian
% optimization fundamentals and theory, common techniques and acquisition
% functions, and implementation details, drawbacks, and real-world challenges
@book{garnett2023,
  author    = {Garnett, Roman},
  title     = {Bayesian Optimization},
  year      = {2023},
  publisher = {Cambridge University Press},
  isbn 	    = {978-1108425780},
  url = {https://bayesoptbook.com}
}

% A survey of static DOE sampling strategies for surrogate modeling, as well as
% a novel model-based strategy proposed by the authors
@article{garud2017,
  author = {Garud, Sushant Suhas and Karimi, Iftekhar A. and Kraft, Markus},
  title = {Smart Sampling Algorithm for Surrogate Model Development},
  year = {2017},
  journal = {Computers \& Chemical Engineering},
  volume = {96},
  pages = {103--114},
  doi = {10.1016/j.compchemeng.2016.10.006},
}

% A review of data analysis and machine learning methods for earth system
% modeling
@article{geer2021,
  title={Learning earth system models from observations: machine learning or data assimilation?},
  author={Geer, Alan J.},
  journal={Philosophical Transactions of the Royal Society A},
  volume={379},
  number={2194},
  pages={20200089},
  year={2021},
  publisher={The Royal Society Publishing},
  doi={10.1098/rsta.2020.0089}
}

% An article on the benefits of co design of algorithms and software and
% hardware in the context of the DOE's exascale computing project (ECP)
@article{germann2021,
  title={Co-design in the {Exascale Computing Project}},
  author={Germann, Timothy C.},
  journal={The International Journal of High Performance Computing Applications},
  volume={35},
  number={6},
  pages={503--507},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England},
  doi = {10.1177/10943420211059380}
}

% Google's OSS Vizier service is a (now open source) blackbox / derivative-free
% optimization numerical software package and service.  As far as I can tell,
% the package is primarily used for solving system optimization and A/B testing
% type problems
@inproceedings{golovin2017,
  author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D.},
  title = {{Google Vizier}: A Service for Black-Box Optimization},
  year = {2017},
  booktitle = {Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17)},
  pages = {1487–1495},
  organization = {ACM},
  location = {Halifax, NS, Canada},
  doi = {10.1145/3097983.3098043}
}

% Theorems on the curse of dimensionality when it comes to drawing data points
% in high-dimensional spaces.  The main theorem implies that the convex hull of
% N points in D dimensions has volume ~0 for D sufficiently large -- this
% occurs because of a concentration of measure type result
@article{gorban2017,
  title={Stochastic separation theorems},
  author={Gorban, Alexander N and Tyukin, Ivan Yu},
  journal={Neural Networks},
  volume={94},
  pages={255--259},
  year={2017},
  publisher={Elsevier},
  doi={10.1016/j.neunet.2017.07.014}
}

% The OpenMDAO open source numerical software library for modeling and solving
% multidisciplinary engineering design optimization problems.  Combines
% surrogate modeling, gradient based optimization, parallel computing
% frameworks, and derivative-free optimization techniques in one package so
% in order to solve large mixed-variable blackbox optimization problems.
% Developed by NASA Glenn
@article{gray2019,
  title={{OpenMDAO}: An open-source framework for multidisciplinary design, analysis, and optimization},
  author={Gray, Justin S. and Hwang, John T. and Martins, Joaquim R.R.A. and Moore, Kenneth T. and Naylor, Bret A.},
  journal={Structural and Multidisciplinary Optimization},
  volume={59},
  number={4},
  pages={1075--1104},
  year={2019},
  publisher={Springer},
  doi={10.1007/s00158-019-02211-z}
}

% Platypus: an open source numerical software package for performing
% multiobjective optimization in Python and comparing results
@techreport{hadka2015,
  title={Platypus -- multiobjective optimization in {P}ython},
  author={Hadka, David},
  year={2015},
  institution={GitHub},
  number={Version 1.0.4},
  url={https://platypus.readthedocs.io/en/latest}
}

% Chimera: a scientific software package for steering self-driving labs via
% multiobjective optimization -- the software is similar to what we did with
% ParMOO + MDML in the MERF at Argonne.  They focus on applications in robot
% calibration and molecular system design.  The multiobjective component helps
% them to explore the solution space with experimentation
@article{hase2018,
  title={Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories},
  author={H{\"a}se, Florian and Roch, Lo{\"\i}c M and Aspuru-Guzik, Al{\'a}n},
  journal={Chemical science},
  volume={9},
  number={39},
  pages={7642--7655},
  year={2018},
  publisher={Royal Society of Chemistry},
  doi = {10.1039/C8SC02239A}
}

% The official publication of the open source numerical software numpy:  the
% standard for basic multivariable computations, vector operations, and simple
% linear algebra in Python
@article{harris2020,
 author = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
          van der Walt and Ralf Gommers and Pauli Virtanen and David
          Cournapeau and Eric Wieser and Julian Taylor and Sebastian
          Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
          and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
          Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
          R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
          G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
          Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
          Travis E. Oliphant},
 title = {Array programming with {NumPy}},
 year = {2020},
 journal = {Nature},
 volume = {585},
 number = {7825},
 pages = {357--362},
 publisher = {Springer Science and Business Media {LLC}},
 doi = {10.1038/s41586-020-2649-2}
}

% The official textbook on the Pyomo modeling language: an open source
% optimization modeling language and scientific software developed at Sandia by
% Bill Hart et al.  Pyomo is a standard for solving large-scale mathematical
% programming (linear and nonlinear optimization) problems in Python
@book{hart2017,
  title={Pyomo -- optimization modeling in {P}ython},
  author={Hart, William E. and Laird, Carl D. and Watson, Jean-Paul and Woodruff, David L. and Hackebeil, Gabriel A. and Nicholson, Bethany L. and Siirola, John D.},
  year={2017},
  edition={2},
  series={Springer Optimization and Its Applications},
  publisher={Springer Cham},
  address={Cham, Switzerland},
  doi={10.1007/978-3-319-58821-6}
}

% A survey paper on multiobjective reinforcement learning -- RL is basically
% optimization with the addition of a dynamically changing state variable, so
% this is sort of relevant to multiobjecte optimization research
@article{hayes2022,
  title={A practical guide to multi-objective reinforcement learning and planning},
  author={Hayes, Conor F and R{\u{a}}dulescu, Roxana and Bargiacchi, Eugenio and K{\"a}llstr{\"o}m, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M and Dazeley, Richard and Heintz, Fredrik and others},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={1},
  pages={1--59},
  year={2022},
  doi={10.1007/s10458-022-09552-y}
}

% VTDIRECT95 reference: a high-performance parallel Fortran implementation of
% the famous single-objective blackbox (direct search) optimization algorithm
% DIRECT.  The numerical software is now open source (maintained by me) on Dr.
% Watson's GitHub page.
@article{he2009,
    author = {He, Jian and Watson, Layne T. and Sosonkina, Masha},
    title = {Algorithm 897: {VTDIRECT95}: Serial and Parallel Codes for the Global Optimization Algorithm {DIRECT}},
    year = {2009},
    volume = {36},
    number = {3},
    journal = {ACM Transactions on Mathematical Software},
    pages = {17},
    doi = {10.1145/1527286.1527291}
}

% The better scientific software tech report, with recommendations and rules of
% thumb for improving the quality of scientific software within the DOE
@techreport{heroux2020,
  author = {Heroux, Michael A. and McInnes, Lois and Bernholdt, David E. and Dubey, Anshu and Gonsiorowski, Elsa and Marques, Osni and Moulton, J. David and Norris, Boyana and Raybourn, Elaine and Balay, Satish and Bartlett, Roscoe A. and Childers, Lisa and Gamblin, Todd and Grubel, Patricia and Gupta, Rinku and Hartman-Baker, Rebecca and Hill, Judith C. and Hudson, Stephen and Junghans, Christoph and Klinvex, Alicia and Milewicz, Reed and Miller, Mark and Ah Nam, Hai and O'Neal, Jared and Riley, Katherine and Sims, Ben and Schuler, Jean and Smith, Barry F. and Vernon, Louis and Watson, Gregory R. and Willenbring, James and Wolfenbarger, Paul},
  title = {Advancing Scientific Productivity through Better Scientific Software: Developer Productivity and Software Sustainability Report},
  year = {2020},
  institution={Oak Ridge National Laboratory},
  address={Oak Ridge, TN, USA},
  number={ORNL TM-2020 1459 / ECP-U-RPT-2020-0001},
  doi = {10.2172/1606662},
}

% A numerical-quadrature based approximation to discrepancy.  For
% high-dimensional ill-spaced points, the quadrature error can be huge, and
% although discrepancies should always be between 0 and 1, the approximation
% can approach (13/12)^d - 1, where d is the dimension of the problem.  When
% points are randomly or quasi-randomly sampled, such large values of the
% approximation could be indicative of measure collapse.  This is the technique
% used in scipy.stats.qmc.discrepancy(...)
@article{hickernell1998,
  title={A generalized discrepancy and quadrature error bound},
  author={Hickernell, Fred J.},
  journal={Mathematics of computation},
  volume={67},
  number={221},
  pages={299--322},
  year={1998},
  doi={10.1090/S0025-5718-98-00894-1}
}

% An article on Google GlassBox research: Google's research division dedicated
% to interpretable machine learning
@article{hof2015,
  author = {Hof, Robert D.},
  title = {Google Tries to Make Machine Learning a Little More Human},
  year = {2015},
  month = {nov},
  journal = {MIT Technology Review},
  url = {https://www.technologyreview.com/2015/11/05/165175/google-tries-to-make-machine-learning-a-little-more-human/},
  note = {Last accessed: June 20, 2022}
}

% Techniques for optimizing molecule properties via a latent-space embedding
% that comes from a molecule autoencoder, from IBM Research
@article{hoffman2022,
    author = {Hoffman, Samuel C. and Chenthamarakshan, Vijil and Wadhawan, Kahini and Chen, Pin-Yu and Das, Payel},
    title = {Optimizing molecules using efficient queries from property evaluations},
    year = {2022},
    journal = {Nature Machine Intelligence},
    volume = {4},
    number = {1},
    pages = {21--31},
    doi = {10.1038/s42256-021-00422-y}
}

% The official JOSS paper for libEnsemble: an open source software library for
% performing parallel and distributed computations involving ensembles of
% computationally expensive function evaluations in Python
@techreport{hudson2022a,
  doi = {10.21105/joss.06031}, 
  year = {2023},
  volume = {8}, 
  number = {92},
  pages = {6031},
  author = {Stephen Hudson and Jeffrey Larson and John-Luke Navarro and Stefan M. Wild},
  title = {{libEnsemble: A} complete {Python} toolkit for dynamic ensembles of calculations},
  journal = {Journal of Open Source Software}
}

% The original libEnsemble publication focusing on its techniques for
% distributing and evaluating ensembles of functions in parallel.  Although not
% discussed, libEnsemble was already open source at the time
@article{hudson2022b,
  title   = {{libEnsemble}: A Library to Coordinate the Concurrent
             Evaluation of Dynamic Ensembles of Calculations},
  author  = {Stephen Hudson and Jeffrey Larson and John-Luke Navarro and Stefan M. Wild},
  journal = {{IEEE} Transactions on Parallel and Distributed Systems},
  volume  = {33},
  number  = {4},
  pages   = {977--988},
  year    = {2022},
  doi     = {10.1109/tpds.2021.3082815}
}

% A thorough survey on techniques and algorithms for solving multiobjective
% simulation optimization problems, including algorithms and techniques for
% handling small finite design spaces, discrete integer design spaces, and
% continuous design spaces.  Covers theory, algorithms, and popular heuristics
% for all.
@article{hunter2019,
  author={Hunter, Susan R. and Applegate, Eric A. and Arora, Viplove and Chong, Bryan},
  title={An introduction to multiobjective simulation optimization},
  year={2019},
  journal={ACM Transactions on Modeling and Computer Simulation},
  volume={29},
  number={1},
  pages={1--36},
  doi={10.1145/3299872}
}

% The orgiinal NSGA-III paper part 2: a multiobjective evolutionary algorithm
% similar to NSGA-II, solving the drawback of NSGA-II performing poorly with
% many objectives by having the user provide a collection of well-spaced
% reference points and optimizing toward those -- this paper focuses on how to
% handle constraints
@article{jain2013,
  title={An evolutionary many-objective optimization algorithm using reference-point based nondominated sorting approach, part {II}: Handling constraints and extending to an adaptive approach},
  author={Jain, Himanshu and Deb, Kalyanmoy},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={18},
  number={4},
  pages={602--622},
  year={2013},
  publisher={IEEE},
  doi={10.1109/TEVC.2013.2281534}
}

% A modification to the original Sobol sequence generator code
@article{joe2003,
author = {Joe, Stephen and Kuo, Frances Y.},
title = {Remark on Algorithm 659: Implementing Sobol's Quasirandom Sequence Generator},
year = {2003},
publisher = {Association for Computing Machinery},
volume = {29},
number = {1},
doi = {10.1145/641876.641879},
journal = {ACM Transactions on Mathematical Software},
pages = {49–57},
numpages = {9},
}

% The numerical software algorithm used in scipy algorithm for generating Sobol
% sequences (low discrepancy sequences)
@article{joe2008,
  title={Constructing Sobol sequences with better two-dimensional projections},
  author={Joe, Stephen and Kuo, Frances Y.},
  journal={SIAM Journal on Scientific Computing},
  volume={30},
  number={5},
  pages={2635--2654},
  year={2008},
  publisher={SIAM},
  doi={10.1137/070709359}
}

% Original publication on maximin and minimax designs for
% design-of-experiments.  I.e., minimize the maximum distance from any point in
% the bounding box to the nearest point in the design, and maximize the minimum
% distance between any pair of points in the design.
@article{johnson1990,
title = {Minimax and maximin distance designs},
journal = {Journal of Statistical Planning and Inference},
volume = {26},
number = {2},
pages = {131-148},
year = {1990},
doi = {https://doi.org/10.1016/0378-3758(90)90122-B},
author = {M.E. Johnson and L.M. Moore and D. Ylvisaker}
}

% D.R. Jones' original paper on the landmark DIRECT (DIviding RECTangles)
% algorithm for direct search global blackbox optimization.  The idea is that
% you can perform branch-and-bound style Lipschitzian optimization without
% knoweldge of the Lipschitz constant by dividing a rectangular design space
% into rectangular regions (rectangles) and subdividing those rectangles that
% could be potentially optimal given any Lipschitz constant by choosing those
% boxes on the lower left convex hull of the objective value at the center vs
% box diameter scatter plot
@article{jones1993,
  title={Lipschitzian optimization without the Lipschitz constant},
  author={Jones, Donald R. and Perttunen, Cary D. and Stuckman, Bruce E.},
  journal={Journal of optimization Theory and Applications},
  volume={79},
  pages={157--181},
  year={1993},
  publisher={Springer}
}

% Introducing Dragonfly: an open source numerical software package for solving
% neural architecture search problems via Bayesian optimization and solving an
% optimal transport problem to evaluate the distance between two networks.
% Considered a bit of a landmark paper for neural network architecture search
% problems.  The open source Python software is widely used for a variety of
% applications outside NAS, including molecular discovery
@article{kandasamy2020,
  author   = {Kandasamy, Kirthevasan and Vysyaraju, Karun Raju and Neiswanger, Willie and Paria, Biswajit and Collins, Christopher R. and Schneider, Jeff and Poczos, Barnabus and Xing, Eric P.},
  title    = {Tuning Hyperparameters without Grad Students: Scalable and Robust
              {Bayesian} Optimisation with {Dragonfly}},
  year     = {2020},
  journal  = {Journal of Machine Learning Research},
  volume   = {21},
  number   = {81},
  pages    = {1--27},
  software = {http://github.com/dragonfly/dragonfly},
  url     = {http://jmlr.org/papers/v21/18-223.html}
}

% A survey of multiobjective optimization algorithms for hyperparameter tuning
% in the context of automatic machine learning (autoML)
@article{karl2022,
  title = {Multi-Objective Hyperparameter Optimization in Machine Learning -- An Overview},
  volume = {3},
  DOI = {10.1145/3610536},
  number = {4},
  journal = {ACM Transactions on Evolutionary Learning and Optimization},
  author = {Karl,  Florian and Pielok,  Tobias and Moosbauer,  Julia and Pfisterer,  Florian and Coors,  Stefan and Binder,  Martin and Schneider,  Lennart and Thomas,  Janek and Richter,  Jakob and Lang,  Michel and Garrido-Merchán,  Eduardo C. and Branke,  Juergen and Bischl,  Bernd},
  year = {2023},
  pages = {1--50}
}


% A review of physics-informed machine learning techniques and models by
% Karniadakis himself, who is credited with starting the field of SciML
@article{karniadakis2021,
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title = {Physics-informed machine learning},
  year = {2021},
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  doi = {10.1038/s42254-021-00314-5}
}

% An introductory-level paper to theory-guided data science.  I haven't read
% but I assume it covers the same topics he taught in his class at Virginia
% Tech
@article{karpatne2017,
  author={Karpatne, Anuj and Atluri, Gowtham and Faghmous, James H. and Steinbach, Michael and Banerjee, Arindam and Ganguly, Auroop and Shekhar, Shashi and Samatova, Nagiza and Kumar, Vipin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  title={Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data},
  year={2017},
  volume={29},
  number={10},
  pages={2318-2331},
  doi={10.1109/TKDE.2017.2720168}
}

% Proposes a new technique called manifold sampling to solve blackbox
% optimization problems where a smooth blackbox function is composed with a
% piecewise linear (non blackbox) function.  Normally, this would create a
% nonsmooth blackbox function, but by modeling the blackbox function separately
% and sampling the different manifolds produced by the changes in active
% components of the piecewise function, we can still model the blackbox
% function with smooth techniques and solve the optimization problem
% efficiently
@article{khan2018,
  author       = {Kamil A. Khan and Jeffrey Larson and Stefan M. Wild},
  title        = {Manifold Sampling for Optimization of Nonconvex Functions that are Piecewise Linear Compositions of Smooth Components},
  volume       = {28},
  number       = {4},
  pages        = {3001--3024},
  year         = {2018},
  doi          = {10.1137/17m114741x},
  journal = {{SIAM} Journal on Optimization},
}

% The SPEA2+ algorithm for solving multiobjective optimization problems with
% evolutionary algorithms.  Apparently this is widely-used numerical software,
% but I can't find the download
@inproceedings{kim2004,
  author={Kim, Mifa and Hiroyasu, Tomoyuki and Miki, Mitsunori and Watanabe, Shinya},
  title={{SPEA2+}: Improving the performance of the {S}trength {P}areto {E}volutionary {A}lgorithm 2},
  year={2004},
  booktitle={Proc. International Conference on Parallel Problem Solving from Nature (PPSN VIII)},
  organization={Springer},
  location={Birmingham, UK},
  pages={742--751},
  isbn={978-3-540-30217-9_75}
}

% The original ParEGO algorithm paper: in each iteration of the algorithm, the
% authors use NSGA-II to optimize the expected improvement of the Gaussian
% process surrogates of each objective.  Then, results are scalarized using
% augmented chebyshev and the best results are evaluated for the next iteration
@article{knowles2006,
  author       = {Knowles, Joshua},
  title        = {{ParEGO:} A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  year         = {2006},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume       = {8},
  number       = {5},
  pages        = {1341--66},
  doi          = {10.1109/tevc.2005.851274}
}

% The SORCER software is a service oriented computing environment used by the
% US Airforce research lab (AFRL) to distribute expensive computations (such as
% design optimizations) across their large distributed network of computing
% resources
@inproceedings{kolonay2011,
  author={Kolonay, Raymond M. and Sobolewski, Michael},
  title={Service oriented computing environment ({SORCER}) for large scale, distributed, dynamic fidelity aeroelastic analysis},
  year={2011},
  booktitle={International Forum on Aeroelasticity and Structural Dynamics (IFASD 2011), Optimization},
  pages={26--30},
  location={Paris, France},
  publisher={Citeseer},
  url={http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.656.7539}
}

% The textbook on discrepancy that Manisha used to learn about low discrepancy
% sequences and their motivation
@book{kuipers1974,
    author = {Kuipers, L. and Niederreiter, H.},
     title = {Uniform distribution of sequences},
    series = {Pure and Applied Mathematics},
 publisher = {Wiley-Interscience [John Wiley \& Sons], New
              York-London-Sydney},
      year = {1974},
     pages = {xiv--390},
}

% Stochastic approximation algorithm (i.e., stochastic gradient descent) and
% how to analyze its radius of convergence for a fixed step-size -- you can
% decay its step size at a square-summable but not summable rate to guarantee
% convergence in the limit
@article{lai2003,
  title={Stochastic approximation},
  author={Lai, Tze Leung},
  journal={The annals of Statistics},
  volume={31},
  number={2},
  pages={391--406},
  year={2003},
  publisher={Institute of Mathematical Statistics}
}

% The APOSMM Python software is a framework for implementing multistart
% derivative-free optimization algorithms and running them in asynchronously
@article{larson2018,
  title={Asynchronously parallel optimization solver for finding multiple minima},
  author={Larson, Jeffrey and Wild, Stefan M},
  journal={Mathematical Programming Computation},
  volume={10},
  pages={303--332},
  year={2018},
  publisher={Springer},
  doi={10.1007/s12532-017-0131-4}
}

% A thorough survey of techniques and algorithms in derivative-free
% optimization
@article{larson2019,
  title={Derivative-free optimization methods},
  author={Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
  journal={Acta Numerica},
  volume={28},
  pages={287--404},
  year={2019},
  publisher={Cambridge University Press},
  doi = {10.1017/S0962492919000060}
}

% Jeff's GOOMBAH paper on exploiting composite structures in derivative-free
% optimization, meaning that a blackbox function is composed with an algebraic
% function and we want to optimize the result, while exploiting the fact that
% we know the equation of the algebraic function.  Similar technique is used in
% ParMOO to exploit this same structure and others that are specific to the
% multiobjective case
@article{larson2023,
  doi = {10.1007/s12532-023-00245-5},
  volume = {16},
 pages = {1--36},
  author = {Jeffrey Larson and Matt Menickelly},
  title = {Structure-Aware Methods for Expensive Derivative-Free Nonsmooth Composite Optimization},
  year = {2024},
  journal = {Mathematical Programming Computation},
}

% An adaptive scheme for selecting epsilon-constraint scalarizations when
% solving multiobjective optimization problems.
@article{laumanns2006,
  author={Laumanns, Marco and Thiele, Lothar and Zitzler, Eckart},
  title={An efficient, adaptive parameter variation scheme for metaheuristics based on the epsilon-constraint method},
  year={2006},
  journal={European Journal of Operational Research},
  volume={169},
  number={3},
  pages={932--942},
  publisher={Elsevier},
  doi={10.1016/j.ejor.2004.08.029}
}

% A whitepaper describing Cereberus's wafer scale neuromorphic computing
% architectures, as used in the AI incubator at Argonne
@techreport{lavely2022,
  author={Lavely, Adam},
  title={Powering Extreme-Scale {HPC} with {C}erebras Wafer-Scale Accelerators},
  year={2022},
  institution={Cereberas Systems, Inc.},
  address={Sunnyvale, CA, USA},
  url={https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Powering-Extreme-Scale-HPC-with-Cerebras.pdf}
}

% Peter Lax's classic textbook on functional analysis and all the core theorems
% Banach spaces, Hilbert spaces, and approximation theory
@book{lax2002,
    author = {Lax, Peter D.},
     title = {Functional analysis},
    series = {Pure and Applied Mathematics (New York)},
 publisher = {Wiley-Interscience [John Wiley \& Sons], New York},
      year = {2002},
     pages = {xx--580},
      isbn = {0-471-55604-1},
}

% NOMAD v3 is a widely-used open source numerical software package (in C++) for
% blackbox and derivative-free optimization via the MADS algorithms. Includes
% the official implementations of algorithms such as Ortho-MADS, PSD-MADS, and
% BiMADS.  Support for parallel computing and surrogate modeling, and fairly
% extensible.  Can be linked as a C++ library, or usage from command line
% interface.  Used by a variety of industries and officially supported by Exxon
% Mobile.  Has recently been replaced by the major refactor/rewrite in NOMAD
% v4.  Still an example of widely-used open source numerical software, but the
% NOMAD v4 paper gives a look at how open source software practices have
% changed (improved) in the last 10 years
@article{ledigabel2011,
  author       = {{Le Digabel}, S{\'e}bastien},
  title        = {Algorithm 909: {NOMAD}: Nonlinear Optimization with the {MADS} Algorithm},
  year         = {2011},
  journal = {ACM Transactions on Mathematical Software},
  volume       = {37},
  number       = {4},
  pages        = {44},
  doi          = {10.1145/1916461.1916468},
}

% A taxonomy of constraint types encountered when solving blackbox / simulation
% optimization: quantifiable vs nonquantifiable, relaxable vs unrelaxable, a
% priori vs simulation-based, and known vs hidden.
@techreport{ledigabel2024,
  author      = {S\'ebastien {Le Digabel} and Stefan M. Wild},
  title       = {A Taxonomy of Constraints in Black-Box Simulation-Based Optimization},
  year        = {2024},
  journal = {Optimization and Engineering},
  number      = {1505.07881},
  Volume      = {25},
  Number      = {2},
  Pages       = {1125--1143},
  doi        = {10.1007/s11081-023-09839-3},
}

% pyDOE a popular software repository for the common design-of-experiments in
% Python.  This has been widely replaced by the new release of scipy which
% includes scipy.stats.qmc, which includes most of these techniques (used for
% monte carlo sampling, but still the same techniques)
@misc{lee2015,
  title={py{DOE}: The experimental design package for python},
  author={Lee, Abraham D. et al.},
  year={2015},
  publisher={GitHub},
  journal={GitHub repository},
  version={0.3.8},
  url={https://github.com/tisimst/pyDOE}
}

% An algorithm that combines direct search / pattern search with an augmented
% Lagrangian penalty term in order to solve a constrained blackbox optimization
% problem
@article{lewis2002,
   author = {Lewis, Robert Michael and Torczon, Virginia},
   title = {A Globally Convergent Augmented {L}agrangian Pattern Search Algorithm for Optimization with General Constraints and Simple Bounds},
   journal = {SIAM Journal on Optimization},
   volume = {12},
   number = {4},
   pages = {1075-1089},
   year = {2002},
   doi = {10.1137/S1052623498339727},
}


% An early paper proposing the usage of surrogate models within
% multiobjective evolutionary algorithms in order to improve their performance
% on computationally expensive blackbox / simulation optimization problems,
% where the function evaluation budget may be limited
@inproceedings{liu2016,
  author    = {Liu, Bo and Sun, Nan and Zhang, Qingfu and Grout, Vic and Gielen, Georges},
  title     = {A surrogate model assisted evolutionary algorithm for computationally expensive design optimization problems with discrete variables},
  year      = {2016},
  booktitle = {Proc. 2016 {IEEE} Congress on Evolutionary Computation ({CEC})},
  publisher = {{IEEE}},
  location={Vancouver, BC, Canada},
  pages={1650--1657},
  doi       = {10.1109/cec.2016.7743986},
}

% Adaptive Kriging model-based sampling basically means using an interpolating
% Gaussian process's uncertainty information to select where to sample the next
% point during an adaptive sampling algorithm (for generating
% design-of-experiments or design space exploration).
@article{liu2017,
  author = {Haitao Liu and Jianfei Cai and Yew-Soon Ong},
  title = {An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error},
  year = {2017},
  journal = {Computers \& Chemical Engineering, Special Section - ESCAPE-26},
  volume = {106},
  pages = {171--182},
  doi = {10.1016/j.compchemeng.2017.05.025},
}

% The DFMO algorithm is a multiobjective line search, which the authors
% recommend combining with MODIR to improve its convergence to the Pareto front
% after identifying the global Pareto front.  The open source numerical
% software is implemented in Fortran and is currently bundled inside the MODIR
% software package on the authors' GitHub account
@article{liuzzi2016,
  author={Liuzzi, Giampaolo and Lucidi, Stefano and Rinaldi, Francesco},
  title={A derivative-free approach to constrained multiobjective nonsmooth optimization},
  year={2016},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={4},
  pages={2744--2774},
  doi={10.1137/15M1037810}
}

% The official GitHub account for the DFO-lib -- open source numerical software
% in Fortran for solving blackbox optimization and multiobjective optimization
% problems.  The library used to be obtained via download from Liuzzi's
% personal website, where it was referred to as the DFO lib.  In 2024 it
% appears to have been migrated to a GitHub account, with each individual piece
% of software in its own separate repository.  Therefore, any reference to the
% DFO-lib must now be directed to the account as a whole, not an individual
% repository.
@misc{liuzzi2024,
  title={DFO-lib},
  author={Liuzzi, Giampaolo et al.},
  year={2024},
  publisher={GitHub},
  journal={GitHub repository},
  version={0.3.8},
  url={https://github.com/DerivativeFreeLibrary}
}

% Publication describing Graphcore's IPU architecture (intelligence processing
% unit), which is a neuromorphic parallel processor optimized for
% high-throughput vector operations
@inproceedings{louw2021,
  title={Using the {Graphcore IPU} for traditional {HPC} applications},
  author={Louw, Thorben and McIntosh-Smith, Simon},
  booktitle={Proc. 3rd Workshop on Accelerated Machine Learning (AccML)},
  year={2021},
  location={virtual event},
  pages={1--9},
  url={https://easychair.org/publications/preprint/ztfj}
}

% Multiobjective extension of the DIRECT algorithm for derivative-free blackbox
% optimization.  This algorithm may suffer from some scalability issues, but is
% a good first step
@article{lovison2020,
  author={Lovison, Alberto and Miettinen, Kaisa},
  title={On the Extension of the {DIRECT} Algorithm to Multiple Objectives},
  year={2021},
  journal={Journal of Global Optimization},
  volume={79},
  pages={387--412},
  publisher={Springer},
  doi={10.1007/s10898-020-00942-8}
}

% Summary of discussions from Oak Ridge National Laboratory Summit discussing
% GPU-based architectures and other developments from the exascale computing
% project
@article{luo2020,
  author={Luo, L. and P. Straatsma, T. and Suarez, L. E. Aguilar and Broer, R. and Bykov, D. and F. D'Azevedo, E. and S. Faraji, S. and C. Gottiparthi, K. and De Graaf, C. and A. Harris, J. and A. Havenith, R. W. and Jensen, H. J. Aa. and Joubert, W. and K. Kathir, R. and Larkin, J. and W. Li, Y. and I. Lyakh, D. and B. Messer, O. E. and R. Norman, M. and C. Oefelein, J. and Sankaran, R. and F. Tillack, A. and L. Barnes, A. and Visscher, L. and C. Wells, J. and Wibowo, M.},
  title={Pre-exascale accelerated application development: The {ORNL Summit} experience},
  year={2020},
  journal={IBM Journal of Research and Development},
  volume={64},
  number={3/4},
  pages={11:1-11:21},
  doi={10.1147/JRD.2020.2965881}
}

% Discussion of online learning in the context of multiobjective optimization
@inproceedings{mannor2014,
  author = 	 {Mannor, Shie and Perchet, Vianney and Stoltz, Gilles},
  title = 	 {Approachability in unknown games: {O}nline learning meets multi-objective optimization},
  year = 	 {2014},
  booktitle = 	 {Proc. 27th Conference on Learning Theory (PMLR)},
  volume = 	 {35},
  pages = 	 {339--355},
  series = 	 {Proceedings of Machine Learning Research},
  address =  {Barcelona, Spain},
  month = 	 {13--15 June},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v35/mannor14.html},
}


% Survey of common multiobjective optimization algorithms and scalarization
% techniques in the context of engineering design optimization
@article{marler2004,
  author={Marler, Timothy R. and Arora, Jasbir S.},
  title={Survey of multi-objective optimization methods for engineering},
  year={2004},
  journal={Structural and Multidisciplinary Optimization},
  volume={26},
  number={6},
  pages={369--395},
  doi={10.1007/s00158-003-0368-6}
}

% pyMDO: an open source numerical Python framework for modeling and solving
% multidisciplinary engineering design optimization problems
@article{martins2009,
  author = {Martins, Joaquim R. R. A. and Marriage, Christopher and Tedford, Nathan},
  title = {{pyMDO}: An Object-Oriented Framework for Multidisciplinary Design Optimization},
  year = {2009},
  publisher = {ACM},
  journal = {ACM Transactions on Mathematical Software},
  volume = {36},
  number = {4},
  pages = {20},
  doi = {10.1145/1555386.1555389}
}

% Report on challenges and experiences gained in porting PETSc to GPUs for the
% exascale computing project
@article{mills2021,
  author = {Richard Tran Mills and Mark F. Adams and Satish Balay and Jed Brown and Alp Dener and Matthew Knepley and Scott E. Kruger and Hannah Morgan and Todd Munson and Karl Rupp and Barry F. Smith and Stefano Zampini and Hong Zhang and Junchao Zhang},
  title = {Toward performance-portable {PETSc} for {GPU}-based exascale systems},
  journal = {Parallel Computing},
  volume = {108},
  pages = {102831},
  year = {2021},
  doi = {10.1016/j.parco.2021.102831}
}

% DESDEO an open source Python framework for implementing interactive
% multiobjective optimization solvers
@article{misitano2021,
  author={Misitano, Giovanni and Saini, Bhupinder S. and Afsar, Bekir and Shavazipour, Babooshka and Miettinen, Kaisa},
  journal={IEEE Access},
  title={{DESDEO}: The Modular and Open Source Framework for Interactive Multiobjective Optimization},
  year={2021},
  volume={9},
  pages={148277-148295},
  doi={10.1109/ACCESS.2021.3123825}
}

% MORDRED: A 3D molecular descriptor calculator, which is widely used for
% embedding molecules into a continuous latent space (parameterized by their
% descriptors) which can be used to solve chemical property optimization
% problems.  The MORDRED software is available open source in Python.
@article{moriwaki2018,
  author={Moriwaki, Hirotomo and Tia, Yu-Shi and Kawashita, Norihito and Takagi, Tatsuya},
  year={2018},
  title={Mordred: a molecular descriptor calculator},
  journal={Journal of Cheminformatics},
  volume={10},
  articleno={4},
  numpages={14},
  doi={10.1186/s13321-018-0258-y}
}

% SOCEMO: A response surface modeling (RSM) based algorithm for solving
% multiobjective optimization problems.  Uses a Latin hypercube
% design-of-experiments, RBF surrogate modeling, multiple scalarizations, and
% solves the scalarized subproblem via evolutionary algorithms to produce a
% batch of evaluations in each iteration of the algorithm
@article{muller2017,
  author={M{\"u}ller, Juliane},
  year={2017},
  title={{SOCEMO}: {S}urrogate optimization of computationally expensive
  multiobjective problems},
  journal={INFORMS Journal on Computing},
  volume={29},
  number={4},
  pages={581--596},
  doi={10.1287/ijoc.2017.0749}
}

% This is the last TAO (toolkit for advanced optimization) reference before
% this open source numerical simulation optimization software when merged with
% PETSc, into a single PETSc + TAO release
@techreport{munson2015,
  title={{TAO} 3.5 Users Manual},
  author={Munson, Todd and Sarich, Jason and Wild, Stefan and Benson, Steven and McInnes, Lois Curfman},
  year={2015},
  institution = {Argonne National Laboratory},
  number      = {ANL/MCS-TM-322 version 3.5},
  address     = {Lemont, IL, USA},
  url         = {https://www.mcs.anl.gov/petsc/petsc-3.5.4/docs/tao_manual.pdf}
}

% The classical textbook on response surface methodology and modeling
% practices.  Contains useful information on the basic framework and
% applications of RSM.  Also a useful reference for many of the options for
% specific techniques: Chapter 7 is a good reference for basic techniques in
% multiobjective RSM and Chapters 8-9 surveys the basic methods in
% design-of-experiments
@book{myers2016,
  author={Myers, Raymond H. and Montgomery, Douglas C. and Anderson-Cook, Christine M.},
  year={2016},
  title={Response Surface Methodology: Process and Design Optimization Using
  Designed Experiments},
  edition={4},
  publisher={John Wiley \& Sons, Inc.},
  address={Hoboken, NJ, USA},
  isbn={9781118916032}
}

% An application of VTMOP for the multiobjective optimization (tuning) of the
% LCLS-II photoinjector (linear accelerator at SLAC).  Had to use some hacks to
% get VTMOP to work, such as penalizing bad regions of the Pareto front, but
% ultimately performed better than NSGA-II
@article{neveu2023,
    author={Neveu, Nicole and Chang, Tyler H. and Franz, Paris and Hudson, Stephen and Larson, Jeffrey},
    title={Comparison of multiobjective optimization methods for the {LCLS-II} photoinjector},
    year = {2023},
    journal={Computer Physics Communication},
    volume={283},
    articleno={108566},
    numpages={10},
    doi = {10.1016/j.cpc.2022.108566}
}

% The original publication where Sobol's sequences are generalized to the class
% now known as low-discrepancy sequenes
@article{niederreiter1988,
    author = {Niederreiter, Harald},
     title = {Low-discrepancy and low-dispersion sequences},
   journal = {Journal of Number Theory},
    volume = {30},
      year = {1988},
    number = {1},
     pages = {51--70},
      issn = {0022-314X},
       doi = {10.1016/0022-314X(88)90025-X},
       url = {https://doi.org/10.1016/0022-314X(88)90025-X}
}

% The classic textbook by Nocedal on fundamental techniques in nonlinear
% programming, such as local modeling and trust-region methods
@book{nocedal2006,
  author={Nocedal, Jorge and Wright, Stephen J.},
  year={2006},
  title={Numerical Optimization},
  edition={2},
  series={Springer Series in Operations Research and Financial Engineering},
  publisher={Springer Verlag},
  address={New York, NY, USA},
  doi={10.1007/978-0-387-40065-5}
}

% PABO - a multiobjective Bayesian optimization software package that is
% specialized for NAS -- basically an inner network is used as a surrogate and
% an outer network is used to predict which designs to evaluate next -- I have
% serious reservations about this kind of approach, and it doesn't seem to work
% that well
@inproceedings{parsa2019,
  author={Parsa, Maryam and Ankit, Aayush and Ziabari, Amirkoushyar and Roy, Kaushik},
  title={{PABO}: Pseudo Agent-Based Multi-Objective {B}ayesian Hyperparameter Optimization for Efficient Neural Accelerator Design},
  year={2019},
  booktitle={Proc. 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1-8},
  organization={IEEE/ACM},
  address={Westin Westminster, CO, USA},
  doi={10.1109/ICCAD45719.2019.8942046}
}

% H-PABO multiobjective Bayesian optimization framework, specialized for NAS,
% and improvement on PABO
@article{parsa2020,
  author={Parsa, Maryam and Mitchell, John P. and Schuman, Catherine D. and Patton, Robert M. and Potok, Thomas E. and Roy, Kaushik},
  title={Bayesian Multi-objective Hyperparameter Optimization for Accurate, Fast, and Efficient Neural Network Accelerator Design},
  year={2020},
  journaL={Frontiers in Neuroscience},
  volume={14},
  pages={667},
  doi={10.3389/fnins.2020.00667}
}

% The official publicatino for pytorch a gold standard in open source software,
% providing automatic differentiation and numerical linear algebra in Python,
% targeted at implementing deep learning algorithms.  Pytorch is pretty much a
% standard in not just open source software, but also machine learning
% software, and also numerical software
@inproceedings{paszke2019,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
 year = {2019},
 booktitle = {Advances in Neural Information Processing Systems},
 volume = {32},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 pages={1--12}
}


% The original TOMS open source numerical software code implementing Sobol
% sequence (low discrepancy sequence) generation in Fortran 90. Apparently
% there is a bug or limitation to this code, fixed by Joe et al. 2003 in their
% TOMS Remark on 659, and subsequent publication of a new generator used in
% Scipy
@article{paul1988,
author = {Bratley, Paul and Fox, Bennett L.},
title = {Algorithm 659: Implementing Sobol's Quasirandom Sequence Generator},
year = {1988},
publisher = {Association for Computing Machinery},
volume = {14},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/42288.214372},
doi = {10.1145/42288.214372},
journal = {ACM Transactions on Mathematical Software},
pages = {88–100},
numpages = {13},
}

% The official publicatino for scikit-learn a gold standard in open source
% software, providing a clean interface to several standard implementations of
% numerical approximation, optimization, machine learning, and deep learning
% algorithms.  Scikit-learn is pretty much a standard in not just open source
% software, but also machine learning software, and also numerical software
@article{pedregosa2011,
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  title={Scikit-learn: Machine learning in {P}ython},
  year={2011},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  url={https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf}
}

% The COBYLA paper on Powell's constrainted optimization by linear
% approximation algorithm.  COBYLA basically performs gradient descent on a
% constrainted blackbox optimization problem by fitting a linear model to the
% underlying function and following its gradient within a shrinking trust
% region.  COBYLA is typically able to do this taking typically only one
% or two function evaluation per iteration since typically only one point is
% exiting the trust-region per iteration.  (Occasionally, additional model
% improvement points must be sampled to maintain the interpolation set
% geometry).  This makes COBYLA barely more expensive then true gradient
% descent.  COBYQA has supplanted COBYLA since then (the Q standing for
% quadratic), but I personally prefer COBYLA still as a find the locally linear
% models to be more robust against noisy and nonsmooth data, still efficiently
% finding local minima even though COBYLA was not design for such problems.
% The original open source software was in impossibly complex old-style
% Fortran.  A modern Fortran version has been provided in Pima by Zaikun Zhang,
% and a modern Python implementation is provided in PDFO by Ragonneau and
% Zhang.
@inproceedings{powell1994,
  title={A direct search optimization method that models the objective and constraint functions by linear interpolation},
  author={Powell, Michael J. D.},
  year={1994},
  booktitle={Gomez, S. and Hennart, J. P. (eds) Advances in Optimization and Numerical Analysis, vol 275},
  organization={Springer},
  pages={51--67},
  doi={10.1007/978-94-015-8330-5_4}
}

% Review of minimax and maximin techniques and computational methods.  Minimax
% is what we want, but it is hard to compute in high dimensions (would require
% optimizing a Delaunay triangulation, which is exponential complexity to
% compute).  Maximin is easier to compute (and often used in many algorithms
% because of this).  However, minimax gives better dispersion
@article{pronzato2017,
  title={Minimax and maximin space-filling designs: some properties and methods for construction},
  author={Pronzato, Luc},
  journal={Journal de la Soci{\'e}t{\'e} Fran{\c{c}}aise de Statistique},
  volume={158},
  number={1},
  pages={7--36},
  year={2017},
  url={http://www.numdam.org/item/JSFS_2017__158_1_7_0}
}

% An active learning method for extreme event modeling.  Specifically, the
% authors provide a novel acquisition function which targets extreme events
@article{pickering2022,
  title={Discovering and forecasting extreme events via active learning in neural operators},
  author={Pickering, Ethan and Guth, Stephen and Karniadakis, George Em and Sapsis, Themistoklis P},
  journal={Nature Computational Science},
  volume={2},
  number={12},
  pages={823--833},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

% Experiences, challenges, and techniques for integrating the blackbox
% optimization solvers VTDIRECT95 and QNSTOP into the parallel service
% architecture SORCER
@inproceedings{raghunath2017,
  author={Raghunath, Chaitra and Chang, Tyler H. and Watson, Layne T. and Jrad, Mohamad and Kapania, Rakesh K. and Kolonay, Raymond M.},
  title={Global deterministic and stochastic optimization in a service oriented architecture},
  year={2017},
  booktitle={Proc. 2017 Spring Simulation Conference (SpringSim 2017), the 25th High Performance Computing Symposium (HPC '17)},
  organization={SCS},
  address={Virginia Beach, VA, USA},
  pages={7},
  doi = {10.22360/springsim.2017.hpc.023}
}

% PDFO: An open source modern Python implementation of Powell's derivative-free
% numerical optimization software suite, which is considered to be the standard
% (baseline) in derivative-free optimization solvers
@misc{ragonneau2021,
  title={{PDFO}: Cross-Platform Interfaces for {P}owell’s Derivative-Free Optimization Solvers},
  author={Ragonneau, Tom M. and Zhang, Zaikun},
  year={2021},
  publisher={GitHub},
  journal={GitHub repository},
  version={1.2},
  url={https://github.com/pdfo/pdfo}
}

% Using RBF surrogates in the context of multiobjective optimization.  I
% believe he ultimately solved the surrogate problems with NSGA-II or some
% other heuristic
@article{regis2016,
  author       = {Regis, Rommel G.},
  title        = {Multi-objective constrained black-box optimization using radial basis function surrogates},
  year         = {2016},
  journal = {Journal of Computational Science},
  volume       = {16},
  pages        = {140--155},
  doi          = {10.1016/j.jocs.2016.05.013},
}

% Official publication of the scipy.stats.qmc module, which is the newly
% released module for performing quasi-monte carlo sampling and
% design-of-experiments in scipy.  Scipy is an open source numerical software
% package which is the standard for advanced numerical methods and scientific
% software packages in Python.  Most of scipy are wrappers for much older
% Fortran or C++ code, that has been highly optimized.
@article{roy2023,
  author = {Pamphile T. Roy and Art B. Owen and Maximilian Balandat and Matt Haberland},
  title = {Quasi-Monte Carlo Methods in Python},
  year = {2023},
  journal = {Journal of Open Source Software},
  volume = {8},
  number = {84},
  pages = {5309},
  publisher = {The Open Journal},
  doi = {10.21105/joss.05309},
}

% The original publication of PAWS: An adaptive weighted sum and trust-regions
% method for solving biobjective (multiobjective) optimization problems.  The
% key here is that the trust-regions help the weighted sum scalarization reach
% into nonconvex regions of the Pareto front, even though that would not
% normally be possible
@inproceedings{ryu2009,
  author={Ryu, Jong-hyun and Kim, Sujin and Wan, Hong},
  title={Pareto front approximation with adaptive weighted sum method in multiobjective simulation optimization},
  year={2009},
  booktitle={Proc. 2009 Winter Simulation Conference (WSC '09)},
  organization={IEEE},
  location={Austin, TX, USA},
  pages={623--633},
  doi={10.1109/WSC.2009.5429562}
}

% The latest version of PAWS: An adaptive weighted sum and trust-regions method
% for solving biobjective (multiobjective) optimization problems.  The key here
% is that the trust-regions help the weighted sum scalarization reach into
% nonconvex regions of the Pareto front, even though that would not normally be
% possible
@article{ryu2014,
  title={A derivative-free trust-region method for biobjective optimization},
  author={Ryu, Jong-Hyun and Kim, Sujin},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={1},
  pages={334--362},
  year={2014},
  publisher={SIAM},
  doi={10.1137/120864738}
}

% Techniques and optimal sampling criteria for performing adaptive sampling to
% enable downstream Gaussian process regression modeling
@article{sapsis2022,
  title={Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with {Gaussian} process regression},
  author={Sapsis, Themistoklis P. and Blanchard, Antoine},
  journal={Philosophical Transactions of the Royal Society A},
  volume={380},
  number={2229},
  pages={20210197},
  year={2022},
  publisher={The Royal Society},
  doi={10.1098/rsta.2021.0197}
}

% The SMT 2.0 paper, major improvements to the open source numerical software
% package (in Python) pySMT for solving multidisciplinary engineering design
% optimization (MDO) problems, while utilizing derivatives and providing
% numerical stability analysis for each surrogate model class.  In SMT 2.0,
% support is added for hierarchical and mixed variables, and major improvements
% have been made to the structure, completeness, and features of the SMT
% library.
@article{saves2024,
title = {SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
journal = {Advances in Engineering Software},
volume = {188},
pages = {103571},
year = {2024},
issn = {0965-9978},
doi = {10.1016/j.advengsoft.2023.103571},
url = {https://www.sciencedirect.com/science/article/pii/S096599782300162X},
author = {Paul Saves and Rémi Lafage and Nathalie Bartoli and Youssef Diouane and Jasper Bussemaker and Thierry Lefebvre and John T. Hwang and Joseph Morlier and Joaquim R.R.A. Martins},
}

% Multiobjective molecule property optimization by optimizing processes in a
% continuous flow reactor (CFR) using the multiobjective evolutionary algorithm
% TS-EMO (thompson sampling evolutionary multiobjective optimization?)
@article{schweidtmann2018,
  author={Schweidtmann, Artur M. and Clayton, Adam D. and Holmes, Nicholas and Bradford, Eric and Bourne, Richard A. and Lapkin, Alexei A.},
  title={Machine learning meets continuous flow chemistry: Automated optimization towards the {Pareto} front of multiple objectives},
  year={2018},
  journal={Chemical Engineering Journal},
  volume={352},
  pages={277--282},
  publisher={Elsevier},
  doi={10.1016/j.cej.2018.07.031}
}

% A survey of hypervolume indicator usage in multiobjective evolutionary
% optimization, mainly in terms of algorithms that use hypervolume improvement
% and also as a performance indicator
@article{shang2020,
  author={Shang, Ke and Ishibuchi, Hisao and He, Linjun and Pang, Lie Meng},
  title={A survey on the hypervolume indicator in evolutionary multiobjective optimization},
  year={2020},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={25},
  number={1},
  pages={1--20},
  publisher={IEEE},
  doi={10.1109/TEVC.2020.3013290}
}

% A recent paper on the numerical performance of finite-difference-based
% methods for DFO.  Personally, I don't think this is a viable approach given
% the performance of model-based methods.  However, one of their experiments
% corroborates my experience that you can just run COBYLA on noisy and
% nonsmooth blackbox optimization problems, and even though it was not designed
% for those problems, it still does extremely well and regularly finds local
% minima (at least up to the noise level) -- from Nocedal's lab
@article{shi2023,
author = {Hao-Jun Michael Shi and Melody Qiming Xuan and Figen Oztoprak and Jorge Nocedal and},
title = {On the numerical performance of finite-difference-based methods for derivative-free optimization},
journal = {Optimization Methods and Software},
volume = {38},
number = {2},
pages = {289--311},
year = {2023},
publisher = {Taylor \& Francis},
doi = {10.1080/10556788.2022.2121832},
url = {https://doi.org/10.1080/10556788.2022.2121832},
}


% The EDBO software: open source Python software for performing multiobjective
% bayesian optimization for chemical synthesis and molecular discovery.  Links
% a multiobjective optimization solver with the MORDRED software for getting
% molecular descriptors and optimizes for the desired properties
@article{shields2021,
  author   = {Shields, Benjamin J. and Stevens, Jason and Li, Jun and Parasram, Marvin and Damani, Farhan and Alvarado, Jesus I. M. and Janey, Jacob M. and Adams, Rryan P. and Doyle, Abigail G.},
  title    = {Bayesian reaction optimization as a tool for chemical synthesis},
  year     = {2021},
  journal  = {Nature},
  volume   = {590},
  number   = {7844},
  pages    = {89--96},
  software = {http://github.com/b-shields/edbo},
  doi = {10.1038/s41586-021-03213-y}
}

% One of the most popular textbooks in the field of multidisciplinary
% engineering design optimization.  The introduction provides plenty of
% motivation for solving computational expensive multiobjective simulation /
% blackbox optimization problems
@book{ss2015,
  author={Sobieszczanski-Sobieski, Jaroslaw and Morris, Alan and Van Tooren, Michel},
  title={Multidisciplinary Design Optimization Supported by Knowledge Based Engineering},
  year={2015},
  publisher={John Wiley \& Sons, Ltd.},
  address={Chichester, UK},
  isbn={978-1-118-49212-3}
}

% The original publication of the Sobol sequence algorithm for generating
% well-distributed points for in the context of good nodes for numerical
% integration.  This is now referred to as a low-discrepancy sequence and is
% also used for design-of-experiments and quasi-random number generation
@article{sobol1967,
    author = {Sobol, I. M.},
     title = {Distribution of points in a cube and approximate evaluation of
              integrals},
   journal = {\v{Z}urnal Vy\v{c}islitel\cprime no\u{\i} Matematiki i Matemati\v{c}esko\u{\i} Fiziki},
    volume = {7},
      year = {1967},
     pages = {784--802},
      issn = {0044-4669},
}

% The FAIR principles of data management:  Scientific data should be findable,
% accessible, interpretable, and reproducible
@article{stall2019,
  author={Stall, Shelley and Yarmey, Lynn and Cutcher-Gershenfeld, Joel and Hanson, Brooks and Lehnert, Kerstin and Nosek, Brian and Parsons, Mark and Robinson, Erin and Wyborn, Lesley},
  title={Make scientific data {FAIR}},
  year={2019},
  publisher={Nature Publishing Group},
  journal={Nature},
  volume={570},
  pages={27--29},
  doi={10.1038/d41586-019-01720-7}
}

% The original publication of the weighted Chebyshev scalarization scheme for
% multiobjective optimization
@article{steuer1983,
  title={An interactive weighted Tchebycheff procedure for multiple objective programming},
  author={Steuer, Ralph E and Choo, Eng-Ung},
  journal={Mathematical programming},
  volume={26},
  pages={326--344},
  year={1983},
  publisher={Springer},
  doi={10.1007/BF02591870}
}

% BoostDMS is numerical software library providing access to Custodio's direct
% search and pattern search software, including MultiGLODS and DMS, in Matlab
% with full parallel computing support
@article{tavares2021,
  author      = {S. Tavares and C. P. Br\'as and A. L. Cust\'odio and V. Duarte and P. Medeiros},
  title       = {Parallel Strategies for Direct Multisearch},
  doi = {10.1007/s11075-022-01364-1},
  year = {2022},
  volume = {92},
  number = {3},
  pages = {1757--1788},
  journal = {Numerical Algorithms} 
}

% A trust-region + RBF surrogate-based multiobjective optimization algorithm
% for solving heterogeneous multiobjective optimization problems (where one or
% more objectives is a computationally expensive blackbox, and one or more is
% not).  The key is to just use the derivative of all the non blackbox
% objectives and use the model derivative for the blackbox terms.  The
% algorithm itself follows something like Orbit
@article{thomann2019,
  author       = {Thomann, Jana and Eichfelder, Gabriele},
  title        = {A Trust-Region Algorithm for Heterogeneous Multiobjective Optimization},
  year         = {2019},
  journal = {{SIAM} Journal on Optimization},
  volume       = {29},
  number       = {2},
  pages        = {1017--1047},
  doi          = {10.1137/18m1173277},
}

% An open source MATLAB platform for implementing and running wide-scale
% comparisons against other multiobjective evolutionary algorithms on standard
% multiobjective test problems
@article{tian2017,
  author={Tian, Ye and Cheng, Ran and Zhang, Xingyi and Jin, Yaochu},
  title={{PlatEMO}: A {MATLAB} Platform for Evolutionary Multi-Objective Optimization [Educational Forum]},
  year={2017},
  journal={IEEE Computational Intelligence Magazine},
  volume={12},
  number={4},
  pages={73-87},
  doi={10.1109/MCI.2017.2742868}
}

% A survey of Pareto front visualization techniques in multiobjective
% optimization
@article{tusar2015,
  author={Tu{\v s}ar, Tea and Filipi{\v c}, Bogdan},
  title={Visualization of {P}areto Front Approximations in Evolutionary Multiobjective Optimization: A Critical Review and the Prosection Method},
  year={2015},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={19},
  number={2},
  pages={225-245},
  doi={10.1109/TEVC.2014.2313407}
}

% A robust RBF surrogate-based model, with adaptive scaling of the basis
% function radii to maintain numerical stability and a custom LHS sampling
% technique
@article{urquhart2020,
  author = {Magnus Urquhart and Emil Ljungskog and Simone Sebben},
  title = {Surrogate-based optimisation using adaptively scaled radial basis functions},
  year = {2020},
  journal = {Applied Soft Computing},
  volume = {88},
  pages = {106050},
  doi = {10.1016/j.asoc.2019.106050},
}

% A tutorial on how to compute Latin hypercube samples (LHS) and some basic
% properties and ongoing research related to design-of-experiments
@article{viana2016,
  title={A tutorial on Latin hypercube design of experiments},
  author={Viana, Felipe AC},
  journal={Quality and reliability engineering international},
  volume={32},
  number={5},
  pages={1975--1985},
  year={2016},
  publisher={Wiley Online Library}
}

% SciPy official publication:  Scipy is an open source numerical software
% package which is the standard for advanced numerical methods and scientific
% software packages in Python.  Most of scipy are wrappers for much older
% Fortran or C++ code, that has been highly optimized.
@article{virtanen2020,
  author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
     Travis E. and {Haberland}, Matt and {Reddy}, Tyler and
     {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
     and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
     St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
     {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
     R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
     {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
     Eric W. and {VanderPlas}, Jake and {Laxalde}, Denis and
     {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
     {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
     and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
     {van Mulbregt}, Paul and {Contributors}, {SciPy 1.0}},
  title = {{SciPy} 1.0: {F}undamental Algorithms for Scientific
     Computing in {P}ython},
  year = {2020},
  journal = {Nature Methods},
  volume={17},
  number={3},
  pages={261--272},
  doi={10.1038/s41592-019-0686-2}
}

% Open source numerical Python software pyomo.DOE, implementing model-driven
% design-of-experiments generation in Pyomo
@article{wang2022,
    author = {Wang, Jialu and Dowling, Alexander W.},
    title = {Pyomo.DOE: An open-source package for model-based design of experiments in Python},
    year = {2022},
    journal = {AIChE Journal},
    volume = {68},
    number = {12},
    articleno = {e17813},
    doi = {https://doi.org/10.1002/aic.17813}
}

% Survey of design-of-experiments techniques and modifications for HPC system
% analysis, specifically related to linearly constrained and integer lattice
% design spaces
@article{wang2023,
  author = {Wang, Yueyao and Xu, Li and Hong, Yili and Pan, Rong and Chang, Tyler H. and Lux, Thomas C. H. and Bernard, Jon and Watson, Layne T. and Cameron, Kirk W.},
  title = {Design strategies and approximation methods for high-performance computing variability management},
  year = {2023},
  journal = {Journal of Quality Technology},
  volume = {55},
  number = {1},
  pages = {88--103},
  publisher = {Taylor \& Francis},
  doi = {10.1080/00224065.2022.2035285}
}

% The original publication of the reference point method for scalarizing
% multiobjective optimization problems (minimize the distance to a reference
% point)
@incollection{wierzbicki1999,
  author={Wierzbicki, Andrzej P.},
  editor={Gal, Tomas and Stewart, Theodor J. and Hanne, Thomas},
  title={Reference Point Approaches},
  year={1999},
  booktitle={Multicriteria Decision Making: Advances in MCDM Models, Algorithms, Theory, and Applications},
  publisher={Springer US},
  address={Boston, MA},
  pages={237--275},
  doi={10.1007/978-1-4615-5025-9_9}
}

% The POUNDERS composite blackbox / simulation optimization algorithm, which is
% an open source numerical software package for exploiting the sum-of-squares
% structure in derivative-free least squares problems.  Specifically, POUNDERS
% models the blackbox function / simulation's outputs using a fully linear
% model then uses the sum-of-squares structure to get a free Hessian
% approximation, and achieve second-order convergence for the price of
% first-order convergence.  Although not included, open source numerical
% software implementations are now available in Python and Matlab through the
% PyOptUs GitHub group
@incollection{wild2017,
  author    = {Stefan M. Wild},
  title     = {Solving Derivative-Free Nonlinear Least Squares Problems with {POUNDERS}},
  year      = {2017},
  booktitle = {Advances and Trends in Optimization with Engineering Applications},
  publisher = {SIAM},
  editor    = {Terlaky, Tamas and Anjos, Miguel F. and Ahmed, Shabbir},
  pages     = {529--540},
  isbn     = {978-1-611974-67-6},
  url      = {http://www.mcs.anl.gov/papers/P5120-0414.pdf},
  doi       = {10.1137/1.9781611974683.ch40},
}

% ORBIT: the original algorithm for solving optimization problems via
% sequentially minimizing RBF interpolants with a linear tail inside a sequence
% of trust-regions.  I can't remember if it's open source, but Stefan has a
% high quality numerical software in MATLAB
@article{wild2007,
  author       = {Wild, Stefan M. and Regis, Rommel G. and Shoemaker, Christine A.},
  title        = {{ORBIT:} {O}ptimization by Radial Basis Function Interpolation in Trust-Regions},
  year         = {2008},
  journal = {SIAM Journal on Scientific Computing},
  volume       = {30},
  number       = {6},
  pages        = {3197--3219},
  doi          = {10.1137/070691814},
}

% Analysis of the convergence rate of RBF-based surrogates with linear tail
% (fully linear model) inside a sequence of trust regions.  This is the
% theory used in ORBIT
@article{wild2011,
  author       = {Wild, Stefan M. and Shoemaker, Christine A.},
  title        = {Global Convergence of Radial Basis Function Trust Region Derivative-Free Algorithms},
  year         = {2011},
  journal = {SIAM Journal on Optimization},
  volume       = {21},
  number       = {3},
  pages        = {761--781},
  doi          = {10.1137/09074927X}
}

% A hypervolume-based approach to a multiobjective DIRECT algorithm for
% multiobjective blackbox / simulation optimization
@inproceedings{wong2016,
  author = {Wong, Cheryl Sze Yin and Al-Dujaili, Abdullah and Sundaram, Suresh},
  title = {Hypervolume-Based {DIRECT} for Multi-Objective Optimisation},
  year = {2016},
  booktitle = {Proc. 2016 Genetic and Evolutionary Computation Conference Companion (GECCO '16)},
  publisher={ACM},
  location = {Denver, CO, USA},
  pages = {1201–1208},
  doi = {10.1145/2908961.2931702}
}

% Hammersley's and Halton sequences -- other low-discrepancy sequences that are
% commonly used in design-of-experiments
@article{wong1997,
  title={Sampling with {H}ammersley and {H}alton points},
  author={Wong, Tien-Tsin and Luk, Wai-Shing and Heng, Pheng-Ann},
  journal={Journal of graphics tools},
  volume={2},
  number={2},
  pages={9--24},
  year={1997},
  publisher={Taylor \& Francis}
}

% A survey of interactive techniques and software in multiobjective
% optimization including the known challenges limitations
@article{xin2018,
  author={Xin, Bin and Chen, Lu and Chen, Jie and Ishibuchi, Hisao and Hirota, Kaoru and Liu, Bo},
  journal={IEEE Access},
  title={Interactive Multiobjective Optimization: A Review of the State-of-the-Art},
  year={2018},
  volume={6},
  pages={41256--41279},
  doi={10.1109/ACCESS.2018.2856832}
}

% Multiobjective Bayesian optimization with hypervolume improvement-based
% acquisition
@article{yang2019a,
  author       = {Yang, Kaifeng and Emmerich, Michael and Deutz, Andr{\'{e}} and B\"{a}ck, Thomas},
  title        = {Multi-Objective {Bayesian} Global Optimization Using Expected Hypervolume Improvement Gradient},
  year         = {2019},
  journal = {Swarm and Evolutionary Computation},
  volume       = {44},
  pages        = {945--956},
  doi          = {10.1016/j.swevo.2018.10.007}
}

% Multiobjective Bayesian optimization with hypervolume improvement-based
% acquisition
@inproceedings{yang2019b,
  author    = {Yang, Kaifeng and Palar, Pramudita Satria and Emmerich, Michael and Shimoyama, Koji and B\"{a}ck, Thomas},
  title     = {A multi-point mechanism of expected hypervolume improvement for parallel multi-objective {Bayesian} global optimization},
  year      = {2019},
  booktitle = {Proc. Genetic and Evolutionary Computation Conference (GECCO19)},
  publisher = {ACM},
  doi       = {10.1145/3321707.3321784}
}

% Solving DFT model calibrations for chemical design via active learning
@article{yuan2023,
  title={Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials},
  author={Yuan, Xiaoze and Zhou, Yuwei and Peng, Qing and Yang, Yong and Li, Yongwang and Wen, Xiaodong},
  journal={Nature Computational Materials},
  volume={9},
  number={1},
  pages={12},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

% PhD thesis on extracting high-dimensional (many objective) Pareto fronts
% including a thorough survey of such algorithms
@phdthesis{yukish2004,
  author={Yukish, Michael},
  title={Algorithms to identify {P}areto points in multi-dimensional data sets},
  year={2004},
  school={The Pennsylvania State University, Dept. of Mechanical Engineering},
  url={https://etda.libraries.psu.edu/catalog/6336}
}

% Convergence analysis when solving composite sum-of-squares blackbox /
% simulation optimization problems by modeling the blackbox function /
% simulation's outputs using a fully linear model then using the sum-of-squares
% structure to get a free Hessian approximation.  Basically, one can achieve
% second-order convergence for the price of first-order convergence
@article{zhang2012,
  author       = {Zhang, Hongchao and Conn, Andrew R.},
  title        = {On the Local Convergence of a Derivative-Free Algorithm for Least-Squares Minimization},
  year         = {2012},
  journal = {Computational Optimization and Applications},
  volume       = {51},
  number       = {2},
  pages        = {481--507},
  doi          = {10.1007/s10589-010-9367-x},
}

% An algorithm for solving composite sum-of-squares blackbox / simulation
% optimization problems by modeling the blackbox function / simulation's
% outputs using a fully linear model then using the sum-of-squares structure to
% get a free Hessian approximation
@article{zhang2010,
  author       = {Hongchao Zhang and Andrew R. Conn and Katya Scheinberg},
  title        = {A Derivative-Free Algorithm for Least-Squares Minimization},
  year         = {2010},
  journal = {SIAM Journal on Optimization},
  volume       = {20},
  number       = {6},
  pages        = {3555--3576},
  doi          = {10.1137/09075531X},
}

% Prima: open source reference implementation of all of Powell's numerical
% optimization solvers for blackbox / simulation optimization problems in
% modern Fortran.  IMO, these should be considered the state-of-the-art and
% reference implementations for all blackbox optimization research
@misc{Zhang_2023,
    title        = {{PRIMA: Reference Implementation for Powell's Methods with Modernization and Amelioration}},
    author       = {Zhang, Zaikun},
    year         = {2023},
    howpublished = {github repository},
    url={http://www.libprima.net},
    doi={10.5281/zenodo.8052654},
}

% An application for multiobjective optimization in the context of aircraft
% wing design.  We are optimizing one objective that is the lift/drag ratio,
% and another that describes the controllability.  Problem is solved using
% multiobjective particle swarm
@inproceedings{zhao2018,
  author = {Zhao, Wei and Kapania, Rakesh K.},
  title = {Multiobjective Optimization of Composite Flying-wings with {SpaRibs} and Multiple Control Surfaces},
  year={2018},
  booktitle = {Proc. 2018 Multidisciplinary Analysis and Optimization Conference},
  location={Atlanta, GA, USA},
  organization={AIAA},
  pages = {3424},
  doi = {10.2514/6.2018-3424}
}

% The original publication for L-BFGS-B software, which solves
% bound-constrained optimization problems using a limited-memory BFGS.
% This is the standard implementation that is used in all L-BFGS-B codes to
% date, such as scipy, all machine learning codes, and most engineering codes
% and nonlinear systems solvers.  The code is open source high-quality
% numerical software, written in old-style Fortran
@article{zhu1997,
  author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
  title = {Algorithm 778: {L-BFGS-B}: {F}ortran Subroutines for Large-Scale Bound-Constrained Optimization},
  year = {1997},
  journal = {ACM Transactions on Mathematical Software},
  publisher = {ACM},
  volume = {23},
  number = {4},
  pages = {550–560},
  doi = {10.1145/279232.279236}
}

% SPEA2 strength Pareto evolutionary algorithm -- an old evolutionary algorithm
% that was once a competitor to NSGA-II (and with significant overlap in
% co-authorship), but is now largely obsolete.
@article{zitzler2001,
  title={{SPEA2}: Improving the strength {Pareto} evolutionary algorithm},
  author={Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
  journal={TIK-report},
  volume={103},
  year={2001},
  publisher={Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich (ETH Zurich), Institut f{\"u}r Technische},
  code={https://github.com/manuparra/spea2},
  doi={10.3929/ethz-a-004284029}
}
